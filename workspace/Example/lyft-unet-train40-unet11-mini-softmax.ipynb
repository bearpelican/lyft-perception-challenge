{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "%reload_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fastai.conv_learner import *\n",
    "from fastai.models.resnet import vgg_resnet50\n",
    "\n",
    "import json\n",
    "from glob import glob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.backends.cudnn.benchmark=True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH = Path('../data/all')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_img(im, figsize=None, ax=None, alpha=None):\n",
    "    if not ax: fig,ax = plt.subplots(figsize=figsize)\n",
    "    ax.imshow(im, alpha=alpha)\n",
    "    ax.set_axis_off()\n",
    "    return ax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "VEHICLES=10\n",
    "ROADS=7\n",
    "ROAD_LINES=6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAIN_DN = 'CameraRGB'\n",
    "MASKS_DN = 'CameraSeg'\n",
    "workers=7\n",
    "random_crop=True\n",
    "pseudo_label=False\n",
    "val_folder = 'sample_test_sync'\n",
    "# val_folder = 'val'\n",
    "S_PREFIX = '40-unet11-mini-softmax'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision.datasets.folder import pil_loader\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.transforms.functional as TTF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MatchedFilesDataset(Dataset):\n",
    "    def __init__(self, fnames, y, tfms, path):\n",
    "        self.path,self.fnames = path,fnames\n",
    "        self.open_fn = pil_loader\n",
    "        self.y=y\n",
    "        self.open_y_fn = pil_loader\n",
    "        assert(len(fnames)==len(y))\n",
    "        \n",
    "        self.n = self.get_n()\n",
    "        self.c = self.get_c()\n",
    "        self.tfms = tfms\n",
    "        \n",
    "    def get_x(self, i): return self.open_fn(os.path.join(self.path, self.fnames[i]))\n",
    "    def get_y(self, i): return self.open_y_fn(os.path.join(self.path, self.y[i]))\n",
    "    def get_n(self): return len(self.fnames)\n",
    "    def get_c(self): return 2\n",
    "    \n",
    "    def get(self, tfms, x, y):\n",
    "        for fn in tfms:\n",
    "            #pdb.set_trace()\n",
    "            x, y = fn(x, y)\n",
    "        return (x, y)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        x,y = self.get_x(idx),self.get_y(idx)\n",
    "        return self.get(self.tfms, x, y)\n",
    "    \n",
    "    def __len__(self): return self.n\n",
    "\n",
    "    def resize_imgs(self, targ, new_path):\n",
    "        dest = resize_imgs(self.fnames, targ, self.path, new_path)\n",
    "        return self.__class__(self.fnames, self.y, self.transform, dest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Seems to speed up training by ~2%\n",
    "class DataPrefetcher():\n",
    "    def __init__(self, loader, stop_after=None):\n",
    "        self.loader = loader\n",
    "        self.dataset = loader.dataset\n",
    "        self.stream = torch.cuda.Stream()\n",
    "        self.stop_after = stop_after\n",
    "        self.next_input = None\n",
    "        self.next_target = None\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.loader)\n",
    "    \n",
    "    def preload(self):\n",
    "        try:\n",
    "            self.next_input, self.next_target = next(self.loaditer)\n",
    "        except StopIteration:\n",
    "            self.next_input = None\n",
    "            self.next_target = None\n",
    "            return\n",
    "        with torch.cuda.stream(self.stream):\n",
    "            self.next_input = self.next_input.cuda(async=True)\n",
    "            self.next_target = self.next_target.cuda(async=True)\n",
    "\n",
    "    def __iter__(self):\n",
    "        count = 0\n",
    "        self.loaditer = iter(self.loader)\n",
    "        self.preload()\n",
    "        while self.next_input is not None:\n",
    "            torch.cuda.current_stream().wait_stream(self.stream)\n",
    "            input = self.next_input\n",
    "            target = self.next_target\n",
    "            self.preload()\n",
    "            count += 1\n",
    "            yield input, target\n",
    "            if type(self.stop_after) is int and (count > self.stop_after):\n",
    "                break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def crop_bg_pil(x,y):\n",
    "    w, h = x.size\n",
    "    top = int(h/3.75)\n",
    "    bot = int(h*.9 + h/150)\n",
    "    pad_right=32-w%32\n",
    "    if pad_right == 32: pad_right = 0\n",
    "    return TTF.crop(x, top, 0, bot-top, w+pad_right), TTF.crop(y, top, 0, bot-top, w+pad_right)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RHF(object):\n",
    "    def __init__(self, p=0.5): self.p = p\n",
    "    def __call__(self, x, y):\n",
    "        if random.random() < self.p:\n",
    "            return TTF.hflip(x), TTF.hflip(y)\n",
    "        return x,y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RR(object):\n",
    "    def __init__(self, degrees=2): self.degrees = degrees\n",
    "    def __call__(self, x, y):\n",
    "        angle = random.uniform(-self.degrees, self.degrees)\n",
    "        return TTF.rotate(x, angle), TTF.rotate(y, angle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tfm_x_wrapper(tfm):\n",
    "    return lambda x,y: (tfm(x), y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RC():\n",
    "    def __init__(self, targ_sz):\n",
    "        self.targ_sz = targ_sz\n",
    "\n",
    "    def __call__(self, x, y):\n",
    "        rand_w = random.uniform(0, 1)\n",
    "        rand_h = random.uniform(0, 1)\n",
    "        w,h = x.size\n",
    "        t_w,t_h = self.targ_sz\n",
    "        start_x = np.floor(rand_w*(w-t_w)).astype(int)\n",
    "        start_y = np.floor(rand_h*(h-t_h)).astype(int)\n",
    "        return TTF.crop(x, start_y, start_x, t_h, t_w), TTF.crop(y, start_y, start_x, t_h, t_w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_y(y_img):\n",
    "    yr = (y_img==ROADS) | (y_img==ROAD_LINES)\n",
    "    yc = (y_img==VEHICLES)\n",
    "    cutoff_y = int(yc.shape[0]*.875)\n",
    "    yc[cutoff_y:,:] = 0\n",
    "    rn = ~(yr | yc)\n",
    "    return torch.from_numpy(np.stack((rn,yc,yr)).astype(int))\n",
    "\n",
    "def xy_tensor(x,y):\n",
    "    y_img = np.array(y, np.int32, copy=False)\n",
    "    return TTF.to_tensor(x), convert_y(y_img[:,:,0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RRC(transforms.RandomResizedCrop):\n",
    "    def __call__(self, x, y):\n",
    "        i, j, h, w = self.get_params(x, self.scale, self.ratio)\n",
    "        x = TTF.resized_crop(x, i, j, h, w, self.size, self.interpolation)\n",
    "        y = TTF.resized_crop(y, i, j, h, w, self.size, self.interpolation)\n",
    "        return x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def torch_loader(f_ext, data_path, bs, size, workers=7, random_crop=False, pseudo_label=False, val_folder=None):\n",
    "    # Data loading code\n",
    "    x_names = np.sort(np.array(glob(str(data_path/f'CameraRGB{f_ext}'/'*.png'))))\n",
    "    y_names = np.sort(np.array(glob(str(data_path/f'CameraSeg{f_ext}'/'*.png'))))\n",
    "\n",
    "    x_n = x_names.shape[0]\n",
    "    val_idxs = list(range(x_n-300, x_n))\n",
    "    \n",
    "    if pseudo_label:\n",
    "        x_names_test = np.sort(np.array(glob(f'../data/pseudo/CameraRGB{f_ext}/*.png')))\n",
    "        y_names_test = np.sort(np.array(glob(f'../data/pseudo/CameraSeg{f_ext}/*.png')))\n",
    "        x_names = np.concatenate((x_names, x_names_test))\n",
    "        x_names = np.concatenate((y_names, y_names_test))\n",
    "        print(f'Pseudo-Labels: {len(x_names_test)}')\n",
    "    if val_folder:\n",
    "        x_names_val = np.sort(np.array(glob(f'../data/{val_folder}/CameraRGB{f_ext}/*.png')))\n",
    "        y_names_val = np.sort(np.array(glob(f'../data/{val_folder}/CameraSeg{f_ext}/*.png')))\n",
    "        val_x,val_y = x_names_val, y_names_val\n",
    "        trn_x,trn_y = x_names, y_names\n",
    "        print(f'Val Labels:', len(val_x))\n",
    "    else:\n",
    "        ((val_x,trn_x),(val_y,trn_y)) = split_by_idx(val_idxs, x_names, y_names)\n",
    "    print(f'Val x:{len(val_x)}, y:{len(val_y)}')\n",
    "    print(f'Trn x:{len(trn_x)}, y:{len(trn_y)}')\n",
    "    print(f'All x:{len(x_names)}')\n",
    "    normalize = transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "    \n",
    "    train_tfms = [\n",
    "        crop_bg_pil,\n",
    "        tfm_x_wrapper(transforms.ColorJitter(.2,.2,.2)),\n",
    "#         tfm_x_wrapper(Lighting(0.1, __imagenet_pca['eigval'], __imagenet_pca['eigvec'])),\n",
    "        RR(),\n",
    "        RHF(),\n",
    "#         RC((size,size)),\n",
    "        xy_tensor,\n",
    "        tfm_x_wrapper(normalize),\n",
    "    ]\n",
    "    if random_crop:\n",
    "        train_tfms.insert(3,RRC(size, scale=(0.4, 1.0)))\n",
    "    train_dataset = MatchedFilesDataset(trn_x, trn_y, train_tfms, path='')\n",
    "    train_loader = torch.utils.data.DataLoader(\n",
    "        train_dataset, batch_size=bs, shuffle=True,\n",
    "        num_workers=workers, pin_memory=True)\n",
    "\n",
    "    val_tfms = [\n",
    "        crop_bg_pil,\n",
    "        xy_tensor,\n",
    "        tfm_x_wrapper(normalize)\n",
    "    ]\n",
    "    val_dataset = MatchedFilesDataset(val_x, val_y, val_tfms, path='')\n",
    "    val_loader = torch.utils.data.DataLoader(\n",
    "        val_dataset, batch_size=bs, shuffle=False,\n",
    "        num_workers=workers, pin_memory=True)\n",
    "\n",
    "    train_loader = DataPrefetcher(train_loader)\n",
    "    val_loader = DataPrefetcher(val_loader)\n",
    "    \n",
    "    data = ModelData(data_path, train_loader, val_loader)\n",
    "    return data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def denorm(x):\n",
    "    x_np = x.cpu().numpy()\n",
    "    x_np = np.rollaxis(x_np, 0, 3)\n",
    "    mean=np.array([0.485, 0.456, 0.406])\n",
    "    std=np.array([0.229, 0.224, 0.225])\n",
    "    x_np = x_np*std+mean\n",
    "    return x_np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualize data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sz = 64\n",
    "bs = 2\n",
    "ext = '-150'\n",
    "md = torch_loader(ext, PATH, bs, sz, workers, random_crop, pseudo_label, val_folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x,y = md.val_ds[259]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(y[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_out = denorm(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(x_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x,y = md.trn_ds[259]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(y[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_out = denorm(x)\n",
    "plt.imshow(x_out)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## U-net (ish)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision.models import vgg11_bn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vgg11(pre): return children(vgg11_bn(pre))[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_meta = {\n",
    "    resnet18:[8,6], resnet34:[8,6], resnet50:[8,6], resnet101:[8,6], resnet152:[8,6],\n",
    "    vgg11:[0,13], vgg16:[0,22], vgg19:[0,22],\n",
    "    resnext50:[8,6], resnext101:[8,6], resnext101_64:[8,6],\n",
    "    wrn:[8,6], inceptionresnet_2:[-2,9], inception_4:[-1,9],\n",
    "    dn121:[0,7], dn161:[0,7], dn169:[0,7], dn201:[0,7],\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_base(f):\n",
    "    cut,lr_cut = model_meta[f]\n",
    "    layers = cut_model(f(True), cut)\n",
    "    return nn.Sequential(*layers), lr_cut"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SaveFeatures():\n",
    "    features=None\n",
    "    def __init__(self, m): self.hook = m.register_forward_hook(self.hook_fn)\n",
    "    def hook_fn(self, module, input, output): self.features = output\n",
    "    def remove(self): self.hook.remove()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "class UnetBlock(nn.Module):\n",
    "    def __init__(self, up_in, x_in, n_out):\n",
    "        super().__init__()\n",
    "        up_out = x_out = n_out//2\n",
    "        self.x_conv  = nn.Conv2d(x_in,  x_out,  1)\n",
    "        self.tr_conv = nn.ConvTranspose2d(up_in, up_out, 2, stride=2)\n",
    "        self.bn = nn.BatchNorm2d(n_out)\n",
    "        \n",
    "    def forward(self, up_p, x_p):\n",
    "        up_p = self.tr_conv(up_p)\n",
    "        x_p = self.x_conv(x_p)\n",
    "        cat_p = torch.cat([up_p,x_p], dim=1)\n",
    "        return self.bn(F.relu(cat_p, inplace=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Unet11Mini(nn.Module):\n",
    "    def __init__(self, f=vgg11):\n",
    "        super().__init__()\n",
    "        m_base, lr_cut = get_base(f)\n",
    "        self.rn = m_base\n",
    "        self.lr_cut = lr_cut\n",
    "        self.sfs = [SaveFeatures(self.rn[0][i]) for i in [2,6,13,20,27]]\n",
    "        self.up0 = UnetBlock(512,512,256)\n",
    "        self.up1 = UnetBlock(256,512,256)\n",
    "        self.up2 = UnetBlock(256,256,256)\n",
    "        self.up3 = UnetBlock(256,128,128)\n",
    "        self.up4 = UnetBlock(128,64,64)\n",
    "        self.up5  = nn.Conv2d(64,3,1)\n",
    "        \n",
    "    def forward(self,x):\n",
    "        x = F.relu(self.rn(x))\n",
    "        x = self.up0(x, self.sfs[4].features)\n",
    "        x = self.up1(x, self.sfs[3].features)\n",
    "        x = self.up2(x, self.sfs[2].features)\n",
    "        x = self.up3(x, self.sfs[1].features)\n",
    "        x = self.up4(x, self.sfs[0].features)\n",
    "        x = self.up5(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "class UnetModel():\n",
    "    def __init__(self,model,name='unet'):\n",
    "        self.model,self.name = model,name\n",
    "\n",
    "    def get_layer_groups(self, precompute):\n",
    "        if isinstance(self.model, FP16):\n",
    "            model = self.model.module\n",
    "        else:\n",
    "            model = self.model\n",
    "        lgs = list(split_by_idxs(children(model.rn), [model.lr_cut]))\n",
    "#         print('LGS:', lgs)\n",
    "#         print('Add:', children(model)[1:])\n",
    "        return lgs + [children(model)[1:]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def car_f_p_r(pred, targs):\n",
    "#     p2 = F.sigmoid(pred)\n",
    "    _,idx = torch.max(pred, 1)\n",
    "    p2 = idx\n",
    "    return fbeta_score(p2==1, targs[:,1,:,:], beta=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rd_f(pred, targs):\n",
    "    _,idx = torch.max(pred, 1)\n",
    "#     p2 = F.sigmoid(pred)\n",
    "    p2 = idx\n",
    "    f,p,r = fbeta_score(p2==2, targs[:,2,:,:], beta=0.5)\n",
    "    return f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fbeta_score(y_pred, y_true, beta, threshold=None, eps=1e-9):\n",
    "    beta2 = beta**2\n",
    "\n",
    "    if threshold:\n",
    "        y_pred = torch.ge(y_pred.float(), threshold).float()\n",
    "    else:\n",
    "        y_pred = y_pred.float()\n",
    "    y_true = y_true.float()\n",
    "\n",
    "    true_positive = (y_pred * y_true).sum()\n",
    "    precision = true_positive/(y_pred.sum()+(eps))\n",
    "    recall = true_positive/(y_true.sum()+eps)\n",
    "    \n",
    "    fb = (precision*recall)/(precision*beta2 + recall + eps)*(1+beta2)\n",
    "    \n",
    "    return fb, precision, recall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def new_acc(pred, targs):\n",
    "    _,idx = torch.max(pred, 1)\n",
    "    _,t_idx = torch.max(targs,1)\n",
    "#     idx = F.softmax(pred) > 0.5\n",
    "    return (idx == t_idx).float().mean()\n",
    "def dice_mult(pred, targs):\n",
    "#     pred = (pred>0).float()\n",
    "    mx,idx = torch.max(pred, 1)\n",
    "    pred = idx.float()\n",
    "    targs = targs.float()\n",
    "    return 2. * (pred*targs).sum() / (pred+targs).sum()\n",
    "def dice(pred, targs):\n",
    "    pred = (pred>0).float()\n",
    "    return 2. * (pred*targs).sum() / (pred+targs).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dice_coeff_weight(pred, target, weight):\n",
    "    smooth = 1.\n",
    "    num,c,h,w = pred.shape\n",
    "    m1 = pred.view(num, c, -1)  # Flatten\n",
    "    m2 = target.view(num, c, -1)  # Flatten\n",
    "    intersection = (m1 * m2)\n",
    "    w = V(weight.view(1,-1,1))\n",
    "    i_w = (w*intersection).sum()\n",
    "    m1_w = (w*m1).sum()\n",
    "    m2_w = (w*m2).sum()\n",
    "    return (2. * i_w + smooth) / (m1_w + m2_w + smooth)\n",
    "\n",
    "def dice_coeff(pred, target):\n",
    "    smooth = 1.\n",
    "    num,c,h,w = pred.shape\n",
    "    m1 = pred.view(num, c, -1)  # Flatten\n",
    "    m2 = target.view(num, c, -1)  # Flatten\n",
    "    intersection = (m1 * m2).sum()\n",
    "    return (2. * intersection + smooth) / (m1.sum() + m2.sum() + smooth)\n",
    "\n",
    "\n",
    "class SoftDiceLoss(nn.Module):\n",
    "    def __init__(self, weight=None, size_average=True):\n",
    "        super(SoftDiceLoss, self).__init__()\n",
    "        self.weight = weight\n",
    "\n",
    "    def forward(self, logits, targets):\n",
    "        probs = F.softmax(logits)\n",
    "        num = targets.size(0)  # Number of batches\n",
    "        if isinstance(logits.data, torch.cuda.HalfTensor):\n",
    "            targets = targets.half()\n",
    "        else:\n",
    "            targets = targets.float()\n",
    "            \n",
    "        if self.weight is not None:\n",
    "            score = dice_coeff_weight(probs, targets, self.weight)\n",
    "        else:\n",
    "            score = dice_coeff(probs, targets)\n",
    "        score = 1 - score.sum() / num\n",
    "        return score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_learner(md, m_fn=Unet11Mini, half=False):\n",
    "    m = to_gpu(m_fn())\n",
    "    models = UnetModel(m)\n",
    "    learn = ConvLearner(md, models)\n",
    "    learn.opt_fn=optim.Adam\n",
    "    class_weights = torch.cuda.FloatTensor([1,500,1])\n",
    "    if half:\n",
    "        class_weights = class_weights.half()\n",
    "        learn.half()\n",
    "#     learn.crit=nn.CrossEntropyLoss(weight=class_weights)\n",
    "    learn.crit=SoftDiceLoss(weight=class_weights)\n",
    "    learn.metrics=[new_acc, rd_f, car_f_p_r]\n",
    "    return learn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val Labels: 400\n",
      "Val x:400, y:400\n",
      "Trn x:7280, y:7280\n",
      "All x:7280\n",
      " 68%|██████▊   | 39/57 [00:20<00:09,  1.95it/s, loss=0.994]"
     ]
    }
   ],
   "source": [
    "ext = '-150'\n",
    "sz = 96\n",
    "bs = 128\n",
    "md = torch_loader(ext, PATH, bs, sz, workers, random_crop, pseudo_label, val_folder)\n",
    "\n",
    "learn = get_learner(md)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# learn.half()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.freeze_to(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %pdb on"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.lr_find()\n",
    "learn.sched.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr=5e-3\n",
    "wd=1e-5\n",
    "\n",
    "lrs = np.array([lr/200,lr/20,lr])/2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8ab07adae2a5457f93554dfada38031b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Epoch', max=30), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch      trn_loss   val_loss   new_acc    rd_f       car_f_p_r  \n",
      "    0      0.999308   0.999175   0.654409   0.091089   0.455964   0.199536   0.674342  \n",
      "    1      0.998741   0.997782   0.683885   0.51279    0.656571   0.444118   0.747819  \n",
      "    2      0.996243   0.993388   0.699461   0.531565   0.672513   0.671262   0.673726  \n",
      "    3      0.994777   0.99333    0.768737   0.623807   0.636338   0.731994   0.617167  \n",
      "    4      0.99413    0.993124   0.864077   0.76889    0.629107   0.73826    0.608033  \n",
      "    5      0.993826   0.992989   0.917855   0.909471   0.616429   0.777742   0.587335  \n",
      "    6      0.99369    0.992704   0.937686   0.910545   0.643082   0.769032   0.619094  \n",
      "    7      0.993622   0.993046   0.942142   0.923247   0.580268   0.825452   0.541584  \n",
      "    8      0.993584   0.992805   0.941233   0.93642    0.628764   0.797974   0.598617  \n",
      "    9      0.993539   0.993054   0.938702   0.931703   0.586964   0.832595   0.548734  \n",
      "    10     0.993526   0.992577   0.941848   0.907246   0.706654   0.700766   0.70877   \n",
      "    11     0.993506   0.992718   0.947138   0.93475    0.637027   0.800734   0.607428  \n",
      "    12     0.993552   0.992637   0.934852   0.935248   0.662261   0.773688   0.640625  \n",
      "    13     0.993537   0.9929     0.945531   0.933101   0.599046   0.787555   0.566708  \n",
      "    14     0.99352    0.993355   0.948507   0.926831   0.533341   0.847051   0.489452  \n",
      "    15     0.99351    0.992559   0.946298   0.928668   0.683596   0.737184   0.6725    \n",
      "    16     0.993484   0.992689   0.948181   0.938673   0.645653   0.785319   0.619382  \n",
      "    17     0.993458   0.992588   0.946208   0.936052   0.695875   0.752775   0.684608  \n",
      "    18     0.993464   0.992816   0.951213   0.940062   0.588438   0.837083   0.548884  \n",
      "    19     0.993453   0.992611   0.946734   0.932858   0.655696   0.784005   0.631311  \n",
      "    20     0.993419   0.992679   0.949864   0.931687   0.647953   0.770808   0.624843  \n",
      "    21     0.993408   0.992586   0.9462     0.933781   0.688426   0.728627   0.680858  \n",
      "    22     0.993383   0.992592   0.949884   0.94501    0.660697   0.801685   0.634938  \n",
      "    23     0.993354   0.992704   0.947277   0.943719   0.645327   0.799766   0.617519  \n",
      "    24     0.993338   0.992516   0.950545   0.935912   0.678186   0.777638   0.658871  \n",
      "    25     0.993322   0.992508   0.952178   0.944833   0.663454   0.806183   0.636965  \n",
      "    26     0.993317   0.992435   0.949846   0.945032   0.677473   0.798663   0.654616  \n",
      "    27     0.993303   0.992449   0.950884   0.945478   0.675535   0.79835    0.652438  \n",
      "    28     0.993282   0.992379   0.952757   0.944655   0.690718   0.792593   0.671043  \n",
      "    29     0.99327    0.992422   0.952111   0.945877   0.676611   0.808452   0.651994  \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[array([0.99242]),\n",
       " 0.9521105456352233,\n",
       " 0.9458767799410359,\n",
       " 0.6766112687594098,\n",
       " 0.808452305137957,\n",
       " 0.6519942356551464]"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "learn.fit(lr,1,wds=wd,cycle_len=30,use_clr_beta=(20,10,0.95,0.85))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.save(f'128urn-{S_PREFIX}-tmp')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.load(f'128urn-{S_PREFIX}-tmp')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.unfreeze()\n",
    "learn.bn_freeze(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "955268aedd9a4bbaa7fd193e2e9cdd98",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Epoch', max=30), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch      trn_loss   val_loss   new_acc    rd_f       car_f_p_r  \n",
      "    0      0.993236   0.992335   0.955284   0.947272   0.700695   0.782364   0.684485  \n",
      "    1      0.993236   0.992244   0.952279   0.948719   0.716147   0.782978   0.702756  \n",
      "    2      0.993214   0.992333   0.954356   0.951248   0.682087   0.81163    0.657552  \n",
      "    3      0.993182   0.992364   0.945698   0.95101    0.675439   0.837289   0.645922  \n",
      "    4      0.993158   0.992074   0.958257   0.955926   0.717931   0.811415   0.699273  \n",
      "    5      0.993151   0.99214    0.96206    0.954897   0.714143   0.835035   0.690738  \n",
      "    6      0.993145   0.992065   0.957849   0.95418    0.729136   0.805441   0.713522  \n",
      "    7      0.993139   0.992106   0.955676   0.95645    0.706368   0.828214   0.682469  \n",
      "    8      0.993142   0.991955   0.957887   0.961853   0.757753   0.798148   0.749693  \n",
      "    9      0.993127   0.991885   0.961931   0.954975   0.777492   0.776974   0.779045  \n",
      "    10     0.993116   0.991958   0.961936   0.953755   0.727384   0.831955   0.706474  \n",
      "    11     0.993112   0.991961   0.962213   0.959201   0.762749   0.783238   0.759719  \n",
      "    12     0.993116   0.992248   0.961898   0.956532   0.644917   0.90026    0.602962  \n",
      "    13     0.993119   0.99216    0.963205   0.958007   0.686934   0.868277   0.653989  \n",
      "    14     0.993098   0.991875   0.964461   0.965259   0.760721   0.803899   0.751797  \n",
      "    15     0.993072   0.99185    0.964388   0.963458   0.766247   0.805367   0.758436  \n",
      "    16     0.993053   0.991892   0.964081   0.960161   0.730557   0.859517   0.704968  \n",
      "    17     0.993047   0.991772   0.967603   0.956601   0.76004    0.83911    0.743525  \n",
      "    18     0.993031   0.991746   0.966303   0.964489   0.768533   0.837847   0.753981  \n",
      "    19     0.993018   0.991793   0.967064   0.964753   0.754608   0.852271   0.734412  \n",
      "    20     0.993008   0.991756   0.968163   0.966291   0.764763   0.837129   0.749592  \n",
      "    21     0.992994   0.991754   0.966986   0.968228   0.76493    0.836648   0.749902  \n",
      "    22     0.992978   0.991711   0.97014    0.967426   0.768111   0.84226    0.7523    \n",
      "    23     0.992985   0.991709   0.968832   0.966477   0.773778   0.840983   0.759692  \n",
      "    24     0.992984   0.991732   0.969162   0.969886   0.76129    0.8562     0.741649  \n",
      "    25     0.992966   0.991712   0.967943   0.97029    0.767274   0.854039   0.749422  \n",
      "    26     0.992947   0.99167    0.969658   0.969508   0.774307   0.847557   0.759018  \n",
      "    27     0.992951   0.991666   0.971569   0.970021   0.783254   0.834424   0.772794  \n",
      "    28     0.992937   0.99167    0.970876   0.969954   0.777912   0.845585   0.763963  \n",
      "    29     0.992921   0.991658   0.97123    0.969257   0.779922   0.843189   0.766836  \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[array([0.99166]),\n",
       " 0.9712297582626342,\n",
       " 0.9692567885657465,\n",
       " 0.7799215876153395,\n",
       " 0.8431893086359818,\n",
       " 0.7668359723788067]"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "learn.fit(lrs,1,wds=wd,cycle_len=30,use_clr_beta=(20,10,0.95,0.85))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.save(f'128urn-{S_PREFIX}-0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.load(f'128urn-{S_PREFIX}-0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x,y = next(iter(md.val_dl))\n",
    "py = to_np(learn.model(V(x[:10])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# py = np.argmax(py,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_img(denorm(x[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "py.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_img(py[0][0]>0.5);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_img(py[0][1]>0.5);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_img(y[0][1]);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 256x256"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val Labels: 400\n",
      "Val x:400, y:400\n",
      "Trn x:7280, y:7280\n",
      "All x:7280\n"
     ]
    }
   ],
   "source": [
    "ext = '-300'\n",
    "sz=192\n",
    "bs=64\n",
    "md = torch_loader(ext, PATH, bs, sz, workers, random_crop, pseudo_label, val_folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr=1e-2\n",
    "wd=1e-6\n",
    "\n",
    "lrs = np.array([lr/200,lr/20,lr])/2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import gc\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn = get_learner(md)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.load(f'128urn-{S_PREFIX}-0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.freeze_to(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.lr_find()\n",
    "learn.sched.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "learn.load(f'128urn-{S_PREFIX}-0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9f993e86b90f4f508292a1521e4dc086",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Epoch', max=30), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch      trn_loss   val_loss   new_acc    rd_f       car_f_p_r  \n",
      "    0      0.992921   0.988144   0.805895   0.682862   0.75425    0.826089   0.739593  \n",
      "    1      0.987261   0.986115   0.939412   0.93894    0.716518   0.881772   0.685985  \n",
      "    2      0.986358   0.985823   0.945986   0.943079   0.734102   0.87631    0.707008  \n",
      "    3      0.986212   0.985649   0.944006   0.946519   0.75232    0.871318   0.728938  \n",
      "    4      0.986187   0.985898   0.945126   0.925124   0.713381   0.894246   0.680218  \n",
      "    5      0.986126   0.985627   0.948616   0.935463   0.744225   0.886477   0.716543  \n",
      "    6      0.986133   0.985758   0.949417   0.938549   0.73892    0.864659   0.71385   \n",
      "    7      0.986146   0.985626   0.94506    0.919119   0.744544   0.883927   0.717354  \n",
      "    8      0.986095   0.985431   0.949904   0.928103   0.784234   0.853908   0.769459  \n",
      "    9      0.986022   0.985473   0.946883   0.926345   0.764491   0.880697   0.74087   \n",
      "    10     0.9861     0.985685   0.944123   0.920935   0.735632   0.897641   0.704404  \n",
      "    11     0.986065   0.98573    0.938616   0.930925   0.742785   0.882628   0.715162  \n",
      "    12     0.986118   0.986102   0.932266   0.930411   0.683544   0.92335    0.642758  \n",
      "    13     0.986022   0.985557   0.94688    0.92783    0.749164   0.886759   0.722151  \n",
      "    14     0.986034   0.985856   0.935579   0.932277   0.711944   0.910639   0.675947  \n",
      "    15     0.985986   0.985595   0.942416   0.920225   0.76317    0.868263   0.741762  \n",
      "    16     0.985995   0.985668   0.937971   0.937617   0.740867   0.890672   0.711848  \n",
      "    17     0.985965   0.98573    0.947619   0.93606    0.727774   0.8989     0.695805  \n",
      "    18     0.985987   0.985629   0.944846   0.935689   0.758618   0.861807   0.737573  \n",
      "    19     0.985988   0.985709   0.941883   0.939642   0.771521   0.829424   0.759159  \n",
      "    20     0.985968   0.985914   0.944615   0.939525   0.735015   0.852989   0.711402  \n",
      "    21     0.985937   0.986078   0.949128   0.939591   0.741636   0.813074   0.726712  \n",
      "    22     0.985931   0.985897   0.94782    0.921429   0.739786   0.846951   0.718084  \n",
      "    23     0.985916   0.985752   0.949179   0.943202   0.747569   0.858748   0.725107  \n",
      "    24     0.985878   0.986468   0.948589   0.938009   0.74135    0.74857    0.740634  \n",
      "    25     0.98583    0.985944   0.949987   0.936595   0.750592   0.81304    0.738197  \n",
      "    26     0.985817   0.985971   0.951414   0.944402   0.750514   0.81128    0.738157  \n",
      "    27     0.985783   0.98613    0.95068    0.942196   0.749156   0.784875   0.742334  \n",
      "    28     0.985803   0.98624    0.950537   0.940851   0.745633   0.773633   0.740462  \n",
      "    29     0.985796   0.986211   0.950912   0.942561   0.749538   0.771857   0.745721  \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[array([0.98621]),\n",
       " 0.9509122443199157,\n",
       " 0.9425609811416956,\n",
       " 0.7495383307133763,\n",
       " 0.7718567619958732,\n",
       " 0.7457207154466802]"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "learn.fit(lr,1,wds=wd,cycle_len=30,use_clr_beta=(20,10,0.95,0.85))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.save(f'256urn-{S_PREFIX}-tmp')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.load(f'256urn-{S_PREFIX}-tmp')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.unfreeze()\n",
    "learn.bn_freeze(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "53ee3a50668b43be991cc4bdf21dabd3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Epoch', max=30), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch      trn_loss   val_loss   new_acc    rd_f       car_f_p_r  \n",
      "    0      0.985762   0.986153   0.951144   0.943055   0.750216   0.780454   0.74453   \n",
      "    1      0.98578    0.986279   0.950603   0.941715   0.741742   0.771281   0.736404  \n",
      "    2      0.985767   0.986238   0.952571   0.941306   0.746478   0.7709     0.742119  \n",
      "    3      0.985756   0.98633    0.951546   0.94596    0.747028   0.751631   0.747522  \n",
      "    4      0.985743   0.986129   0.952989   0.941553   0.7453     0.786567   0.737305  \n",
      "    5      0.985774   0.986098   0.953926   0.94257    0.749903   0.785816   0.742964  \n",
      "    6      0.985757   0.98572    0.955499   0.945851   0.749345   0.851534   0.728891  \n",
      "    7      0.985752   0.986349   0.95424    0.942021   0.741463   0.759774   0.738776  \n",
      "    8      0.985751   0.985834   0.955304   0.944609   0.756265   0.819051   0.743736  \n",
      "    9      0.985756   0.985961   0.954295   0.946129   0.741333   0.820721   0.725578  \n",
      "    10     0.985747   0.986663   0.952623   0.94997    0.747626   0.704079   0.761412  \n",
      " 85%|████████▌ | 97/114 [00:16<00:02,  5.98it/s, loss=0.986]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Process Process-1620:\n",
      "Process Process-1621:\n",
      "Process Process-1623:\n",
      "Process Process-1624:\n",
      "Process Process-1619:\n",
      "Process Process-1618:\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/paperspace/anaconda3/envs/lyft/lib/python3.6/multiprocessing/process.py\", line 258, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/home/paperspace/anaconda3/envs/lyft/lib/python3.6/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/paperspace/anaconda3/envs/lyft/lib/python3.6/multiprocessing/process.py\", line 258, in _bootstrap\n",
      "    self.run()\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/paperspace/anaconda3/envs/lyft/lib/python3.6/site-packages/torch/utils/data/dataloader.py\", line 55, in _worker_loop\n",
      "    samples = collate_fn([dataset[i] for i in batch_indices])\n",
      "  File \"/home/paperspace/anaconda3/envs/lyft/lib/python3.6/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/paperspace/anaconda3/envs/lyft/lib/python3.6/site-packages/torch/utils/data/dataloader.py\", line 55, in <listcomp>\n",
      "    samples = collate_fn([dataset[i] for i in batch_indices])\n",
      "  File \"/home/paperspace/anaconda3/envs/lyft/lib/python3.6/multiprocessing/process.py\", line 258, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/home/paperspace/anaconda3/envs/lyft/lib/python3.6/multiprocessing/process.py\", line 258, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/home/paperspace/anaconda3/envs/lyft/lib/python3.6/multiprocessing/process.py\", line 258, in _bootstrap\n",
      "    self.run()\n",
      "Traceback (most recent call last):\n",
      "  File \"<ipython-input-9-e4a515af82db>\", line 25, in __getitem__\n",
      "    x,y = self.get_x(idx),self.get_y(idx)\n",
      "  File \"/home/paperspace/anaconda3/envs/lyft/lib/python3.6/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/paperspace/anaconda3/envs/lyft/lib/python3.6/site-packages/torch/utils/data/dataloader.py\", line 55, in _worker_loop\n",
      "    samples = collate_fn([dataset[i] for i in batch_indices])\n",
      "  File \"/home/paperspace/anaconda3/envs/lyft/lib/python3.6/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/paperspace/anaconda3/envs/lyft/lib/python3.6/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/paperspace/anaconda3/envs/lyft/lib/python3.6/site-packages/torch/utils/data/dataloader.py\", line 55, in _worker_loop\n",
      "    samples = collate_fn([dataset[i] for i in batch_indices])\n",
      "  File \"/home/paperspace/anaconda3/envs/lyft/lib/python3.6/multiprocessing/process.py\", line 258, in _bootstrap\n",
      "    self.run()\n",
      "Process Process-1622:\n",
      "  File \"/home/paperspace/anaconda3/envs/lyft/lib/python3.6/site-packages/torch/utils/data/dataloader.py\", line 55, in <listcomp>\n",
      "    samples = collate_fn([dataset[i] for i in batch_indices])\n",
      "  File \"/home/paperspace/anaconda3/envs/lyft/lib/python3.6/site-packages/torch/utils/data/dataloader.py\", line 55, in <listcomp>\n",
      "    samples = collate_fn([dataset[i] for i in batch_indices])\n",
      "  File \"<ipython-input-9-e4a515af82db>\", line 13, in get_x\n",
      "    def get_x(self, i): return self.open_fn(os.path.join(self.path, self.fnames[i]))\n",
      "  File \"/home/paperspace/anaconda3/envs/lyft/lib/python3.6/site-packages/torch/utils/data/dataloader.py\", line 55, in _worker_loop\n",
      "    samples = collate_fn([dataset[i] for i in batch_indices])\n",
      "  File \"/home/paperspace/anaconda3/envs/lyft/lib/python3.6/site-packages/torch/utils/data/dataloader.py\", line 55, in _worker_loop\n",
      "    samples = collate_fn([dataset[i] for i in batch_indices])\n",
      "  File \"/home/paperspace/anaconda3/envs/lyft/lib/python3.6/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/paperspace/anaconda3/envs/lyft/lib/python3.6/site-packages/torchvision/datasets/folder.py\", line 52, in pil_loader\n",
      "    return img.convert('RGB')\n",
      "  File \"/home/paperspace/anaconda3/envs/lyft/lib/python3.6/site-packages/torch/utils/data/dataloader.py\", line 55, in <listcomp>\n",
      "    samples = collate_fn([dataset[i] for i in batch_indices])\n",
      "  File \"/home/paperspace/anaconda3/envs/lyft/lib/python3.6/site-packages/torch/utils/data/dataloader.py\", line 55, in _worker_loop\n",
      "    samples = collate_fn([dataset[i] for i in batch_indices])\n",
      "  File \"<ipython-input-9-e4a515af82db>\", line 25, in __getitem__\n",
      "    x,y = self.get_x(idx),self.get_y(idx)\n",
      "  File \"<ipython-input-9-e4a515af82db>\", line 25, in __getitem__\n",
      "    x,y = self.get_x(idx),self.get_y(idx)\n",
      "  File \"/home/paperspace/anaconda3/envs/lyft/lib/python3.6/site-packages/torch/utils/data/dataloader.py\", line 55, in <listcomp>\n",
      "    samples = collate_fn([dataset[i] for i in batch_indices])\n",
      "  File \"/home/paperspace/anaconda3/envs/lyft/lib/python3.6/site-packages/torch/utils/data/dataloader.py\", line 55, in <listcomp>\n",
      "    samples = collate_fn([dataset[i] for i in batch_indices])\n",
      "  File \"<ipython-input-9-e4a515af82db>\", line 25, in __getitem__\n",
      "    x,y = self.get_x(idx),self.get_y(idx)\n",
      "  File \"<ipython-input-9-e4a515af82db>\", line 13, in get_x\n",
      "    def get_x(self, i): return self.open_fn(os.path.join(self.path, self.fnames[i]))\n",
      "  File \"<ipython-input-9-e4a515af82db>\", line 25, in __getitem__\n",
      "    x,y = self.get_x(idx),self.get_y(idx)\n",
      "  File \"<ipython-input-9-e4a515af82db>\", line 13, in get_x\n",
      "    def get_x(self, i): return self.open_fn(os.path.join(self.path, self.fnames[i]))\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/paperspace/anaconda3/envs/lyft/lib/python3.6/site-packages/PIL/Image.py\", line 879, in convert\n",
      "    self.load()\n",
      "  File \"<ipython-input-9-e4a515af82db>\", line 25, in __getitem__\n",
      "    x,y = self.get_x(idx),self.get_y(idx)\n",
      "  File \"<ipython-input-9-e4a515af82db>\", line 13, in get_x\n",
      "    def get_x(self, i): return self.open_fn(os.path.join(self.path, self.fnames[i]))\n",
      "  File \"/home/paperspace/anaconda3/envs/lyft/lib/python3.6/multiprocessing/process.py\", line 258, in _bootstrap\n",
      "    self.run()\n",
      "  File \"<ipython-input-9-e4a515af82db>\", line 13, in get_x\n",
      "    def get_x(self, i): return self.open_fn(os.path.join(self.path, self.fnames[i]))\n",
      "  File \"/home/paperspace/anaconda3/envs/lyft/lib/python3.6/site-packages/torchvision/datasets/folder.py\", line 52, in pil_loader\n",
      "    return img.convert('RGB')\n",
      "  File \"/home/paperspace/anaconda3/envs/lyft/lib/python3.6/site-packages/torchvision/datasets/folder.py\", line 52, in pil_loader\n",
      "    return img.convert('RGB')\n",
      "  File \"/home/paperspace/anaconda3/envs/lyft/lib/python3.6/site-packages/PIL/ImageFile.py\", line 231, in load\n",
      "    n, err_code = decoder.decode(b)\n",
      "Exception in thread Thread-352:\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/paperspace/anaconda3/envs/lyft/lib/python3.6/threading.py\", line 916, in _bootstrap_inner\n",
      "    self.run()\n",
      "  File \"/home/paperspace/anaconda3/envs/lyft/lib/python3.6/threading.py\", line 864, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/paperspace/anaconda3/envs/lyft/lib/python3.6/site-packages/torch/utils/data/dataloader.py\", line 68, in _worker_manager_loop\n",
      "    r = in_queue.get()\n",
      "  File \"/home/paperspace/anaconda3/envs/lyft/lib/python3.6/multiprocessing/queues.py\", line 337, in get\n",
      "    return _ForkingPickler.loads(res)\n",
      "  File \"/home/paperspace/anaconda3/envs/lyft/lib/python3.6/site-packages/torch/multiprocessing/reductions.py\", line 70, in rebuild_storage_fd\n",
      "    fd = df.detach()\n",
      "  File \"/home/paperspace/anaconda3/envs/lyft/lib/python3.6/multiprocessing/resource_sharer.py\", line 57, in detach\n",
      "    with _resource_sharer.get_connection(self._id) as conn:\n",
      "  File \"/home/paperspace/anaconda3/envs/lyft/lib/python3.6/multiprocessing/resource_sharer.py\", line 87, in get_connection\n",
      "    c = Client(address, authkey=process.current_process().authkey)\n",
      "  File \"/home/paperspace/anaconda3/envs/lyft/lib/python3.6/multiprocessing/connection.py\", line 487, in Client\n",
      "    c = SocketClient(address)\n",
      "  File \"/home/paperspace/anaconda3/envs/lyft/lib/python3.6/multiprocessing/connection.py\", line 614, in SocketClient\n",
      "    s.connect(address)\n",
      "FileNotFoundError: [Errno 2] No such file or directory\n",
      "\n",
      "ERROR:root:Internal Python error in the inspect module.\n",
      "Below is the traceback from this internal error.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  File \"<ipython-input-9-e4a515af82db>\", line 13, in get_x\n",
      "    def get_x(self, i): return self.open_fn(os.path.join(self.path, self.fnames[i]))\n",
      "  File \"/home/paperspace/anaconda3/envs/lyft/lib/python3.6/site-packages/torchvision/datasets/folder.py\", line 52, in pil_loader\n",
      "    return img.convert('RGB')\n",
      "  File \"/home/paperspace/anaconda3/envs/lyft/lib/python3.6/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/paperspace/anaconda3/envs/lyft/lib/python3.6/site-packages/torchvision/datasets/folder.py\", line 52, in pil_loader\n",
      "    return img.convert('RGB')\n",
      "  File \"/home/paperspace/anaconda3/envs/lyft/lib/python3.6/site-packages/PIL/Image.py\", line 879, in convert\n",
      "    self.load()\n",
      "KeyboardInterrupt\n",
      "  File \"/home/paperspace/anaconda3/envs/lyft/lib/python3.6/site-packages/PIL/Image.py\", line 879, in convert\n",
      "    self.load()\n",
      "  File \"/home/paperspace/anaconda3/envs/lyft/lib/python3.6/site-packages/torch/utils/data/dataloader.py\", line 55, in _worker_loop\n",
      "    samples = collate_fn([dataset[i] for i in batch_indices])\n",
      "  File \"/home/paperspace/anaconda3/envs/lyft/lib/python3.6/site-packages/PIL/Image.py\", line 879, in convert\n",
      "    self.load()\n",
      "  File \"/home/paperspace/anaconda3/envs/lyft/lib/python3.6/site-packages/PIL/Image.py\", line 879, in convert\n",
      "    self.load()\n",
      "  File \"/home/paperspace/anaconda3/envs/lyft/lib/python3.6/site-packages/PIL/ImageFile.py\", line 231, in load\n",
      "    n, err_code = decoder.decode(b)\n",
      "  File \"/home/paperspace/anaconda3/envs/lyft/lib/python3.6/site-packages/torchvision/datasets/folder.py\", line 52, in pil_loader\n",
      "    return img.convert('RGB')\n",
      "  File \"/home/paperspace/anaconda3/envs/lyft/lib/python3.6/site-packages/PIL/ImageFile.py\", line 231, in load\n",
      "    n, err_code = decoder.decode(b)\n",
      "  File \"/home/paperspace/anaconda3/envs/lyft/lib/python3.6/site-packages/PIL/ImageFile.py\", line 231, in load\n",
      "    n, err_code = decoder.decode(b)\n",
      "  File \"/home/paperspace/anaconda3/envs/lyft/lib/python3.6/site-packages/torch/utils/data/dataloader.py\", line 55, in <listcomp>\n",
      "    samples = collate_fn([dataset[i] for i in batch_indices])\n",
      "  File \"/home/paperspace/anaconda3/envs/lyft/lib/python3.6/site-packages/PIL/Image.py\", line 879, in convert\n",
      "    self.load()\n",
      "  File \"<ipython-input-9-e4a515af82db>\", line 25, in __getitem__\n",
      "    x,y = self.get_x(idx),self.get_y(idx)\n",
      "KeyboardInterrupt\n",
      "KeyboardInterrupt\n",
      "  File \"/home/paperspace/anaconda3/envs/lyft/lib/python3.6/site-packages/PIL/ImageFile.py\", line 231, in load\n",
      "    n, err_code = decoder.decode(b)\n",
      "  File \"<ipython-input-9-e4a515af82db>\", line 13, in get_x\n",
      "    def get_x(self, i): return self.open_fn(os.path.join(self.path, self.fnames[i]))\n",
      "  File \"/home/paperspace/anaconda3/envs/lyft/lib/python3.6/site-packages/PIL/ImageFile.py\", line 231, in load\n",
      "    n, err_code = decoder.decode(b)\n",
      "KeyboardInterrupt\n",
      "KeyboardInterrupt\n",
      "KeyboardInterrupt\n",
      "  File \"/home/paperspace/anaconda3/envs/lyft/lib/python3.6/site-packages/torchvision/datasets/folder.py\", line 52, in pil_loader\n",
      "    return img.convert('RGB')\n",
      "  File \"/home/paperspace/anaconda3/envs/lyft/lib/python3.6/site-packages/PIL/Image.py\", line 879, in convert\n",
      "    self.load()\n",
      "  File \"/home/paperspace/anaconda3/envs/lyft/lib/python3.6/site-packages/PIL/ImageFile.py\", line 231, in load\n",
      "    n, err_code = decoder.decode(b)\n",
      "KeyboardInterrupt\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/home/paperspace/anaconda3/envs/lyft/lib/python3.6/site-packages/IPython/core/interactiveshell.py\", line 2963, in run_code\n",
      "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "  File \"<ipython-input-63-bb8e1bf64ab7>\", line 1, in <module>\n",
      "    learn.fit(lrs/6,1,wds=wd,cycle_len=30,use_clr_beta=(20,10,0.95,0.85))\n",
      "  File \"/home/paperspace/anaconda3/envs/lyft/lib/python3.6/site-packages/fastai/learner.py\", line 287, in fit\n",
      "    return self.fit_gen(self.model, self.data, layer_opt, n_cycle, **kwargs)\n",
      "  File \"/home/paperspace/anaconda3/envs/lyft/lib/python3.6/site-packages/fastai/learner.py\", line 234, in fit_gen\n",
      "    swa_eval_freq=swa_eval_freq, **kwargs)\n",
      "  File \"/home/paperspace/anaconda3/envs/lyft/lib/python3.6/site-packages/fastai/model.py\", line 132, in fit\n",
      "    loss = model_stepper.step(V(x),V(y), epoch)\n",
      "  File \"/home/paperspace/anaconda3/envs/lyft/lib/python3.6/site-packages/fastai/model.py\", line 68, in step\n",
      "    return torch_item(raw_loss.data)\n",
      "  File \"/home/paperspace/anaconda3/envs/lyft/lib/python3.6/site-packages/fastai/model.py\", line 31, in torch_item\n",
      "    def torch_item(x): return x.item() if hasattr(x,'item') else x[0]\n",
      "KeyboardInterrupt\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/paperspace/anaconda3/envs/lyft/lib/python3.6/site-packages/IPython/core/interactiveshell.py\", line 1863, in showtraceback\n",
      "    stb = value._render_traceback_()\n",
      "AttributeError: 'KeyboardInterrupt' object has no attribute '_render_traceback_'\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/paperspace/anaconda3/envs/lyft/lib/python3.6/site-packages/IPython/core/ultratb.py\", line 1095, in get_records\n",
      "    return _fixed_getinnerframes(etb, number_of_lines_of_context, tb_offset)\n",
      "  File \"/home/paperspace/anaconda3/envs/lyft/lib/python3.6/site-packages/IPython/core/ultratb.py\", line 311, in wrapped\n",
      "    return f(*args, **kwargs)\n",
      "  File \"/home/paperspace/anaconda3/envs/lyft/lib/python3.6/site-packages/IPython/core/ultratb.py\", line 345, in _fixed_getinnerframes\n",
      "    records = fix_frame_records_filenames(inspect.getinnerframes(etb, context))\n",
      "  File \"/home/paperspace/anaconda3/envs/lyft/lib/python3.6/inspect.py\", line 1483, in getinnerframes\n",
      "    frameinfo = (tb.tb_frame,) + getframeinfo(tb, context)\n",
      "  File \"/home/paperspace/anaconda3/envs/lyft/lib/python3.6/inspect.py\", line 1441, in getframeinfo\n",
      "    filename = getsourcefile(frame) or getfile(frame)\n",
      "  File \"/home/paperspace/anaconda3/envs/lyft/lib/python3.6/inspect.py\", line 696, in getsourcefile\n",
      "    if getattr(getmodule(object, filename), '__loader__', None) is not None:\n",
      "  File \"/home/paperspace/anaconda3/envs/lyft/lib/python3.6/inspect.py\", line 742, in getmodule\n",
      "    os.path.realpath(f)] = module.__name__\n",
      "  File \"/home/paperspace/anaconda3/envs/lyft/lib/python3.6/posixpath.py\", line 388, in realpath\n",
      "    path, ok = _joinrealpath(filename[:0], filename, {})\n",
      "  File \"/home/paperspace/anaconda3/envs/lyft/lib/python3.6/posixpath.py\", line 422, in _joinrealpath\n",
      "    if not islink(newpath):\n",
      "  File \"/home/paperspace/anaconda3/envs/lyft/lib/python3.6/posixpath.py\", line 171, in islink\n",
      "    st = os.lstat(path)\n",
      "  File \"/home/paperspace/anaconda3/envs/lyft/lib/python3.6/site-packages/torch/utils/data/dataloader.py\", line 175, in handler\n",
      "    _error_if_any_worker_fails()\n",
      "RuntimeError: DataLoader worker (pid 23586) exited unexpectedly with exit code 1.\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m"
     ]
    }
   ],
   "source": [
    "learn.fit(lrs/6,1,wds=wd,cycle_len=30,use_clr_beta=(20,10,0.95,0.85))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.fit(lrs/8,1,wds=wd, cycle_len=8,use_clr=(20,4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.save(f'256urn-{S_PREFIX}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.load(f'256urn-{S_PREFIX}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x,y = next(iter(md.trn_dl))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x,y = next(iter(md.val_dl))\n",
    "py = to_np(learn.model(V(x[:8])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(denorm(x[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "py.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_img(py[0][0]>0);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "py[-1].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y[-1].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_img(y[-1]);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### No Crop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ext = '-300'\n",
    "sz=192\n",
    "bs=32\n",
    "md = torch_loader(ext, PATH, bs, sz, workers, False, pseudo_label, val_folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn = get_learner(md)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.load(f'256urn-{S_PREFIX}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.unfreeze()\n",
    "learn.bn_freeze(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr=1e-3\n",
    "wd=1e-6\n",
    "\n",
    "lrs = np.array([lr/200,lr/20,lr])/2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.fit(lr,1,wds=wd, cycle_len=10,use_clr=(5,5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.save(f'256urn-{S_PREFIX}-nocrop')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 512x512"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAIN_DN = 'CameraRGB'\n",
    "MASKS_DN = 'CameraSeg'\n",
    "\n",
    "ext = ''\n",
    "sz=384\n",
    "bs=16\n",
    "md = torch_loader(ext, PATH, bs, sz, workers, random_crop, pseudo_label, val_folder)\n",
    "\n",
    "learn = get_learner(md)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.lr_find()\n",
    "learn.sched.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# learn.load(f'256urn-{S_PREFIX}')\n",
    "learn.load(f'256urn-{S_PREFIX}-nocrop')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.freeze_to(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr=1e-3\n",
    "wd=5e-7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "learn.fit(lr,1, wds=wd, cycle_len=4,use_clr=(10,4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.fit(lr/4,1, wds=wd, cycle_len=4,use_clr=(10,4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.save(f'600urn-{S_PREFIX}-tmp')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.load(f'600urn-{S_PREFIX}-tmp')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.unfreeze()\n",
    "learn.bn_freeze(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lrs = np.array([lr/200,lr/20,lr])/2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.fit(lrs/10,1, wds=wd,cycle_len=4,use_clr=(20,8))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.fit(lrs/40,1, wds=wd,cycle_len=4,use_clr=(20,8))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.save(f'600urn-{S_PREFIX}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.sched.plot_loss()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### No Crop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAIN_DN = 'CameraRGB'\n",
    "MASKS_DN = 'CameraSeg'\n",
    "\n",
    "ext = ''\n",
    "sz=384\n",
    "bs=16\n",
    "md = torch_loader(ext, PATH, bs, sz, workers, False, pseudo_label, val_folder)\n",
    "\n",
    "learn = get_learner(md)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.load(f'600urn-{S_PREFIX}')\n",
    "# learn.load(f'256urn-{S_PREFIX}-nocrop')\n",
    "# learn.load('600urn-19-weights-26-r8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr=5e-4\n",
    "wd=5e-7\n",
    "lrs = np.array([lr/200,lr/20,lr])/2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.unfreeze()\n",
    "learn.bn_freeze(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.fit(lrs,1, wds=wd,cycle_len=4,use_clr=(20,2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.save(f'600urn-{S_PREFIX}-nocrop-tmp')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.load(f'600urn-{S_PREFIX}-nocrop-tmp')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.unfreeze()\n",
    "learn.bn_freeze(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.fit(lrs/10,1, wds=wd,cycle_len=4,use_clr=(20,2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.save(f'600urn-{S_PREFIX}-nocrop-tmp-2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.fit(lrs/100,1, wds=wd,cycle_len=4,use_clr=(20,2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.fit(lrs/50,1, wds=wd,cycle_len=4,use_clr_beta=(20,20,0.95,0.85))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.save(f'600urn-{S_PREFIX}-nocrop')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualize Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.load(f'600urn-36-resnet-softmax-nocrop-tmp')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_it = iter(md.val_dl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "x,y = next(val_it)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x[1].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res = learn.model(V(x).cuda())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "mx,idx = torch.max(res,1)\n",
    "idx = idx.cpu().data.numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "i=0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(denorm(x[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_img(idx[i]);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx[0].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_img(py[idx][1]);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "py[-1].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y[-1].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_img(y[idx][0]);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys, skvideo.io, json, base64\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "from io import BytesIO, StringIO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m = Unet34()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cuda_enabled = torch.cuda.is_available()\n",
    "DATA_PATH = Path('../data/all')\n",
    "model_path = str(DATA_PATH/'models/600urn-36-resnet-softmax-nocrop-tmp.h5')\n",
    "# model_path = str(PATH/'models/600urn-19-weights-26-r9.h5')\n",
    "if cuda_enabled:\n",
    "    m = m.cuda()\n",
    "    m.load_state_dict(torch.load(model_path, map_location=lambda storage, loc: storage))\n",
    "else:\n",
    "    m.load_state_dict(torch.load(model_path))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# file = sys.argv[-1]\n",
    "\n",
    "R_PATH = Path.cwd()\n",
    "file = R_PATH/'test_video.mp4'\n",
    "mismatched_idxs = []\n",
    "student_output = R_PATH/'tester_data_crop'\n",
    "ans_key = R_PATH/'results.json'\n",
    "\n",
    "# R_PATH = Path('../data/lyft-answers')\n",
    "# file = R_PATH/'test_video.mp4'\n",
    "# ans_key = R_PATH/'results.json'\n",
    "# mismatched_idxs = list(range(15,44)) + list(range(200,750))\n",
    "# student_output = R_PATH/'tester_data_crop'\n",
    "\n",
    "if file == 'demo.py':\n",
    "  print (\"Error loading video\")\n",
    "  quit\n",
    "\n",
    "# Define encoder function\n",
    "def encode(array):\n",
    "\tpil_img = Image.fromarray(array)\n",
    "\tbuff = BytesIO()\n",
    "\tpil_img.save(buff, format=\"PNG\")\n",
    "\treturn base64.b64encode(buff.getvalue()).decode(\"utf-8\")\n",
    "\n",
    "video = skvideo.io.vread(file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imagenet_stats = torch.cuda.FloatTensor([[0.485, 0.456, 0.406], [0.229, 0.224, 0.225]])\n",
    "def normalize(x):\n",
    "    if x.mean() > 1:\n",
    "        x.div_(255.0)\n",
    "    m,s = imagenet_stats\n",
    "#     return TVF.normalize(x, m, s)\n",
    "#     print(x.shape)\n",
    "    x.sub_(m.view(-1, 1, 1))\n",
    "    x.div_(s.view(-1, 1, 1))\n",
    "    return x\n",
    "\n",
    "def crop_bg(x):\n",
    "    # Original\n",
    "    h = x.shape[2]\n",
    "    top = int(h/3.75)\n",
    "    bot = int(h*.9 + h/150)\n",
    "    return x[:,:,top:bot,:]\n",
    "\n",
    "def pad(x):\n",
    "#     print(x.shape)\n",
    "    # Original\n",
    "    b,c,w,h = x.shape\n",
    "#     print(x.shape)\n",
    "    if h%32 == 0:\n",
    "        return x, 0\n",
    "    pad_right=32-h%32\n",
    "    if pad_right:\n",
    "        x = F.pad(x, (0,pad_right,0,0), 'constant', 0)\n",
    "    return x, pad_right\n",
    "    \n",
    "def undo(idx):\n",
    "    idx\n",
    "    idx = F.pad(idx, (0,0,226,54), \"constant\", 0)\n",
    "\n",
    "def preprocess(video):\n",
    "#     f1 = video[:,200:520,:,:]\n",
    "    f1 = np.rollaxis(video, 3, 1)\n",
    "    f1 = torch.from_numpy(f1).float().cuda()\n",
    "    f1 = crop_bg(f1)\n",
    "    f1 = normalize(f1)\n",
    "    return f1.contiguous()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "video_p = preprocess(video)\n",
    "if len(video_p) == 31:\n",
    "    video_p = torch.cat((video_p[:15], video_p[16:]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = []\n",
    "raw_results = []\n",
    "raw_frames = []\n",
    "answer_key = {}\n",
    "bs = 4\n",
    "\n",
    "        \n",
    "for i in range(0,video_p.shape[0],bs):\n",
    "    x = video_p[i:i+bs]\n",
    "    \n",
    "#     x = torch.from_numpy(f1).contiguous().float()\n",
    "#     if cuda_enabled:\n",
    "#         x = x.cuda()\n",
    "        \n",
    "    x,p = pad(x)\n",
    "    preds = m(torch.autograd.Variable(x))\n",
    "#     preds = F.sigmoid(preds)\n",
    "    # Indexes\n",
    "    mx,idx = torch.max(preds, 1)\n",
    "    for i in idx:\n",
    "        raw_frames.append(i.data.cpu().numpy())\n",
    "    if p > 0:\n",
    "        idx = idx[:,:,:-p]\n",
    "    idx = F.pad(idx, (0,0,160,56), \"constant\", 0)\n",
    "    frame_idx = 1+i\n",
    "    for frame in idx:\n",
    "        frame = frame.data.cpu().numpy()\n",
    "#         results.append(frame)\n",
    "        raw_frames.append(frame)\n",
    "        binary_car_result = (frame==1).astype('uint8')\n",
    "        binary_road_result = (frame==2).astype('uint8')\n",
    "#         answer_key[frame_idx] = [encode(binary_car_result), encode(binary_road_result)]\n",
    "        raw_results.append([binary_car_result, binary_road_result])\n",
    "        results.append([encode(binary_car_result), encode(binary_road_result)])\n",
    "        frame_idx+=1\n",
    "    \n",
    "    # Preds\n",
    "#     if p > 0:\n",
    "#         preds = preds[:,:,:,:-p]\n",
    "#     preds = F.pad(preds, (0,0,160,56,0,0), \"constant\", 0)\n",
    "#     frame_idx = 1+i\n",
    "#     for frame in preds:\n",
    "#         frame = frame.data.cpu().float().numpy()\n",
    "#         f_results.append(frame)\n",
    "#         binary_car_result = (frame[0]>0.5).astype('uint8')\n",
    "#         binary_road_result = (frame[1]>0.5).astype('uint8')\n",
    "#         answer_key[frame_idx] = [encode(binary_car_result), encode(binary_road_result)]\n",
    "#         raw_results.append([binary_car_result, binary_road_result])\n",
    "#         results.append([encode(binary_car_result), encode(binary_road_result)])\n",
    "#         frame_idx+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(video_p.cpu())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "video_p[0,:,50:200,700:].sum(), x[0,:,50:200,700:].cuda().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "video_p[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x[0][2][200].cuda().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(video_p[0] - x[0].cuda()).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "md.val_ds.fnames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_idx = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "video_p[show_idx].mean(), x[show_idx].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(denorm(x[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(denorm(video_p[show_idx]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn_res = learn.model(V(video_p[0:4]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_,learn_idx = torch.max(learn_res,1)\n",
    "learn_idx = learn_idx.cpu().data.numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(learn_idx[show_idx])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn_dl_res = learn.model(V(x[0:4].cuda()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_,learn_dl_idx = torch.max(learn_dl_res,1)\n",
    "learn_dl_idx = learn_dl_idx.cpu().data.numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(learn_dl_idx[show_idx])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m_dl_res = m(V(x.cuda()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_,m_dl_idx = torch.max(m_dl_res,1)\n",
    "m_dl_idx = m_dl_idx.cpu().data.numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(m_dl_idx[show_idx])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m_res = m(V(video_p[0:4]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_,m_idx = torch.max(m_res,1)\n",
    "m_idx = m_idx.cpu().data.numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(m_idx[show_idx])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(m_idx[show_idx])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = video_p[:6]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "b = x[:6].cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "m_res2 = m(V(a))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m_res2.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_,m_idx2 = torch.max(m_res2,1)\n",
    "m_idx2 = m_idx2.cpu().data.numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m_idx2.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(m_idx2[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx = 0\n",
    "plt.figure()\n",
    "plt.imshow(video[idx])\n",
    "plt.imshow((raw_results[idx][0]==1).data, alpha=.4)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(1, len(results)//10*10+1):\n",
    "    s_idx = i if i in mismatched_idxs else i-1\n",
    "#     s_idx = i-1\n",
    "    answer_key[i] = results[s_idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print output in proper json format\n",
    "tester_data = json.dumps(answer_key)\n",
    "with open(student_output, 'w') as f:\n",
    "    f.write(tester_data)\n",
    "# print(json.dumps(answer_key))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decoding car"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decode(packet):\n",
    "\timg = base64.b64decode(packet)\n",
    "\tfilename = R_PATH/'image.png'\n",
    "\twith open(filename, 'wb') as f:\n",
    "\t\t\tf.write(img)\n",
    "\tresult = misc.imread(filename)\n",
    "\treturn result\n",
    "\n",
    "with open(ans_key) as json_data:\n",
    "\tans_data = json.loads(json_data.read())\n",
    "\tjson_data.close()\n",
    "\n",
    "# Load student data\n",
    "with open(student_output) as student_data:\n",
    "\tstudent_ans_data = json.loads(student_data.read())\n",
    "\tstudent_data.close()\n",
    "\n",
    "frames_processed = 0\n",
    "\n",
    "Car_TP = 1 # True Positives\n",
    "Car_FP = 1 # Flase Positives\n",
    "Car_TN = 1 # True Negatives\n",
    "Car_FN = 1 # True Negatives\n",
    "\n",
    "Road_TP = 1 # True Positives\n",
    "Road_FP = 1 # Flase Positives\n",
    "Road_TN = 1 # True Negatives\n",
    "Road_FN = 1 # True Negatives\n",
    "\n",
    "for frame in range(1,len(ans_data.keys())+1):\n",
    "    if frame%3 == 0: continue\n",
    "    truth_data_car =  decode(ans_data[str(frame)][0])\n",
    "    truth_data_road =  decode(ans_data[str(frame)][1])\n",
    "    student_data_car = decode(student_ans_data[str(frame)][0])\n",
    "    student_data_road = decode(student_ans_data[str(frame)][1])\n",
    "#     student_data_car = results[frame-1][0]\n",
    "#     student_data_road = results[frame-1][1]\n",
    "#     student_data_car = f_results[frame-1][0]\n",
    "#     student_data_road = f_results[frame-1][1]\n",
    "#     print(np.mean(student_data_road == 1))\n",
    "#     print(np.mean(student_data_road_f == 1))\n",
    "#     pdb.set_trace()\n",
    "\n",
    "    Car_TP += np.sum(np.logical_and(student_data_car == 1, truth_data_car == 1))\n",
    "    Car_FP += np.sum(np.logical_and(student_data_car == 1, truth_data_car == 0))\n",
    "    Car_TN += np.sum(np.logical_and(student_data_car == 0, truth_data_car == 0))\n",
    "    Car_FN += np.sum(np.logical_and(student_data_car == 0, truth_data_car == 1))\n",
    "\n",
    "    Road_TP += np.sum(np.logical_and(student_data_road == 1, truth_data_road == 1))\n",
    "    Road_FP += np.sum(np.logical_and(student_data_road == 1, truth_data_road == 0))\n",
    "    Road_TN += np.sum(np.logical_and(student_data_road == 0, truth_data_road == 0))\n",
    "    Road_FN += np.sum(np.logical_and(student_data_road == 0, truth_data_road == 1))\n",
    "\n",
    "    frames_processed+=1\n",
    "\n",
    "\n",
    "# Generate results\n",
    "Car_precision = Car_TP/(Car_TP+Car_FP)/1.0\n",
    "Car_recall = Car_TP/(Car_TP+Car_FN)/1.0\n",
    "Car_beta = 2\n",
    "Car_F = (1+Car_beta**2) * ((Car_precision*Car_recall)/(Car_beta**2 * Car_precision + Car_recall))\n",
    "Road_precision = Road_TP/(Road_TP+Road_FP)/1.0\n",
    "Road_recall = Road_TP/(Road_TP+Road_FN)/1.0\n",
    "Road_beta = 0.5\n",
    "Road_F = (1+Road_beta**2) * ((Road_precision*Road_recall)/(Road_beta**2 * Road_precision + Road_recall))\n",
    "\n",
    "print (\"Car F score: %05.3f  | Car Precision: %05.3f  | Car Recall: %05.3f  |\\n\\\n",
    "Road F score: %05.3f | Road Precision: %05.3f | Road Recall: %05.3f | \\n\\\n",
    "Averaged F score: %05.3f\" %(Car_F,Car_precision,Car_recall,Road_F,Road_precision,Road_recall,((Car_F+Road_F)/2.0)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy import misc\n",
    "def decode(packet):\n",
    "\timg = base64.b64decode(packet)\n",
    "\tfilename = PATH/'image.png'\n",
    "\twith open(filename, 'wb') as f:\n",
    "\t\t\tf.write(img)\n",
    "\tresult = misc.imread(filename)\n",
    "\treturn result\n",
    "\n",
    "with open('results.json') as json_data:\n",
    "\tans_data = json.loads(json_data.read())\n",
    "\tjson_data.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_ans(index):\n",
    "    ans = decode(ans_data[str(index)][0])\n",
    "    f, (ax1, ax2, ax3) = plt.subplots(1, 3, figsize=(24, 15))\n",
    "    f.tight_layout()\n",
    "    ax1.imshow(r_np[index])\n",
    "    ax1.set_title('Mine', fontsize=35)\n",
    "    ax2.imshow(ans)\n",
    "    ax2.set_title('Answer', fontsize=35)\n",
    "    ax3.imshow(video[index])\n",
    "    ax2.set_title('Original', fontsize=35)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_ans(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ans = decode(ans_data['1'][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "def plot_res(index):\n",
    "    f, (ax1, ax2) = plt.subplots(1, 2, figsize=(24, 15))\n",
    "    f.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  },
  "toc": {
   "colors": {
    "hover_highlight": "#DAA520",
    "navigate_num": "#000000",
    "navigate_text": "#333333",
    "running_highlight": "#FF0000",
    "selected_highlight": "#FFD700",
    "sidebar_border": "#EEEEEE",
    "wrapper_background": "#FFFFFF"
   },
   "moveMenuLeft": true,
   "nav_menu": {
    "height": "86px",
    "width": "252px"
   },
   "navigate_menu": true,
   "number_sections": true,
   "sideBar": true,
   "threshold": 4,
   "toc_cell": false,
   "toc_section_display": "block",
   "toc_window_display": false,
   "widenNotebook": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
