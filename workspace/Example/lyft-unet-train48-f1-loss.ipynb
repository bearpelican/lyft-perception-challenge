{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "%reload_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fastai.conv_learner import *\n",
    "# from fastai.dataset import *\n",
    "from fastai.models.resnet import vgg_resnet50\n",
    "\n",
    "import json\n",
    "from glob import glob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.backends.cudnn.benchmark=True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH = Path('../data/all')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_img(im, figsize=None, ax=None, alpha=None):\n",
    "    if not ax: fig,ax = plt.subplots(figsize=figsize)\n",
    "    ax.imshow(im, alpha=alpha)\n",
    "    ax.set_axis_off()\n",
    "    return ax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "VEHICLES=10\n",
    "ROADS=7\n",
    "ROAD_LINES=6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAIN_DN = 'CameraRGB'\n",
    "MASKS_DN = 'CameraSeg'\n",
    "workers=7\n",
    "random_crop=True\n",
    "pseudo_label=False\n",
    "val_folder = 'sample_test_sync'\n",
    "# val_folder = 'val'\n",
    "S_PREFIX = '46_wide'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision.datasets.folder import pil_loader\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.transforms.functional as TTF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MatchedFilesDataset(Dataset):\n",
    "    def __init__(self, fnames, y, tfms, path):\n",
    "        self.path,self.fnames = path,fnames\n",
    "        self.open_fn = pil_loader\n",
    "        self.y=y\n",
    "        self.open_y_fn = pil_loader\n",
    "        assert(len(fnames)==len(y))\n",
    "        \n",
    "        self.n = self.get_n()\n",
    "        self.c = self.get_c()\n",
    "        self.tfms = tfms\n",
    "        \n",
    "    def get_x(self, i): return self.open_fn(os.path.join(self.path, self.fnames[i]))\n",
    "    def get_y(self, i): return self.open_y_fn(os.path.join(self.path, self.y[i]))\n",
    "    def get_n(self): return len(self.fnames)\n",
    "    def get_c(self): return 2\n",
    "    \n",
    "    def get(self, tfms, x, y):\n",
    "        for fn in tfms:\n",
    "            #pdb.set_trace()\n",
    "            x, y = fn(x, y)\n",
    "        return (x, y)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        x,y = self.get_x(idx),self.get_y(idx)\n",
    "        return self.get(self.tfms, x, y)\n",
    "    \n",
    "    def __len__(self): return self.n\n",
    "\n",
    "    def resize_imgs(self, targ, new_path):\n",
    "        dest = resize_imgs(self.fnames, targ, self.path, new_path)\n",
    "        return self.__class__(self.fnames, self.y, self.transform, dest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Seems to speed up training by ~2%\n",
    "class DataPrefetcher():\n",
    "    def __init__(self, loader, stop_after=None):\n",
    "        self.loader = loader\n",
    "        self.dataset = loader.dataset\n",
    "        self.stream = torch.cuda.Stream()\n",
    "        self.stop_after = stop_after\n",
    "        self.next_input = None\n",
    "        self.next_target = None\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.loader)\n",
    "    \n",
    "    def preload(self):\n",
    "        try:\n",
    "            self.next_input, self.next_target = next(self.loaditer)\n",
    "        except StopIteration:\n",
    "            self.next_input = None\n",
    "            self.next_target = None\n",
    "            return\n",
    "        with torch.cuda.stream(self.stream):\n",
    "            self.next_input = self.next_input.cuda(async=True)\n",
    "            self.next_target = self.next_target.cuda(async=True)\n",
    "\n",
    "    def __iter__(self):\n",
    "        count = 0\n",
    "        self.loaditer = iter(self.loader)\n",
    "        self.preload()\n",
    "        while self.next_input is not None:\n",
    "            torch.cuda.current_stream().wait_stream(self.stream)\n",
    "            input = self.next_input\n",
    "            target = self.next_target\n",
    "            self.preload()\n",
    "            count += 1\n",
    "            yield input, target\n",
    "            if type(self.stop_after) is int and (count > self.stop_after):\n",
    "                break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def crop_bg_pil(x,y):\n",
    "    w, h = x.size\n",
    "    top = int(h/3.75)\n",
    "    bot = int(h*.9 + h/150)\n",
    "    pad_right=32-w%32\n",
    "    if pad_right == 32: pad_right = 0\n",
    "    return TTF.crop(x, top, 0, bot-top, w+pad_right), TTF.crop(y, top, 0, bot-top, w+pad_right)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RHF(object):\n",
    "    def __init__(self, p=0.5): self.p = p\n",
    "    def __call__(self, x, y):\n",
    "        if random.random() < self.p:\n",
    "            return TTF.hflip(x), TTF.hflip(y)\n",
    "        return x,y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RR(object):\n",
    "    def __init__(self, degrees=2): self.degrees = degrees\n",
    "    def __call__(self, x, y):\n",
    "        angle = random.uniform(-self.degrees, self.degrees)\n",
    "        return TTF.rotate(x, angle), TTF.rotate(y, angle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tfm_x_wrapper(tfm):\n",
    "    return lambda x,y: (tfm(x), y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RC():\n",
    "    def __init__(self, targ_sz):\n",
    "        self.targ_sz = targ_sz\n",
    "\n",
    "    def __call__(self, x, y):\n",
    "        rand_w = random.uniform(0, 1)\n",
    "        rand_h = random.uniform(0, 1)\n",
    "        w,h = x.size\n",
    "        t_w,t_h = self.targ_sz\n",
    "        start_x = np.floor(rand_w*(w-t_w)).astype(int)\n",
    "        start_y = np.floor(rand_h*(h-t_h)).astype(int)\n",
    "        return TTF.crop(x, start_y, start_x, t_h, t_w), TTF.crop(y, start_y, start_x, t_h, t_w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_y_ce(y_img):\n",
    "    y_new = np.zeros(y_img.shape, dtype=int)\n",
    "    y_new[y_img==VEHICLES] = 1\n",
    "    cutoff_y = int(y_new.shape[0]*.875)\n",
    "    y_new[cutoff_y:,:] = 0\n",
    "\n",
    "    y_new[y_img==ROADS] = 2\n",
    "    y_new[y_img==ROAD_LINES] = 2\n",
    "    return torch.from_numpy(y_new).long()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_y(y_img):\n",
    "    yr = (y_img==ROADS) | (y_img==ROAD_LINES)\n",
    "    yc = (y_img==VEHICLES)\n",
    "    cutoff_y = int(yc.shape[0]*.875)\n",
    "    yc[cutoff_y:,:] = 0\n",
    "    rn = ~(yr | yc)\n",
    "    return torch.from_numpy(np.stack((rn,yc,yr)).astype(int))\n",
    "\n",
    "\n",
    "def xy_tensor(x,y):\n",
    "    y_img = np.array(y, np.int32, copy=False)\n",
    "    return TTF.to_tensor(x), convert_y_ce(y_img[:,:,0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RRC(transforms.RandomResizedCrop):\n",
    "    def __call__(self, x, y):\n",
    "        i, j, h, w = self.get_params(x, self.scale, self.ratio)\n",
    "        x = TTF.resized_crop(x, i, j, h, w, self.size, self.interpolation)\n",
    "        y = TTF.resized_crop(y, i, j, h, w, self.size, self.interpolation)\n",
    "        return x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def torch_loader(f_ext, data_path, bs, size, workers=7, random_crop=False, pseudo_label=False, val_folder=None, val_bs=None):\n",
    "    # Data loading code\n",
    "    x_names = np.sort(np.array(glob(str(data_path/f'CameraRGB{f_ext}'/'*.png'))))\n",
    "    y_names = np.sort(np.array(glob(str(data_path/f'CameraSeg{f_ext}'/'*.png'))))\n",
    "\n",
    "    x_n = x_names.shape[0]\n",
    "    val_idxs = list(range(x_n-300, x_n))\n",
    "    \n",
    "    if pseudo_label:\n",
    "        x_names_test = np.sort(np.array(glob(f'../data/pseudo/CameraRGB{f_ext}/*.png')))\n",
    "        y_names_test = np.sort(np.array(glob(f'../data/pseudo/CameraSeg{f_ext}/*.png')))\n",
    "        x_names = np.concatenate((x_names, x_names_test))\n",
    "        x_names = np.concatenate((y_names, y_names_test))\n",
    "        print(f'Pseudo-Labels: {len(x_names_test)}')\n",
    "    if val_folder:\n",
    "        x_names_val = np.sort(np.array(glob(f'../data/{val_folder}/CameraRGB{f_ext}/*.png')))\n",
    "        y_names_val = np.sort(np.array(glob(f'../data/{val_folder}/CameraSeg{f_ext}/*.png')))\n",
    "        val_x,val_y = x_names_val, y_names_val\n",
    "        trn_x,trn_y = x_names, y_names\n",
    "        print(f'Val Labels:', len(val_x))\n",
    "    else:\n",
    "        ((val_x,trn_x),(val_y,trn_y)) = split_by_idx(val_idxs, x_names, y_names)\n",
    "    print(f'Val x:{len(val_x)}, y:{len(val_y)}')\n",
    "    print(f'Trn x:{len(trn_x)}, y:{len(trn_y)}')\n",
    "    print(f'All x:{len(x_names)}')\n",
    "    normalize = transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "    \n",
    "    train_tfms = [\n",
    "        crop_bg_pil,\n",
    "        tfm_x_wrapper(transforms.ColorJitter(.2,.2,.2)),\n",
    "#         tfm_x_wrapper(Lighting(0.1, __imagenet_pca['eigval'], __imagenet_pca['eigvec'])),\n",
    "        RR(),\n",
    "        RHF(),\n",
    "#         RC((size,size)),\n",
    "        xy_tensor,\n",
    "        tfm_x_wrapper(normalize),\n",
    "    ]\n",
    "    if random_crop:\n",
    "        train_tfms.insert(3,RRC(size, scale=(0.4, 1.0)))\n",
    "    train_dataset = MatchedFilesDataset(trn_x, trn_y, train_tfms, path='')\n",
    "    train_loader = torch.utils.data.DataLoader(\n",
    "        train_dataset, batch_size=bs, shuffle=True,\n",
    "        num_workers=workers, pin_memory=True)\n",
    "\n",
    "    val_tfms = [\n",
    "        crop_bg_pil,\n",
    "        xy_tensor,\n",
    "        tfm_x_wrapper(normalize)\n",
    "    ]\n",
    "    val_dataset = MatchedFilesDataset(val_x, val_y, val_tfms, path='')\n",
    "    if val_bs is None: val_bs = bs\n",
    "    val_loader = torch.utils.data.DataLoader(\n",
    "        val_dataset, batch_size=val_bs, shuffle=False,\n",
    "        num_workers=workers, pin_memory=True)\n",
    "\n",
    "    train_loader = DataPrefetcher(train_loader)\n",
    "    val_loader = DataPrefetcher(val_loader)\n",
    "    \n",
    "    data = ModelData(data_path, train_loader, val_loader)\n",
    "    return data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def denorm(x):\n",
    "    x_np = x.cpu().numpy()\n",
    "    x_np = np.rollaxis(x_np, 0, 3)\n",
    "    mean=np.array([0.485, 0.456, 0.406])\n",
    "    std=np.array([0.229, 0.224, 0.225])\n",
    "    x_np = x_np*std+mean\n",
    "    return x_np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## U-net (ish)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision.models import vgg11_bn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vgg11(pre): return children(vgg11_bn(pre))[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_meta = {\n",
    "    resnet18:[8,6], resnet34:[8,6], resnet50:[8,6], resnet101:[8,6], resnet152:[8,6],\n",
    "    vgg11:[0,13], vgg16:[0,22], vgg19:[0,22],\n",
    "    resnext50:[8,6], resnext101:[8,6], resnext101_64:[8,6],\n",
    "    wrn:[8,6], inceptionresnet_2:[-2,9], inception_4:[-1,9],\n",
    "    dn121:[0,7], dn161:[0,7], dn169:[0,7], dn201:[0,7],\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_base(f):\n",
    "    cut,lr_cut = model_meta[f]\n",
    "    layers = cut_model(f(True), cut)\n",
    "    return nn.Sequential(*layers), lr_cut"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SaveFeatures():\n",
    "    features=None\n",
    "    def __init__(self, m): self.hook = m.register_forward_hook(self.hook_fn)\n",
    "    def hook_fn(self, module, input, output): self.features = output\n",
    "    def remove(self): self.hook.remove()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "class UnetBlock(nn.Module):\n",
    "    def __init__(self, up_in, x_in, n_out):\n",
    "        super().__init__()\n",
    "        up_out = x_out = n_out//2\n",
    "        self.x_conv  = nn.Conv2d(x_in,  x_out,  1)\n",
    "        self.tr_conv = nn.ConvTranspose2d(up_in, up_out, 2, stride=2)\n",
    "        self.bn = nn.BatchNorm2d(n_out)\n",
    "        \n",
    "    def forward(self, up_p, x_p):\n",
    "        up_p = self.tr_conv(up_p)\n",
    "        x_p = self.x_conv(x_p)\n",
    "        cat_p = torch.cat([up_p,x_p], dim=1)\n",
    "        return self.bn(F.relu(cat_p, inplace=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Unet34Mod(nn.Module):\n",
    "    def __init__(self, out=3, f=resnet34):\n",
    "        super().__init__()\n",
    "        m_base, lr_cut = get_base(f)\n",
    "        self.rn = m_base\n",
    "        self.lr_cut = lr_cut\n",
    "        self.sfs = [SaveFeatures(self.rn[i]) for i in [2,4,5,6]]\n",
    "        self.up1 = UnetBlock(512,256,256)\n",
    "        self.up2 = UnetBlock(256,128,256)\n",
    "        self.up3 = UnetBlock(256,64,128)\n",
    "        self.up4 = UnetBlock(128,64,64)\n",
    "        self.up5 = UnetBlock(64,32,32)\n",
    "        self.up6 = nn.ConvTranspose2d(32, out, 1)\n",
    "        self.x_skip = nn.Sequential(\n",
    "            nn.Conv2d(out,32,1),\n",
    "            nn.BatchNorm2d(32),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "        \n",
    "    def forward(self,x):\n",
    "        x_skip = self.x_skip(x)\n",
    "        x = self.rn(x)\n",
    "        x = self.up1(x, self.sfs[3].features)\n",
    "        x = self.up2(x, self.sfs[2].features)\n",
    "        x = self.up3(x, self.sfs[1].features)\n",
    "        x = self.up4(x, self.sfs[0].features)\n",
    "        x = self.up5(x, x_skip)\n",
    "        x = self.up6(x)\n",
    "        return torch.squeeze(x)\n",
    "    \n",
    "    def close(self):\n",
    "        for sf in self.sfs: sf.remove()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "class UnetModel():\n",
    "    def __init__(self,model,name='unet'):\n",
    "        self.model,self.name = model,name\n",
    "\n",
    "    def get_layer_groups(self, precompute):\n",
    "        if isinstance(self.model, FP16):\n",
    "            model = self.model.module\n",
    "        else:\n",
    "            model = self.model\n",
    "        lgs = list(split_by_idxs(children(model.rn), [model.lr_cut]))\n",
    "        return lgs + [children(model)[1:]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def carce_f_p_r(pred, targs):\n",
    "    _,idx = torch.max(pred, 1)\n",
    "    return fbeta_score(idx==1, targs[:,:,:]==1, beta=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rdce_f(pred, targs):\n",
    "    _,idx = torch.max(pred, 1)\n",
    "    f,p,r = fbeta_score(idx==2, targs[:,:,:]==2, beta=0.5)\n",
    "    return f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def carsig_f_p_r(pred, targs):\n",
    "    p2 = F.sigmoid(pred)\n",
    "    return fbeta_score(p2[:,0,:,:], targs[:,0,:,:], beta=2, threshold=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rdsig_f(pred, targs):\n",
    "    p2 = F.sigmoid(pred)\n",
    "    f,p,r = fbeta_score(p2[:,1,:,:], targs[:,1,:,:], beta=0.5, threshold=0.5)\n",
    "    return f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def car_f_p_r(pred, targs):\n",
    "    _,idx = torch.max(pred, 1)\n",
    "    return fbeta_score(idx==1, targs[:,1,:,:], beta=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rd_f(pred, targs):\n",
    "    _,idx = torch.max(pred, 1)\n",
    "    f,p,r = fbeta_score(idx==2, targs[:,2,:,:], beta=0.5)\n",
    "    return f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fbeta_score(y_pred, y_true, beta, threshold=None, eps=1e-9):\n",
    "    beta2 = beta**2\n",
    "\n",
    "    if threshold:\n",
    "        y_pred = torch.ge(y_pred.float(), threshold).float()\n",
    "    else:\n",
    "        y_pred = y_pred.float()\n",
    "    y_true = y_true.float()\n",
    "\n",
    "    true_positive = (y_pred * y_true).sum()\n",
    "    precision = true_positive/(y_pred.sum()+(eps))\n",
    "    recall = true_positive/(y_true.sum()+eps)\n",
    "    \n",
    "    fb = (precision*recall)/(precision*beta2 + recall + eps)*(1+beta2)\n",
    "    \n",
    "    return fb, precision, recall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def new_acc_sig(pred, targs):\n",
    "    p2 = F.sigmoid(pred)\n",
    "    return ((p2>0.5).long() == targs).float().mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def new_acc_ce(preds, targs):\n",
    "    mx,idx = torch.max(preds, 1)\n",
    "    return (idx == targs).float().mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def new_acc(pred, targs):\n",
    "    _,idx = torch.max(pred, 1)\n",
    "    _,t_idx = torch.max(targs,1)\n",
    "    return (idx == t_idx).float().mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = torch.ones([64,3,100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "intersection = a * a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "int_sum = intersection.sum(dim=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 1.0000,  4.0000,  0.2500])"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.FloatTensor([1,2,.5]) ** 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dice_coeff_weight(pred, target, weight):\n",
    "    smooth = 1.\n",
    "    num,c,h,w = pred.shape\n",
    "    m1 = pred.view(num, c, -1)  # Flatten\n",
    "    m2 = target.view(num, c, -1)  # Flatten\n",
    "    intersection = (m1 * m2)\n",
    "    w = V(weight.view(1,-1,1))\n",
    "    i_w = (w*intersection).sum()\n",
    "    m1_w = (w*m1).sum()\n",
    "    m2_w = (w*m2).sum()\n",
    "    return (2. * i_w + smooth) / (m1_w + m2_w + smooth)\n",
    "\n",
    "def dice_coeff(pred, target):\n",
    "    smooth = 1.\n",
    "    num,c,h,w = pred.shape\n",
    "    m1 = pred.view(num, c, -1)  # Flatten\n",
    "    m2 = target.view(num, c, -1)  # Flatten\n",
    "    intersection = (m1 * m2).sum()\n",
    "    return (2. * intersection + smooth) / (m1.sum() + m2.sum() + smooth)\n",
    "\n",
    "\n",
    "class SoftDiceLoss(nn.Module):\n",
    "    def __init__(self, weight=None, size_average=True, softmax=True):\n",
    "        super(SoftDiceLoss, self).__init__()\n",
    "        self.weight = weight\n",
    "        self.softmax = softmax\n",
    "\n",
    "    def forward(self, logits, targets):\n",
    "        probs = F.softmax(logits) if self.softmax else F.sigmoid(logits)\n",
    "        num = targets.size(0)  # Number of batches\n",
    "        targets = torch.cat(((targets==0).unsqueeze(1), (targets==1).unsqueeze(1), (targets==2).unsqueeze(1)), dim=1).float()\n",
    "        if isinstance(logits.data, torch.cuda.HalfTensor):\n",
    "            targets = targets.half()\n",
    "        else:\n",
    "            targets = targets.float()\n",
    "        if self.weight is not None:\n",
    "            score = dice_coeff_weight(probs, targets, self.weight)\n",
    "        else:\n",
    "            score = dice_coeff(probs, targets)\n",
    "        score = 1 - score.sum() / num\n",
    "        return score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lyft_score(pred, target, weight):\n",
    "    num,c,h,w = pred.shape\n",
    "    pred = pred.view(num, c, -1)  # Flatten\n",
    "    target = target.view(num, c, -1)  # Flatten\n",
    "    intersection = (pred * target)\n",
    "    int_sum = intersection.sum(dim=-1)\n",
    "    pred_sum = pred.sum(dim=-1)\n",
    "    targ_sum = target.sum(dim=-1)\n",
    "    \n",
    "    eps = 1e-9\n",
    "    precision = int_sum / (pred_sum + eps)\n",
    "    recall = int_sum / (targ_sum + eps)\n",
    "    beta = V(weight ** 2)\n",
    "    \n",
    "    fnum = (1.+beta) * precision * recall\n",
    "    fden = beta * precision + recall + eps\n",
    "    \n",
    "    fscore = fnum / fden\n",
    "    \n",
    "#     fb = (precision*recall)/precision*beta + recall + eps\n",
    "    \n",
    "    avg_w = torch.cuda.FloatTensor([0,.5,.5])\n",
    "    favg = V(avg_w) * fscore\n",
    "#     pdb.set_trace()\n",
    "    return favg.sum(dim=-1)\n",
    "\n",
    "class FLoss(nn.Module):\n",
    "    def __init__(self, weight=torch.cuda.FloatTensor([1,2,0.5]), softmax=True):\n",
    "        super().__init__()\n",
    "        self.weight = weight\n",
    "        self.softmax = softmax\n",
    "\n",
    "    def forward(self, logits, targets):\n",
    "        probs = F.softmax(logits) if self.softmax else F.sigmoid(logits)\n",
    "        num = targets.size(0)  # Number of batches\n",
    "        targets = torch.cat(((targets==0).unsqueeze(1), (targets==1).unsqueeze(1), (targets==2).unsqueeze(1)), dim=1).float()\n",
    "        if isinstance(logits.data, torch.cuda.HalfTensor):\n",
    "            targets = targets.half()\n",
    "        else:\n",
    "            targets = targets.float()\n",
    "            \n",
    "        score = lyft_score(probs, targets, self.weight)\n",
    "        score = 1 - score.sum() / num\n",
    "        return score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_learner(md, m_fn=Unet34Mod, weights=[1,200,2], half=False, softmax=True, dice=False):\n",
    "    out_sz = 3 if softmax else 2\n",
    "    m = to_gpu(m_fn(out_sz))\n",
    "    models = UnetModel(m)\n",
    "    learn = ConvLearner(md, models)\n",
    "    learn.opt_fn=optim.Adam\n",
    "    class_weights = torch.cuda.FloatTensor(weights)\n",
    "    if half:\n",
    "        class_weights = class_weights.half()\n",
    "        learn.half()\n",
    "        \n",
    "    if dice: learn.crit=SoftDiceLoss(weight=class_weights, softmax=softmax)\n",
    "    else: learn.crit=nn.CrossEntropyLoss(weight=class_weights)\n",
    "#     learn.crit = FLoss(softmax=softmax)\n",
    "    \n",
    "    if softmax: learn.metrics = [new_acc_ce, rdce_f, carce_f_p_r]\n",
    "    else: learn.metrics = [new_acc_sig, rdsig_f, carsig_f_p_r]\n",
    "    # learn.metrics=[new_acc, rd_f, car_f_p_r]\n",
    "    \n",
    "    return learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val Labels: 0\n",
      "Val x:0, y:0\n",
      "Trn x:4600, y:4600\n",
      "All x:4600\n"
     ]
    }
   ],
   "source": [
    "ext = '-300'\n",
    "sz=192\n",
    "bs=64\n",
    "random_crop=True\n",
    "md = torch_loader(ext, PATH, bs, sz, workers, random_crop, pseudo_label, val_folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn = get_learner(md, m_fn=Unet34Mod, weights=[1,10,2], softmax=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.freeze_to(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5b5f50cfc38d4fbcb567c8f6c356a12e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Epoch', max=1), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  6%|▌         | 4/72 [00:02<00:43,  1.56it/s, loss=1.18]"
     ]
    }
   ],
   "source": [
    "learn.fit(.001, 1, cycle_len=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val Labels: 0\n",
      "Val x:0, y:0\n",
      "Trn x:4600, y:4600\n",
      "All x:4600\n"
     ]
    }
   ],
   "source": [
    "ext = '-300'\n",
    "sz=192\n",
    "bs=64\n",
    "random_crop=True\n",
    "md = torch_loader(ext, PATH, bs, sz, workers, random_crop, pseudo_label, val_folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn = get_learner(md, m_fn=Unet34Mod, weights=[1,10,2], softmax=True)\n",
    "# learn.load(f'600urn-19-weights')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "97cd4d07d26f44db879c5ce1c191d0ce",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Epoch', max=1), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 90%|█████████ | 153/170 [00:32<00:03,  4.72it/s, loss=3.2]  "
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "learn.lr_find()\n",
    "learn.sched.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr=3e-2\n",
    "wd=1e-7\n",
    "\n",
    "lrs = np.array([lr/200,lr/20,lr])/2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.freeze_to(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "73c09698304e4d15a527fc0d0c59956d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Epoch', max=30), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 25%|██▌       | 18/72 [00:06<00:18,  2.92it/s, loss=0.704]\n",
      "                                                           \r"
     ]
    },
    {
     "ename": "ZeroDivisionError",
     "evalue": "Weights sum to zero, can't be normalized",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mZeroDivisionError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-95-f03f6c7a1667>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mlearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwds\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mwd\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcycle_len\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m30\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0muse_clr\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m20\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/envs/lyft/lib/python3.6/site-packages/fastai/learner.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, lrs, n_cycle, wds, **kwargs)\u001b[0m\n\u001b[1;32m    285\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msched\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    286\u001b[0m         \u001b[0mlayer_opt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_layer_opt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlrs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 287\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit_gen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlayer_opt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_cycle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    288\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    289\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mwarm_up\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwds\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/lyft/lib/python3.6/site-packages/fastai/learner.py\u001b[0m in \u001b[0;36mfit_gen\u001b[0;34m(self, model, data, layer_opt, n_cycle, cycle_len, cycle_mult, cycle_save_name, best_save_name, use_clr, use_clr_beta, metrics, callbacks, use_wd_sched, norm_wds, wds_sched_mult, use_swa, swa_start, swa_eval_freq, **kwargs)\u001b[0m\n\u001b[1;32m    232\u001b[0m             \u001b[0mmetrics\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmetrics\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallbacks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcallbacks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreg_fn\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreg_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclip\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclip\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfp16\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfp16\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    233\u001b[0m             \u001b[0mswa_model\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mswa_model\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0muse_swa\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mswa_start\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mswa_start\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 234\u001b[0;31m             swa_eval_freq=swa_eval_freq, **kwargs)\n\u001b[0m\u001b[1;32m    235\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    236\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mget_layer_groups\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_layer_groups\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/lyft/lib/python3.6/site-packages/fastai/model.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(model, data, n_epochs, opt, crit, metrics, callbacks, stepper, swa_model, swa_start, swa_eval_freq, **kwargs)\u001b[0m\n\u001b[1;32m    158\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    159\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mall_val\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 160\u001b[0;31m             \u001b[0mvals\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvalidate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_stepper\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcur_data\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mval_dl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmetrics\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    161\u001b[0m             \u001b[0mstop\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    162\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mcb\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mcallbacks\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mstop\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstop\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mcb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_epoch_end\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvals\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/lyft/lib/python3.6/site-packages/fastai/model.py\u001b[0m in \u001b[0;36mvalidate\u001b[0;34m(stepper, dl, metrics)\u001b[0m\n\u001b[1;32m    227\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mres_list\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf_res\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    228\u001b[0m             \u001b[0mres\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mres_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 229\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maverage\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweights\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbatch_cnts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maverage\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mres\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweights\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbatch_cnts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    230\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    231\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mget_prediction\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/lyft/lib/python3.6/site-packages/numpy/lib/function_base.py\u001b[0m in \u001b[0;36maverage\u001b[0;34m(a, axis, weights, returned)\u001b[0m\n\u001b[1;32m   1156\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0many\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mscl\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0.0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1157\u001b[0m             raise ZeroDivisionError(\n\u001b[0;32m-> 1158\u001b[0;31m                 \"Weights sum to zero, can't be normalized\")\n\u001b[0m\u001b[1;32m   1159\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1160\u001b[0m         \u001b[0mavg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmultiply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwgt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mresult_dtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0mscl\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mZeroDivisionError\u001b[0m: Weights sum to zero, can't be normalized"
     ]
    }
   ],
   "source": [
    "learn.fit(lr, 1, wds=wd, cycle_len=30,use_clr=(20,2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.save(f'300urn-{S_PREFIX}-tmp')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.unfreeze()\n",
    "learn.set_bn_freeze(learn.model.rn, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e5175e2614854b2db3086a69d5f05304",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Epoch', max=30), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch      trn_loss   val_loss   new_acc_ce rdce_f     carce_f_p_r \n",
      "    0      0.055364   0.060016   0.987541   0.989685   0.887346   0.743076   0.933344  \n",
      "    1      0.055179   0.060181   0.987741   0.990393   0.88807    0.746535   0.932991  \n",
      "    2      0.054761   0.059217   0.987815   0.990437   0.889265   0.746392   0.934702  \n",
      "    3      0.055035   0.058911   0.987921   0.99067    0.889151   0.748439   0.933751  \n",
      "    4      0.054516   0.058375   0.98796    0.990617   0.889028   0.747686   0.933895  \n",
      "    5      0.054491   0.05738    0.988009   0.990705   0.890977   0.746201   0.937109  \n",
      "    6      0.054004   0.05769    0.987937   0.990495   0.889863   0.74415    0.936443  \n",
      "    7      0.054078   0.05663    0.988124   0.990939   0.891512   0.745848   0.938002  \n",
      "    8      0.053899   0.056025   0.988196   0.990839   0.891381   0.747112   0.937331  \n",
      "    9      0.053489   0.057123   0.988379   0.991319   0.891909   0.753707   0.935462  \n",
      "    10     0.053644   0.056526   0.988233   0.990998   0.891204   0.748029   0.936705  \n",
      "    11     0.053492   0.056045   0.988249   0.991191   0.892034   0.746804   0.938296  \n",
      "    12     0.053691   0.056813   0.988309   0.991006   0.890821   0.751018   0.935064  \n",
      "    13     0.053525   0.05627    0.988639   0.991385   0.893242   0.758785   0.935317  \n",
      "    14     0.053165   0.055979   0.988382   0.990992   0.891198   0.7502     0.935876  \n",
      "    15     0.053041   0.056706   0.988583   0.991224   0.891383   0.756319   0.93376   \n",
      "    16     0.053149   0.05618    0.98851    0.991334   0.891737   0.7528     0.935584  \n",
      "    17     0.053116   0.056106   0.988543   0.991069   0.892295   0.754542   0.935714  \n",
      "    18     0.053185   0.055887   0.98874    0.991376   0.893048   0.75966    0.934743  \n",
      "    19     0.052965   0.055266   0.988239   0.991375   0.891407   0.740853   0.939913  \n",
      "    20     0.053513   0.05607    0.988562   0.991627   0.893691   0.753127   0.93815   \n",
      "    21     0.053313   0.05582    0.988554   0.991098   0.892992   0.754693   0.936602  \n",
      "    22     0.052776   0.055771   0.988296   0.990856   0.89244    0.747829   0.938558  \n",
      "    23     0.052452   0.056132   0.988465   0.991304   0.892572   0.751666   0.93723   \n",
      "    24     0.052865   0.055863   0.988743   0.991264   0.893066   0.759247   0.934966  \n",
      "    25     0.052569   0.05539    0.988621   0.991668   0.893877   0.752965   0.938461  \n",
      "    26     0.052484   0.056079   0.988797   0.991463   0.893759   0.760841   0.935289  \n",
      "    27     0.053021   0.055207   0.988799   0.991601   0.89422    0.757714   0.937091  \n",
      "    28     0.053043   0.055514   0.988761   0.991616   0.894085   0.758394   0.936647  \n",
      "    29     0.053027   0.05555    0.988593   0.991341   0.893819   0.753525   0.938186  \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.055549749433994294,\n",
       " 0.988593406677246,\n",
       " 0.991340720653534,\n",
       " 0.8938189601898193,\n",
       " 0.753524842262268,\n",
       " 0.9381862187385559]"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "learn.fit(lrs/40, 1, wds=wd, cycle_len=30,use_clr=(20,2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.save(f'300urn-{S_PREFIX}-rc')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val Labels: 400\n",
      "Val x:400, y:400\n",
      "Trn x:10880, y:10880\n",
      "All x:10880\n"
     ]
    }
   ],
   "source": [
    "ext = '-300'\n",
    "sz=192\n",
    "bs=64\n",
    "random_crop=False\n",
    "md = torch_loader(ext, PATH, bs, sz, workers, random_crop, pseudo_label, val_folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr=1e-4\n",
    "wd=1e-7\n",
    "\n",
    "lrs = np.array([lr/50,lr/10,lr])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.load(f'300urn-{S_PREFIX}-rc')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.unfreeze()\n",
    "learn.set_bn_freeze(learn.model.rn, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2fd4d51f92cb431bb2b9ac1b1f494833",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Epoch', max=20), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch      trn_loss   val_loss   new_acc_ce rdce_f     carce_f_p_r \n",
      "    0      0.052985   0.05597    0.988748   0.991468   0.894211   0.760157   0.936143  \n",
      "    1      0.053173   0.054764   0.988659   0.991097   0.89381    0.755138   0.937512  \n",
      "    2      0.052661   0.055403   0.988518   0.991288   0.893515   0.751348   0.938618  \n",
      "    3      0.052934   0.055318   0.988804   0.991524   0.894327   0.758974   0.936765  \n",
      "    4      0.052941   0.055688   0.988671   0.991229   0.89351    0.75705    0.936388  \n",
      "    5      0.052429   0.055273   0.988552   0.991375   0.893161   0.751703   0.938016  \n",
      "    6      0.052596   0.055518   0.988564   0.991143   0.893197   0.754086   0.937111  \n",
      "    7      0.053555   0.055499   0.988719   0.991184   0.893663   0.758214   0.93617   \n",
      "    8      0.053668   0.054862   0.988669   0.991394   0.894023   0.753467   0.938498  \n",
      "    9      0.052148   0.055557   0.98864    0.991179   0.89302    0.756289   0.936012  \n",
      "    10     0.052854   0.055209   0.988721   0.991133   0.893534   0.758148   0.935986  \n",
      "    11     0.052784   0.055302   0.988572   0.990791   0.893645   0.755996   0.936958  \n",
      "    12     0.053132   0.055111   0.98871    0.991419   0.893889   0.755988   0.937298  \n",
      "    13     0.052613   0.056023   0.988738   0.991523   0.894096   0.760112   0.936022  \n",
      "    14     0.052955   0.05597    0.988496   0.99143    0.893498   0.752259   0.938258  \n",
      "    15     0.05354    0.055807   0.988629   0.991187   0.894098   0.75741    0.93703   \n",
      "    16     0.052698   0.055227   0.988606   0.991148   0.893565   0.754323   0.937527  \n",
      "    17     0.052191   0.055375   0.988529   0.991406   0.893231   0.750569   0.938537  \n",
      "    18     0.052606   0.055462   0.988644   0.991446   0.893664   0.755468   0.937192  \n",
      "    19     0.051992   0.055912   0.98847    0.99115    0.892967   0.751854   0.9377    \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.055912116318941114,\n",
       " 0.9884702086448669,\n",
       " 0.9911496925354004,\n",
       " 0.892966718673706,\n",
       " 0.7518537807464599,\n",
       " 0.9377002668380737]"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "learn.fit(lrs/60, 1, wds=wd, cycle_len=20,use_clr_beta=(20,10,0.95,0.85))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.save(f'300urn-{S_PREFIX}-nocrop')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn = get_learner(md, m_fn=Unet34Mod, weights=[1,2,1], softmax=True, dice=True)\n",
    "learn.load(f'300urn-{S_PREFIX}-rc')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "88d1dccf1938453ea610e3fa1ce41c3b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Epoch', max=10), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch      trn_loss   val_loss   new_acc_ce rdce_f     carce_f_p_r \n",
      "    0      0.970007   0.942198   0.946138   0.947017   0.793183   0.663295   0.848325  \n",
      "    1      0.969987   0.942243   0.945529   0.947036   0.793563   0.660604   0.850569  \n",
      "    2      0.969987   0.942196   0.946568   0.94724    0.79364    0.659759   0.851028  \n",
      " 82%|████████▏ | 278/340 [01:22<00:18,  3.35it/s, loss=0.97]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Process Process-1466:\n",
      "Process Process-1465:\n",
      "Process Process-1468:\n",
      "Process Process-1469:\n",
      "Process Process-1464:\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/paperspace/anaconda3/envs/lyft/lib/python3.6/multiprocessing/process.py\", line 258, in _bootstrap\n",
      "    self.run()\n",
      "Process Process-1470:\n",
      "  File \"/home/paperspace/anaconda3/envs/lyft/lib/python3.6/multiprocessing/process.py\", line 258, in _bootstrap\n",
      "    self.run()\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/paperspace/anaconda3/envs/lyft/lib/python3.6/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "Process Process-1467:\n",
      "  File \"/home/paperspace/anaconda3/envs/lyft/lib/python3.6/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/paperspace/anaconda3/envs/lyft/lib/python3.6/multiprocessing/process.py\", line 258, in _bootstrap\n",
      "    self.run()\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/paperspace/anaconda3/envs/lyft/lib/python3.6/multiprocessing/process.py\", line 258, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/home/paperspace/anaconda3/envs/lyft/lib/python3.6/multiprocessing/process.py\", line 258, in _bootstrap\n",
      "    self.run()\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/paperspace/anaconda3/envs/lyft/lib/python3.6/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/paperspace/anaconda3/envs/lyft/lib/python3.6/site-packages/torch/utils/data/dataloader.py\", line 57, in _worker_loop\n",
      "    samples = collate_fn([dataset[i] for i in batch_indices])\n",
      "  File \"/home/paperspace/anaconda3/envs/lyft/lib/python3.6/multiprocessing/process.py\", line 258, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/home/paperspace/anaconda3/envs/lyft/lib/python3.6/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/paperspace/anaconda3/envs/lyft/lib/python3.6/site-packages/torch/utils/data/dataloader.py\", line 57, in _worker_loop\n",
      "    samples = collate_fn([dataset[i] for i in batch_indices])\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/paperspace/anaconda3/envs/lyft/lib/python3.6/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/paperspace/anaconda3/envs/lyft/lib/python3.6/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/paperspace/anaconda3/envs/lyft/lib/python3.6/site-packages/torch/utils/data/dataloader.py\", line 57, in _worker_loop\n",
      "    samples = collate_fn([dataset[i] for i in batch_indices])\n",
      "  File \"/home/paperspace/anaconda3/envs/lyft/lib/python3.6/site-packages/torch/utils/data/dataloader.py\", line 57, in <listcomp>\n",
      "    samples = collate_fn([dataset[i] for i in batch_indices])\n",
      "  File \"/home/paperspace/anaconda3/envs/lyft/lib/python3.6/site-packages/torch/utils/data/dataloader.py\", line 57, in <listcomp>\n",
      "    samples = collate_fn([dataset[i] for i in batch_indices])\n",
      "  File \"/home/paperspace/anaconda3/envs/lyft/lib/python3.6/site-packages/torch/utils/data/dataloader.py\", line 57, in _worker_loop\n",
      "    samples = collate_fn([dataset[i] for i in batch_indices])\n",
      "  File \"/home/paperspace/anaconda3/envs/lyft/lib/python3.6/site-packages/torch/utils/data/dataloader.py\", line 57, in _worker_loop\n",
      "    samples = collate_fn([dataset[i] for i in batch_indices])\n",
      "  File \"/home/paperspace/anaconda3/envs/lyft/lib/python3.6/multiprocessing/process.py\", line 258, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/home/paperspace/anaconda3/envs/lyft/lib/python3.6/site-packages/torch/utils/data/dataloader.py\", line 57, in _worker_loop\n",
      "    samples = collate_fn([dataset[i] for i in batch_indices])\n",
      "  File \"/home/paperspace/anaconda3/envs/lyft/lib/python3.6/site-packages/torch/utils/data/dataloader.py\", line 57, in <listcomp>\n",
      "    samples = collate_fn([dataset[i] for i in batch_indices])\n",
      "  File \"<ipython-input-9-e4a515af82db>\", line 26, in __getitem__\n",
      "    return self.get(self.tfms, x, y)\n",
      "  File \"/home/paperspace/anaconda3/envs/lyft/lib/python3.6/site-packages/torch/utils/data/dataloader.py\", line 57, in <listcomp>\n",
      "    samples = collate_fn([dataset[i] for i in batch_indices])\n",
      "  File \"/home/paperspace/anaconda3/envs/lyft/lib/python3.6/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"<ipython-input-9-e4a515af82db>\", line 25, in __getitem__\n",
      "    x,y = self.get_x(idx),self.get_y(idx)\n",
      "  File \"<ipython-input-9-e4a515af82db>\", line 26, in __getitem__\n",
      "    return self.get(self.tfms, x, y)\n",
      "  File \"<ipython-input-9-e4a515af82db>\", line 26, in __getitem__\n",
      "    return self.get(self.tfms, x, y)\n",
      "  File \"/home/paperspace/anaconda3/envs/lyft/lib/python3.6/site-packages/torch/utils/data/dataloader.py\", line 138, in default_collate\n",
      "    return [default_collate(samples) for samples in transposed]\n",
      "  File \"/home/paperspace/anaconda3/envs/lyft/lib/python3.6/site-packages/torch/utils/data/dataloader.py\", line 57, in _worker_loop\n",
      "    samples = collate_fn([dataset[i] for i in batch_indices])\n",
      "  File \"<ipython-input-9-e4a515af82db>\", line 21, in get\n",
      "    x, y = fn(x, y)\n",
      "  File \"<ipython-input-9-e4a515af82db>\", line 13, in get_x\n",
      "    def get_x(self, i): return self.open_fn(os.path.join(self.path, self.fnames[i]))\n",
      "  File \"/home/paperspace/anaconda3/envs/lyft/lib/python3.6/site-packages/torch/utils/data/dataloader.py\", line 138, in <listcomp>\n",
      "    return [default_collate(samples) for samples in transposed]\n",
      "  File \"/home/paperspace/anaconda3/envs/lyft/lib/python3.6/site-packages/torch/utils/data/dataloader.py\", line 57, in <listcomp>\n",
      "    samples = collate_fn([dataset[i] for i in batch_indices])\n",
      "  File \"<ipython-input-9-e4a515af82db>\", line 21, in get\n",
      "    x, y = fn(x, y)\n",
      "  File \"/home/paperspace/anaconda3/envs/lyft/lib/python3.6/site-packages/torchvision/datasets/folder.py\", line 52, in pil_loader\n",
      "    return img.convert('RGB')\n",
      "  File \"/home/paperspace/anaconda3/envs/lyft/lib/python3.6/site-packages/torch/utils/data/dataloader.py\", line 115, in default_collate\n",
      "    return torch.stack(batch, 0, out=out)\n",
      "  File \"<ipython-input-14-6d70a2ea31a4>\", line 2, in <lambda>\n",
      "    return lambda x,y: (tfm(x), y)\n",
      "  File \"<ipython-input-9-e4a515af82db>\", line 25, in __getitem__\n",
      "    x,y = self.get_x(idx),self.get_y(idx)\n",
      "  File \"<ipython-input-9-e4a515af82db>\", line 21, in get\n",
      "    x, y = fn(x, y)\n",
      "  File \"/home/paperspace/anaconda3/envs/lyft/lib/python3.6/site-packages/PIL/Image.py\", line 879, in convert\n",
      "    self.load()\n",
      "KeyboardInterrupt\n",
      "  File \"/home/paperspace/anaconda3/envs/lyft/lib/python3.6/site-packages/torch/utils/data/dataloader.py\", line 57, in <listcomp>\n",
      "    samples = collate_fn([dataset[i] for i in batch_indices])\n",
      "  File \"/home/paperspace/anaconda3/envs/lyft/lib/python3.6/site-packages/torchvision/transforms/transforms.py\", line 579, in __call__\n",
      "    return transform(img)\n",
      "  File \"<ipython-input-14-6d70a2ea31a4>\", line 2, in <lambda>\n",
      "    return lambda x,y: (tfm(x), y)\n",
      "  File \"<ipython-input-18-715c74425d45>\", line 5, in __call__\n",
      "    y = TTF.resized_crop(y, i, j, h, w, self.size, self.interpolation)\n",
      "  File \"<ipython-input-9-e4a515af82db>\", line 25, in __getitem__\n",
      "    x,y = self.get_x(idx),self.get_y(idx)\n",
      "  File \"/home/paperspace/anaconda3/envs/lyft/lib/python3.6/site-packages/PIL/ImageFile.py\", line 231, in load\n",
      "    n, err_code = decoder.decode(b)\n",
      "  File \"/home/paperspace/anaconda3/envs/lyft/lib/python3.6/site-packages/torchvision/transforms/functional.py\", line 287, in resized_crop\n",
      "    img = resize(img, size, interpolation)\n",
      "  File \"/home/paperspace/anaconda3/envs/lyft/lib/python3.6/site-packages/torchvision/transforms/transforms.py\", line 579, in __call__\n",
      "    return transform(img)\n",
      "  File \"<ipython-input-9-e4a515af82db>\", line 14, in get_y\n",
      "    def get_y(self, i): return self.open_y_fn(os.path.join(self.path, self.y[i]))\n",
      "KeyboardInterrupt\n",
      "  File \"<ipython-input-9-e4a515af82db>\", line 14, in get_y\n",
      "    def get_y(self, i): return self.open_y_fn(os.path.join(self.path, self.y[i]))\n",
      "  File \"/home/paperspace/anaconda3/envs/lyft/lib/python3.6/site-packages/torchvision/transforms/functional.py\", line 199, in resize\n",
      "    return img.resize(size[::-1], interpolation)\n",
      "  File \"/home/paperspace/anaconda3/envs/lyft/lib/python3.6/site-packages/torchvision/datasets/folder.py\", line 52, in pil_loader\n",
      "    return img.convert('RGB')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  File \"/home/paperspace/anaconda3/envs/lyft/lib/python3.6/site-packages/torchvision/transforms/transforms.py\", line 42, in __call__\n",
      "    img = t(img)\n",
      "  File \"/home/paperspace/anaconda3/envs/lyft/lib/python3.6/site-packages/torchvision/transforms/transforms.py\", line 42, in __call__\n",
      "    img = t(img)\n",
      "  File \"/home/paperspace/anaconda3/envs/lyft/lib/python3.6/site-packages/torchvision/datasets/folder.py\", line 52, in pil_loader\n",
      "    return img.convert('RGB')\n",
      "  File \"/home/paperspace/anaconda3/envs/lyft/lib/python3.6/site-packages/PIL/Image.py\", line 1749, in resize\n",
      "    return self._new(self.im.resize(size, resample, box))\n",
      "  File \"/home/paperspace/anaconda3/envs/lyft/lib/python3.6/site-packages/PIL/Image.py\", line 879, in convert\n",
      "    self.load()\n",
      "  File \"/home/paperspace/anaconda3/envs/lyft/lib/python3.6/site-packages/torchvision/transforms/transforms.py\", line 232, in __call__\n",
      "    return self.lambd(img)\n",
      "  File \"/home/paperspace/anaconda3/envs/lyft/lib/python3.6/site-packages/torchvision/transforms/transforms.py\", line 232, in __call__\n",
      "    return self.lambd(img)\n",
      "  File \"/home/paperspace/anaconda3/envs/lyft/lib/python3.6/site-packages/PIL/Image.py\", line 546, in _new\n",
      "    if im.mode in ('P', 'PA'):\n",
      "  File \"/home/paperspace/anaconda3/envs/lyft/lib/python3.6/site-packages/PIL/ImageFile.py\", line 231, in load\n",
      "    n, err_code = decoder.decode(b)\n",
      "  File \"/home/paperspace/anaconda3/envs/lyft/lib/python3.6/site-packages/torchvision/transforms/transforms.py\", line 550, in <lambda>\n",
      "    transforms.append(Lambda(lambda img: F.adjust_brightness(img, brightness_factor)))\n",
      "  File \"/home/paperspace/anaconda3/envs/lyft/lib/python3.6/site-packages/torchvision/transforms/transforms.py\", line 558, in <lambda>\n",
      "    transforms.append(Lambda(lambda img: F.adjust_saturation(img, saturation_factor)))\n",
      "KeyboardInterrupt\n",
      "KeyboardInterrupt\n",
      "  File \"/home/paperspace/anaconda3/envs/lyft/lib/python3.6/site-packages/torchvision/transforms/functional.py\", line 406, in adjust_brightness\n",
      "    img = enhancer.enhance(brightness_factor)\n",
      "  File \"/home/paperspace/anaconda3/envs/lyft/lib/python3.6/site-packages/torchvision/transforms/functional.py\", line 445, in adjust_saturation\n",
      "    enhancer = ImageEnhance.Color(img)\n",
      "  File \"/home/paperspace/anaconda3/envs/lyft/lib/python3.6/site-packages/PIL/ImageEnhance.py\", line 54, in __init__\n",
      "    self.degenerate = image.convert(self.intermediate_mode).convert(image.mode)\n",
      "  File \"/home/paperspace/anaconda3/envs/lyft/lib/python3.6/site-packages/PIL/Image.py\", line 979, in convert\n",
      "    im = self.im.convert(mode, dither)\n",
      "KeyboardInterrupt\n",
      "  File \"/home/paperspace/anaconda3/envs/lyft/lib/python3.6/site-packages/PIL/Image.py\", line 879, in convert\n",
      "    self.load()\n",
      "  File \"/home/paperspace/anaconda3/envs/lyft/lib/python3.6/site-packages/PIL/ImageEnhance.py\", line 37, in enhance\n",
      "    return Image.blend(self.degenerate, self.image, factor)\n",
      "ERROR:root:Internal Python error in the inspect module.\n",
      "Below is the traceback from this internal error.\n",
      "\n",
      "  File \"/home/paperspace/anaconda3/envs/lyft/lib/python3.6/site-packages/PIL/ImageFile.py\", line 231, in load\n",
      "    n, err_code = decoder.decode(b)\n",
      "KeyboardInterrupt\n",
      "  File \"/home/paperspace/anaconda3/envs/lyft/lib/python3.6/site-packages/PIL/Image.py\", line 2631, in blend\n",
      "    return im1._new(core.blend(im1.im, im2.im, alpha))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/home/paperspace/anaconda3/envs/lyft/lib/python3.6/site-packages/IPython/core/interactiveshell.py\", line 2963, in run_code\n",
      "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "  File \"<ipython-input-71-04462f90c370>\", line 1, in <module>\n",
      "    learn.fit(lrs/60, 1, wds=wd, cycle_len=10,use_clr_beta=(20,10,0.95,0.85))\n",
      "  File \"/home/paperspace/anaconda3/envs/lyft/lib/python3.6/site-packages/fastai/learner.py\", line 287, in fit\n",
      "    return self.fit_gen(self.model, self.data, layer_opt, n_cycle, **kwargs)\n",
      "  File \"/home/paperspace/anaconda3/envs/lyft/lib/python3.6/site-packages/fastai/learner.py\", line 234, in fit_gen\n",
      "    swa_eval_freq=swa_eval_freq, **kwargs)\n",
      "  File \"/home/paperspace/anaconda3/envs/lyft/lib/python3.6/site-packages/fastai/model.py\", line 139, in fit\n",
      "    loss = model_stepper.step(V(x),V(y), epoch)\n",
      "  File \"/home/paperspace/anaconda3/envs/lyft/lib/python3.6/site-packages/fastai/model.py\", line 54, in step\n",
      "    loss = raw_loss = self.crit(output, y)\n",
      "  File \"/home/paperspace/anaconda3/envs/lyft/lib/python3.6/site-packages/torch/nn/modules/module.py\", line 491, in __call__\n",
      "    result = self.forward(*input, **kwargs)\n",
      "  File \"<ipython-input-39-07a7f051937a>\", line 37, in forward\n",
      "    score = dice_coeff_weight(probs, targets, self.weight)\n",
      "  File \"<ipython-input-39-07a7f051937a>\", line 8, in dice_coeff_weight\n",
      "    i_w = (w*intersection).sum()\n",
      "KeyboardInterrupt\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/paperspace/anaconda3/envs/lyft/lib/python3.6/site-packages/IPython/core/interactiveshell.py\", line 1863, in showtraceback\n",
      "    stb = value._render_traceback_()\n",
      "AttributeError: 'KeyboardInterrupt' object has no attribute '_render_traceback_'\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/paperspace/anaconda3/envs/lyft/lib/python3.6/site-packages/IPython/core/ultratb.py\", line 1095, in get_records\n",
      "    return _fixed_getinnerframes(etb, number_of_lines_of_context, tb_offset)\n",
      "  File \"/home/paperspace/anaconda3/envs/lyft/lib/python3.6/site-packages/IPython/core/ultratb.py\", line 311, in wrapped\n",
      "    return f(*args, **kwargs)\n",
      "  File \"/home/paperspace/anaconda3/envs/lyft/lib/python3.6/site-packages/IPython/core/ultratb.py\", line 345, in _fixed_getinnerframes\n",
      "    records = fix_frame_records_filenames(inspect.getinnerframes(etb, context))\n",
      "  File \"/home/paperspace/anaconda3/envs/lyft/lib/python3.6/inspect.py\", line 1483, in getinnerframes\n",
      "    frameinfo = (tb.tb_frame,) + getframeinfo(tb, context)\n",
      "  File \"/home/paperspace/anaconda3/envs/lyft/lib/python3.6/inspect.py\", line 1441, in getframeinfo\n",
      "    filename = getsourcefile(frame) or getfile(frame)\n",
      "  File \"/home/paperspace/anaconda3/envs/lyft/lib/python3.6/inspect.py\", line 696, in getsourcefile\n",
      "    if getattr(getmodule(object, filename), '__loader__', None) is not None:\n",
      "  File \"/home/paperspace/anaconda3/envs/lyft/lib/python3.6/inspect.py\", line 733, in getmodule\n",
      "    if ismodule(module) and hasattr(module, '__file__'):\n",
      "  File \"/home/paperspace/anaconda3/envs/lyft/lib/python3.6/site-packages/torch/utils/data/dataloader.py\", line 178, in handler\n",
      "    _error_if_any_worker_fails()\n",
      "RuntimeError: DataLoader worker (pid 9035) exited unexpectedly with exit code 1.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "KeyboardInterrupt\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m"
     ]
    }
   ],
   "source": [
    "learn.fit(lrs/60, 1, wds=wd, cycle_len=10,use_clr=(20,2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 512"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val Labels: 400\n",
      "Val x:400, y:400\n",
      "Trn x:10880, y:10880\n",
      "All x:10880\n"
     ]
    }
   ],
   "source": [
    "ext = ''\n",
    "sz=320\n",
    "bs=32\n",
    "random_crop=True\n",
    "md = torch_loader(ext, PATH, bs, sz, workers, random_crop, pseudo_label, val_folder, val_bs=16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn = get_learner(md, m_fn=Unet34Mod, weights=[1,4,1], softmax=True, dice=False)\n",
    "learn.load(f'300urn-{S_PREFIX}-rc')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr=1e-2\n",
    "wd=1e-7\n",
    "\n",
    "lrs = np.array([lr/200,lr/20,lr])/2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.unfreeze()\n",
    "learn.set_bn_freeze(learn.model.rn, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Epoch', max=4), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch      trn_loss   val_loss   new_acc_ce rdce_f     carce_f_p_r \n",
      "    0      0.053489   0.104588   0.973112   0.97597    0.802356   0.625759   0.877529  \n",
      "    1      0.048309   0.080375   0.981179   0.985478   0.811278   0.644859   0.878185  \n",
      "    2      0.046582   0.070281   0.984065   0.989717   0.823064   0.65916    0.886763  \n",
      "    3      0.04611    0.067752   0.985115   0.991957   0.827901   0.668408   0.88865   \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.06775206550955773,\n",
       " 0.9851145792007446,\n",
       " 0.991957426071167,\n",
       " 0.8279013395309448,\n",
       " 0.6684084177017212,\n",
       " 0.8886496996879578]"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "learn.fit(lrs, 1, wds=wd, cycle_len=4,use_clr=(20,2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Epoch', max=4), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch      trn_loss   val_loss   new_acc_ce rdce_f     carce_f_p_r \n",
      "    0      0.045932   0.068682   0.984929   0.991801   0.82812    0.669846   0.887793  \n",
      "    1      0.045318   0.069274   0.985143   0.991819   0.827421   0.665278   0.889384  \n",
      "    2      0.045906   0.069047   0.985187   0.991735   0.827113   0.665352   0.889049  \n",
      "    3      0.044951   0.068181   0.985499   0.991803   0.82834    0.666932   0.889667  \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.06818125158548355,\n",
       " 0.9854988932609559,\n",
       " 0.9918025398254394,\n",
       " 0.8283400535583496,\n",
       " 0.6669318664073944,\n",
       " 0.8896665668487549]"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "learn.fit(lrs/20, 1, wds=wd, cycle_len=4,use_clr=(20,2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.save(f'600urn-{S_PREFIX}-320')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val Labels: 400\n",
      "Val x:400, y:400\n",
      "Trn x:10880, y:10880\n",
      "All x:10880\n"
     ]
    }
   ],
   "source": [
    "ext = ''\n",
    "sz=384\n",
    "bs=32\n",
    "random_crop=True\n",
    "md = torch_loader(ext, PATH, bs, sz, workers, random_crop, pseudo_label, val_folder, val_bs=16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn = get_learner(md, m_fn=Unet34Mod, weights=[1,4,1], softmax=True, dice=False)\n",
    "learn.load(f'600urn-{S_PREFIX}-320')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr=1e-3\n",
    "wd=1e-7\n",
    "\n",
    "lrs = np.array([lr/200,lr/20,lr])/2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.unfreeze()\n",
    "learn.set_bn_freeze(learn.model.rn, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3a7c9581518a4b5dac3c9dbcc0c1058b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Epoch', max=8), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch      trn_loss   val_loss   new_acc_ce rdce_f     carce_f_p_r \n",
      "    0      0.040725   0.055485   0.988269   0.992129   0.867376   0.781533   0.894383  \n",
      "    1      0.039416   0.051697   0.988776   0.992362   0.871455   0.776658   0.901703  \n",
      "    2      0.038314   0.049796   0.989238   0.992746   0.872382   0.775361   0.902989  \n",
      "    3      0.037229   0.048926   0.989669   0.992596   0.872614   0.795252   0.895866  \n",
      "    4      0.037591   0.049001   0.989725   0.992958   0.873045   0.797261   0.895493  \n",
      "    5      0.03702    0.048559   0.989891   0.992859   0.872909   0.802478   0.893892  \n",
      "    6      0.036255   0.047538   0.989944   0.99296    0.875282   0.800763   0.897533  \n",
      "    7      0.037069   0.046715   0.990067   0.993009   0.875025   0.796207   0.898874  \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.04671512633562088,\n",
       " 0.990066659450531,\n",
       " 0.9930088424682617,\n",
       " 0.8750253248214722,\n",
       " 0.796207218170166,\n",
       " 0.8988741326332093]"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "learn.fit(lrs, 1, wds=wd, cycle_len=8,use_clr=(20,2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.save(f'600urn-{S_PREFIX}-384')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Nocrop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val Labels: 400\n",
      "Val x:400, y:400\n",
      "Trn x:10880, y:10880\n",
      "All x:10880\n"
     ]
    }
   ],
   "source": [
    "ext = ''\n",
    "sz=384\n",
    "bs=16\n",
    "random_crop=False\n",
    "md = torch_loader(ext, PATH, bs, sz, workers, random_crop, pseudo_label, val_folder, val_bs=16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn = get_learner(md, m_fn=Unet34Mod, weights=[1,4,2], softmax=True)\n",
    "learn.load(f'600urn-{S_PREFIX}-384')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr=1e-3\n",
    "wd=1e-7\n",
    "\n",
    "lrs = np.array([lr/200,lr/20,lr])/2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.unfreeze()\n",
    "# learn.bn_freeze(True)\n",
    "learn.set_bn_freeze(learn.model.rn, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Epoch', max=8), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch      trn_loss   val_loss   new_acc_ce rdce_f     carce_f_p_r \n",
      "    0      0.016879   0.038848   0.990537   0.991297   0.875104   0.812929   0.893647  \n",
      "    1      0.014396   0.035888   0.99096    0.991988   0.879553   0.819282   0.897452  \n",
      "    2      0.014349   0.033638   0.990869   0.99204    0.884501   0.800939   0.910134  \n",
      "    3      0.012756   0.032437   0.991515   0.992272   0.883333   0.827527   0.89978   \n",
      "    4      0.011987   0.032491   0.99165    0.992655   0.881714   0.826788   0.898     \n",
      "    5      0.011842   0.031412   0.991695   0.99226    0.885216   0.82711    0.902687  \n",
      "    6      0.011568   0.031115   0.991677   0.992407   0.888502   0.817356   0.910076  \n",
      "    7      0.011528   0.031186   0.991981   0.992798   0.884804   0.833704   0.899977  \n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.031186033338308335,\n",
       " 0.9919812250137329,\n",
       " 0.9927979564666748,\n",
       " 0.8848035264015198,\n",
       " 0.8337035083770752,\n",
       " 0.8999767613410949]"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "learn.fit(lrs, 1, wds=wd, cycle_len=8,use_clr=(20,2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ae56e5d986fe4bda8a8252f7ed406a84",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Epoch', max=6), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch      trn_loss   val_loss   new_acc_ce rdce_f     carce_f_p_r \n",
      "    0      0.01127    0.031098   0.992009   0.993092   0.886458   0.830219   0.903275  \n",
      "    1      0.011584   0.030888   0.991864   0.992836   0.887783   0.822799   0.907208  \n",
      "    2      0.011409   0.031044   0.991964   0.993074   0.887172   0.82682    0.905131  \n",
      "    3      0.010985   0.030893   0.99201    0.993118   0.887371   0.826897   0.905405  \n",
      "    4      0.011041   0.03095    0.991892   0.992713   0.887056   0.8254     0.905429  \n",
      "    5      0.011192   0.030998   0.991987   0.993092   0.886933   0.828259   0.904498  \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.03099840447306633,\n",
       " 0.9919870662689209,\n",
       " 0.9930915832519531,\n",
       " 0.8869334626197815,\n",
       " 0.8282593774795532,\n",
       " 0.9044981193542481]"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "learn.fit(lrs/50, 1, wds=wd, cycle_len=6,use_clr=(20,2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.save(f'600urn-{S_PREFIX}-384-nocrop')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Change Weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val Labels: 400\n",
      "Val x:400, y:400\n",
      "Trn x:10880, y:10880\n",
      "All x:10880\n"
     ]
    }
   ],
   "source": [
    "ext = ''\n",
    "sz=384\n",
    "bs=16\n",
    "random_crop=False\n",
    "md = torch_loader(ext, PATH, bs, sz, workers, random_crop, pseudo_label, val_folder, val_bs=16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn = get_learner(md, m_fn=Unet34Mod, weights=[1,5,1], softmax=True)\n",
    "learn.load(f'600urn-{S_PREFIX}-384-nocrop')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr=5e-4\n",
    "wd=1e-7\n",
    "\n",
    "lrs = np.array([lr/200,lr/20,lr])/2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.unfreeze()\n",
    "# learn.bn_freeze(True)\n",
    "learn.set_bn_freeze(learn.model.rn, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Epoch', max=6), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch      trn_loss   val_loss   new_acc_ce rdce_f     carce_f_p_r \n",
      "    0      0.01392    0.037564   0.991674   0.994386   0.891247   0.797518   0.920005  \n",
      "    1      0.012501   0.037288   0.991864   0.994379   0.890891   0.803872   0.9174    \n",
      "    2      0.012965   0.037297   0.991996   0.994419   0.890178   0.810147   0.914059  \n",
      "    3      0.013114   0.037404   0.992002   0.994632   0.890476   0.810833   0.914259  \n",
      "    4      0.012298   0.037381   0.992024   0.994645   0.891553   0.810924   0.915733  \n",
      "    5      0.012072   0.036974   0.992022   0.994669   0.892053   0.810652   0.916499  \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.036974334716796876,\n",
       " 0.9920220708847046,\n",
       " 0.9946693015098572,\n",
       " 0.8920534539222718,\n",
       " 0.8106518983840942,\n",
       " 0.9164994955062866]"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "learn.fit(lrs, 1, wds=wd, cycle_len=6,use_clr=(20,2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.save(f'600urn-{S_PREFIX}-384-nocrop-w8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.load(f'600urn-{S_PREFIX}-384-nocrop-w8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.unfreeze()\n",
    "# learn.bn_freeze(True)\n",
    "learn.set_bn_freeze(learn.model.rn, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8934a48ee5554f2bbeed912f9a227f1b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Epoch', max=3), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch      trn_loss   val_loss   new_acc_ce rdce_f     carce_f_p_r \n",
      "    0      0.013442   0.038221   0.992004   0.993738   0.888606   0.821091   0.908769  \n",
      "    1      0.012906   0.037888   0.991942   0.994258   0.890366   0.812273   0.913754  \n",
      "    2      0.013243   0.038058   0.991933   0.994149   0.888776   0.813987   0.911079  \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.03805769369006157,\n",
       " 0.9919331932067871,\n",
       " 0.9941486191749572,\n",
       " 0.8887755012512207,\n",
       " 0.8139868092536926,\n",
       " 0.911078839302063]"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "learn.fit(lrs/40, 1, wds=wd, cycle_len=3,use_clr=(20,2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.save(f'600urn-{S_PREFIX}-384-nocrop-w8-tmp')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Change Weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val Labels: 400\n",
      "Val x:400, y:400\n",
      "Trn x:10880, y:10880\n",
      "All x:10880\n"
     ]
    }
   ],
   "source": [
    "ext = ''\n",
    "sz=384\n",
    "bs=16\n",
    "random_crop=False\n",
    "md = torch_loader(ext, PATH, bs, sz, workers, random_crop, pseudo_label, val_folder, val_bs=16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn = get_learner(md, m_fn=Unet34Mod, weights=[1,5,1], softmax=True)\n",
    "learn.load(f'600urn-{S_PREFIX}-384-nocrop')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr=5e-4\n",
    "wd=1e-7\n",
    "\n",
    "lrs = np.array([lr/200,lr/20,lr])/2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.unfreeze()\n",
    "# learn.bn_freeze(True)\n",
    "# learn.set_bn_freeze(learn.model.rn, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b4e73fac2be84b1b822b9067b3a154e1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Epoch', max=6), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch      trn_loss   val_loss   new_acc_ce rdce_f     carce_f_p_r \n",
      "    0      0.01335    0.037732   0.99177    0.994511   0.890117   0.802169   0.916888  \n",
      "    1      0.012382   0.037682   0.991805   0.994391   0.890295   0.804781   0.916274  \n",
      "    2      0.012682   0.036923   0.991814   0.994446   0.891895   0.79932    0.920062  \n",
      "    3      0.01218    0.037375   0.991923   0.994767   0.89062    0.808708   0.915261  \n",
      "    4      0.01244    0.03697    0.991999   0.994663   0.891578   0.808123   0.916687  \n",
      "    5      0.012399   0.037092   0.991979   0.994535   0.892773   0.806531   0.918908  \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.03709187183529138,\n",
       " 0.9919790077209473,\n",
       " 0.9945353794097901,\n",
       " 0.8927726554870605,\n",
       " 0.8065314531326294,\n",
       " 0.9189083170890808]"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "learn.fit(lrs, 1, wds=wd, cycle_len=6,use_clr=(20,2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.save(f'600urn-{S_PREFIX}-384-nocrop-w8-pt2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.unfreeze()\n",
    "# learn.bn_freeze(True)\n",
    "# learn.set_bn_freeze(learn.model.rn, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ef884142c72e4402bb01e3ed1145c6e4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Epoch', max=3), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch      trn_loss   val_loss   new_acc_ce rdce_f     carce_f_p_r \n",
      "    0      0.01239    0.03721    0.992106   0.994643   0.89128    0.813075   0.914635  \n",
      "    1      0.01225    0.03705    0.992056   0.994562   0.891575   0.810552   0.915958  \n",
      "    2      0.013008   0.037125   0.992048   0.994625   0.891642   0.81077    0.915932  \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.037124730758368966,\n",
       " 0.9920481014251709,\n",
       " 0.9946248435974121,\n",
       " 0.8916415667533875,\n",
       " 0.8107698273658752,\n",
       " 0.915931830406189]"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "learn.fit(lrs/30, 1, wds=wd, cycle_len=3,use_clr=(20,2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.save(f'600urn-{S_PREFIX}-384-nocrop-w8-pt3')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0a49080332ba4b5694b45b74674b30c9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Epoch', max=3), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch      trn_loss   val_loss   new_acc_ce rdce_f     carce_f_p_r \n",
      "    0      0.012664   0.036927   0.99203    0.994574   0.892283   0.809002   0.91738   \n",
      " 38%|███▊      | 257/680 [01:19<02:11,  3.22it/s, loss=0.0123]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Process Process-571:\n",
      "Process Process-572:\n",
      "Process Process-569:\n",
      "KeyboardInterrupt\n",
      "ERROR:root:Internal Python error in the inspect module.\n",
      "Below is the traceback from this internal error.\n",
      "\n",
      "Process Process-574:\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/home/paperspace/anaconda3/envs/lyft/lib/python3.6/site-packages/IPython/core/interactiveshell.py\", line 2963, in run_code\n",
      "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "  File \"<ipython-input-81-32ff456c0ed8>\", line 1, in <module>\n",
      "    learn.fit(lrs/100, 1, wds=wd, cycle_len=3,use_clr=(20,2))\n",
      "  File \"/home/paperspace/anaconda3/envs/lyft/lib/python3.6/site-packages/fastai/learner.py\", line 287, in fit\n",
      "    return self.fit_gen(self.model, self.data, layer_opt, n_cycle, **kwargs)\n",
      "  File \"/home/paperspace/anaconda3/envs/lyft/lib/python3.6/site-packages/fastai/learner.py\", line 234, in fit_gen\n",
      "    swa_eval_freq=swa_eval_freq, **kwargs)\n",
      "  File \"/home/paperspace/anaconda3/envs/lyft/lib/python3.6/site-packages/fastai/model.py\", line 139, in fit\n",
      "    loss = model_stepper.step(V(x),V(y), epoch)\n",
      "  File \"/home/paperspace/anaconda3/envs/lyft/lib/python3.6/site-packages/fastai/model.py\", line 70, in step\n",
      "    self.opt.step()\n",
      "  File \"/home/paperspace/anaconda3/envs/lyft/lib/python3.6/site-packages/torch/optim/adam.py\", line 92, in step\n",
      "    exp_avg.mul_(beta1).add_(1 - beta1, grad)\n",
      "KeyboardInterrupt\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/paperspace/anaconda3/envs/lyft/lib/python3.6/site-packages/IPython/core/interactiveshell.py\", line 1863, in showtraceback\n",
      "    stb = value._render_traceback_()\n",
      "AttributeError: 'KeyboardInterrupt' object has no attribute '_render_traceback_'\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/paperspace/anaconda3/envs/lyft/lib/python3.6/site-packages/IPython/core/ultratb.py\", line 1095, in get_records\n",
      "    return _fixed_getinnerframes(etb, number_of_lines_of_context, tb_offset)\n",
      "  File \"/home/paperspace/anaconda3/envs/lyft/lib/python3.6/site-packages/IPython/core/ultratb.py\", line 311, in wrapped\n",
      "    return f(*args, **kwargs)\n",
      "  File \"/home/paperspace/anaconda3/envs/lyft/lib/python3.6/site-packages/IPython/core/ultratb.py\", line 345, in _fixed_getinnerframes\n",
      "    records = fix_frame_records_filenames(inspect.getinnerframes(etb, context))\n",
      "  File \"/home/paperspace/anaconda3/envs/lyft/lib/python3.6/inspect.py\", line 1483, in getinnerframes\n",
      "    frameinfo = (tb.tb_frame,) + getframeinfo(tb, context)\n",
      "  File \"/home/paperspace/anaconda3/envs/lyft/lib/python3.6/inspect.py\", line 1445, in getframeinfo\n",
      "    lines, lnum = findsource(frame)\n",
      "  File \"/home/paperspace/anaconda3/envs/lyft/lib/python3.6/site-packages/IPython/core/ultratb.py\", line 165, in findsource\n",
      "    file = getsourcefile(object) or getfile(object)\n",
      "  File \"/home/paperspace/anaconda3/envs/lyft/lib/python3.6/inspect.py\", line 696, in getsourcefile\n",
      "    if getattr(getmodule(object, filename), '__loader__', None) is not None:\n",
      "  File \"/home/paperspace/anaconda3/envs/lyft/lib/python3.6/inspect.py\", line 735, in getmodule\n",
      "    if f == _filesbymodname.get(modname, None):\n",
      "  File \"/home/paperspace/anaconda3/envs/lyft/lib/python3.6/site-packages/torch/utils/data/dataloader.py\", line 178, in handler\n",
      "    _error_if_any_worker_fails()\n",
      "RuntimeError: DataLoader worker (pid 21190) exited unexpectedly with exit code 1.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Process Process-568:\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "Process Process-573:\n",
      "Process Process-570:\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/paperspace/anaconda3/envs/lyft/lib/python3.6/multiprocessing/process.py\", line 258, in _bootstrap\n",
      "    self.run()\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/paperspace/anaconda3/envs/lyft/lib/python3.6/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/paperspace/anaconda3/envs/lyft/lib/python3.6/site-packages/torch/utils/data/dataloader.py\", line 52, in _worker_loop\n",
      "    r = index_queue.get()\n",
      "  File \"/home/paperspace/anaconda3/envs/lyft/lib/python3.6/multiprocessing/process.py\", line 258, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/home/paperspace/anaconda3/envs/lyft/lib/python3.6/multiprocessing/process.py\", line 258, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/home/paperspace/anaconda3/envs/lyft/lib/python3.6/multiprocessing/process.py\", line 258, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/home/paperspace/anaconda3/envs/lyft/lib/python3.6/multiprocessing/process.py\", line 258, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/home/paperspace/anaconda3/envs/lyft/lib/python3.6/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/paperspace/anaconda3/envs/lyft/lib/python3.6/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/paperspace/anaconda3/envs/lyft/lib/python3.6/multiprocessing/queues.py\", line 335, in get\n",
      "    res = self._reader.recv_bytes()\n",
      "  File \"/home/paperspace/anaconda3/envs/lyft/lib/python3.6/multiprocessing/process.py\", line 258, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/home/paperspace/anaconda3/envs/lyft/lib/python3.6/multiprocessing/process.py\", line 258, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/home/paperspace/anaconda3/envs/lyft/lib/python3.6/site-packages/torch/utils/data/dataloader.py\", line 57, in _worker_loop\n",
      "    samples = collate_fn([dataset[i] for i in batch_indices])\n",
      "  File \"/home/paperspace/anaconda3/envs/lyft/lib/python3.6/site-packages/torch/utils/data/dataloader.py\", line 52, in _worker_loop\n",
      "    r = index_queue.get()\n",
      "  File \"/home/paperspace/anaconda3/envs/lyft/lib/python3.6/multiprocessing/connection.py\", line 216, in recv_bytes\n",
      "    buf = self._recv_bytes(maxlength)\n",
      "  File \"/home/paperspace/anaconda3/envs/lyft/lib/python3.6/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/paperspace/anaconda3/envs/lyft/lib/python3.6/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/paperspace/anaconda3/envs/lyft/lib/python3.6/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/paperspace/anaconda3/envs/lyft/lib/python3.6/multiprocessing/queues.py\", line 335, in get\n",
      "    res = self._reader.recv_bytes()\n",
      "  File \"/home/paperspace/anaconda3/envs/lyft/lib/python3.6/site-packages/torch/utils/data/dataloader.py\", line 57, in <listcomp>\n",
      "    samples = collate_fn([dataset[i] for i in batch_indices])\n",
      "  File \"/home/paperspace/anaconda3/envs/lyft/lib/python3.6/multiprocessing/connection.py\", line 407, in _recv_bytes\n",
      "    buf = self._recv(4)\n",
      "  File \"/home/paperspace/anaconda3/envs/lyft/lib/python3.6/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/paperspace/anaconda3/envs/lyft/lib/python3.6/multiprocessing/connection.py\", line 216, in recv_bytes\n",
      "    buf = self._recv_bytes(maxlength)\n",
      "  File \"<ipython-input-9-e4a515af82db>\", line 25, in __getitem__\n",
      "    x,y = self.get_x(idx),self.get_y(idx)\n",
      "  File \"/home/paperspace/anaconda3/envs/lyft/lib/python3.6/site-packages/torch/utils/data/dataloader.py\", line 52, in _worker_loop\n",
      "    r = index_queue.get()\n",
      "  File \"/home/paperspace/anaconda3/envs/lyft/lib/python3.6/site-packages/torch/utils/data/dataloader.py\", line 52, in _worker_loop\n",
      "    r = index_queue.get()\n",
      "  File \"/home/paperspace/anaconda3/envs/lyft/lib/python3.6/site-packages/torch/utils/data/dataloader.py\", line 57, in _worker_loop\n",
      "    samples = collate_fn([dataset[i] for i in batch_indices])\n",
      "  File \"/home/paperspace/anaconda3/envs/lyft/lib/python3.6/site-packages/torch/utils/data/dataloader.py\", line 57, in _worker_loop\n",
      "    samples = collate_fn([dataset[i] for i in batch_indices])\n",
      "  File \"<ipython-input-9-e4a515af82db>\", line 13, in get_x\n",
      "    def get_x(self, i): return self.open_fn(os.path.join(self.path, self.fnames[i]))\n",
      "  File \"/home/paperspace/anaconda3/envs/lyft/lib/python3.6/multiprocessing/connection.py\", line 407, in _recv_bytes\n",
      "    buf = self._recv(4)\n",
      "  File \"/home/paperspace/anaconda3/envs/lyft/lib/python3.6/multiprocessing/connection.py\", line 379, in _recv\n",
      "    chunk = read(handle, remaining)\n",
      "  File \"/home/paperspace/anaconda3/envs/lyft/lib/python3.6/multiprocessing/queues.py\", line 335, in get\n",
      "    res = self._reader.recv_bytes()\n",
      "  File \"/home/paperspace/anaconda3/envs/lyft/lib/python3.6/site-packages/torchvision/datasets/folder.py\", line 52, in pil_loader\n",
      "    return img.convert('RGB')\n",
      "  File \"/home/paperspace/anaconda3/envs/lyft/lib/python3.6/site-packages/torch/utils/data/dataloader.py\", line 57, in <listcomp>\n",
      "    samples = collate_fn([dataset[i] for i in batch_indices])\n",
      "  File \"/home/paperspace/anaconda3/envs/lyft/lib/python3.6/site-packages/torch/utils/data/dataloader.py\", line 57, in <listcomp>\n",
      "    samples = collate_fn([dataset[i] for i in batch_indices])\n",
      "  File \"/home/paperspace/anaconda3/envs/lyft/lib/python3.6/multiprocessing/connection.py\", line 216, in recv_bytes\n",
      "    buf = self._recv_bytes(maxlength)\n",
      "KeyboardInterrupt\n",
      "  File \"/home/paperspace/anaconda3/envs/lyft/lib/python3.6/multiprocessing/connection.py\", line 379, in _recv\n",
      "    chunk = read(handle, remaining)\n",
      "  File \"/home/paperspace/anaconda3/envs/lyft/lib/python3.6/site-packages/PIL/Image.py\", line 879, in convert\n",
      "    self.load()\n",
      "  File \"/home/paperspace/anaconda3/envs/lyft/lib/python3.6/multiprocessing/queues.py\", line 335, in get\n",
      "    res = self._reader.recv_bytes()\n",
      "KeyboardInterrupt\n",
      "  File \"<ipython-input-9-e4a515af82db>\", line 25, in __getitem__\n",
      "    x,y = self.get_x(idx),self.get_y(idx)\n",
      "  File \"/home/paperspace/anaconda3/envs/lyft/lib/python3.6/multiprocessing/connection.py\", line 407, in _recv_bytes\n",
      "    buf = self._recv(4)\n",
      "  File \"/home/paperspace/anaconda3/envs/lyft/lib/python3.6/multiprocessing/connection.py\", line 216, in recv_bytes\n",
      "    buf = self._recv_bytes(maxlength)\n",
      "  File \"/home/paperspace/anaconda3/envs/lyft/lib/python3.6/site-packages/PIL/ImageFile.py\", line 231, in load\n",
      "    n, err_code = decoder.decode(b)\n",
      "  File \"/home/paperspace/anaconda3/envs/lyft/lib/python3.6/multiprocessing/connection.py\", line 407, in _recv_bytes\n",
      "    buf = self._recv(4)\n",
      "  File \"/home/paperspace/anaconda3/envs/lyft/lib/python3.6/multiprocessing/connection.py\", line 379, in _recv\n",
      "    chunk = read(handle, remaining)\n",
      "KeyboardInterrupt\n",
      "  File \"<ipython-input-9-e4a515af82db>\", line 26, in __getitem__\n",
      "    return self.get(self.tfms, x, y)\n",
      "  File \"<ipython-input-9-e4a515af82db>\", line 13, in get_x\n",
      "    def get_x(self, i): return self.open_fn(os.path.join(self.path, self.fnames[i]))\n",
      "  File \"/home/paperspace/anaconda3/envs/lyft/lib/python3.6/multiprocessing/connection.py\", line 379, in _recv\n",
      "    chunk = read(handle, remaining)\n",
      "  File \"<ipython-input-9-e4a515af82db>\", line 21, in get\n",
      "    x, y = fn(x, y)\n",
      "KeyboardInterrupt\n",
      "  File \"/home/paperspace/anaconda3/envs/lyft/lib/python3.6/site-packages/torchvision/datasets/folder.py\", line 52, in pil_loader\n",
      "    return img.convert('RGB')\n",
      "KeyboardInterrupt\n",
      "  File \"<ipython-input-11-b7be93f859f9>\", line 7, in crop_bg_pil\n",
      "    return TTF.crop(x, top, 0, bot-top, w+pad_right), TTF.crop(y, top, 0, bot-top, w+pad_right)\n",
      "  File \"/home/paperspace/anaconda3/envs/lyft/lib/python3.6/site-packages/PIL/Image.py\", line 879, in convert\n",
      "    self.load()\n",
      "  File \"/home/paperspace/anaconda3/envs/lyft/lib/python3.6/site-packages/torchvision/transforms/functional.py\", line 255, in crop\n",
      "    return img.crop((j, i, j + w, i + h))\n",
      "  File \"/home/paperspace/anaconda3/envs/lyft/lib/python3.6/site-packages/PIL/ImageFile.py\", line 231, in load\n",
      "    n, err_code = decoder.decode(b)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  File \"/home/paperspace/anaconda3/envs/lyft/lib/python3.6/site-packages/PIL/Image.py\", line 1078, in crop\n",
      "    return self._new(self._crop(self.im, box))\n",
      "KeyboardInterrupt\n",
      "  File \"/home/paperspace/anaconda3/envs/lyft/lib/python3.6/site-packages/PIL/Image.py\", line 1101, in _crop\n",
      "    return im.crop((x0, y0, x1, y1))\n"
     ]
    }
   ],
   "source": [
    "learn.fit(lrs/100, 1, wds=wd, cycle_len=3,use_clr=(20,2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  },
  "toc": {
   "colors": {
    "hover_highlight": "#DAA520",
    "navigate_num": "#000000",
    "navigate_text": "#333333",
    "running_highlight": "#FF0000",
    "selected_highlight": "#FFD700",
    "sidebar_border": "#EEEEEE",
    "wrapper_background": "#FFFFFF"
   },
   "moveMenuLeft": true,
   "nav_menu": {
    "height": "86px",
    "width": "252px"
   },
   "navigate_menu": true,
   "number_sections": true,
   "sideBar": true,
   "threshold": 4,
   "toc_cell": false,
   "toc_section_display": "block",
   "toc_window_display": false,
   "widenNotebook": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
