{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "%reload_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fastai.conv_learner import *\n",
    "# from fastai.dataset import *\n",
    "from fastai.models.resnet import vgg_resnet50\n",
    "\n",
    "import json\n",
    "from glob import glob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.backends.cudnn.benchmark=True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH = Path('../data/all')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_img(im, figsize=None, ax=None, alpha=None):\n",
    "    if not ax: fig,ax = plt.subplots(figsize=figsize)\n",
    "    ax.imshow(im, alpha=alpha)\n",
    "    ax.set_axis_off()\n",
    "    return ax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "VEHICLES=10\n",
    "ROADS=7\n",
    "ROAD_LINES=6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "S_PREFIX = '24-unet'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision.datasets.folder import pil_loader\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.transforms.functional as TTF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MatchedFilesDataset(Dataset):\n",
    "    def __init__(self, fnames, y, tfms, path):\n",
    "        self.path,self.fnames = path,fnames\n",
    "        self.open_fn = pil_loader\n",
    "        self.y=y\n",
    "        self.open_y_fn = pil_loader\n",
    "        assert(len(fnames)==len(y))\n",
    "        \n",
    "        self.n = self.get_n()\n",
    "        self.c = self.get_c()\n",
    "        self.tfms = tfms\n",
    "#         self.sz = self.get_sz()\n",
    "        \n",
    "#     def get_sz(self): return self.transform.sz\n",
    "    def get_x(self, i): return self.open_fn(os.path.join(self.path, self.fnames[i]))\n",
    "    def get_y(self, i): return self.open_y_fn(os.path.join(self.path, self.y[i]))\n",
    "    def get_n(self): return len(self.fnames)\n",
    "    def get_c(self): return 2\n",
    "    \n",
    "    def get(self, tfms, x, y):\n",
    "        for fn in tfms:\n",
    "            #pdb.set_trace()\n",
    "            x, y = fn(x, y)\n",
    "        return (x, y)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        x,y = self.get_x(idx),self.get_y(idx)\n",
    "        return self.get(self.tfms, x, y)\n",
    "    \n",
    "    def __len__(self): return self.n\n",
    "\n",
    "    def resize_imgs(self, targ, new_path):\n",
    "        dest = resize_imgs(self.fnames, targ, self.path, new_path)\n",
    "        return self.__class__(self.fnames, self.y, self.transform, dest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Seems to speed up training by ~2%\n",
    "class DataPrefetcher():\n",
    "    def __init__(self, loader, stop_after=None):\n",
    "        self.loader = loader\n",
    "        self.dataset = loader.dataset\n",
    "        self.stream = torch.cuda.Stream()\n",
    "        self.stop_after = stop_after\n",
    "        self.next_input = None\n",
    "        self.next_target = None\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.loader)\n",
    "    \n",
    "    def preload(self):\n",
    "        try:\n",
    "            self.next_input, self.next_target = next(self.loaditer)\n",
    "        except StopIteration:\n",
    "            self.next_input = None\n",
    "            self.next_target = None\n",
    "            return\n",
    "        with torch.cuda.stream(self.stream):\n",
    "            self.next_input = self.next_input.cuda(async=True)\n",
    "            self.next_target = self.next_target.cuda(async=True)\n",
    "\n",
    "    def __iter__(self):\n",
    "        count = 0\n",
    "        self.loaditer = iter(self.loader)\n",
    "        self.preload()\n",
    "        while self.next_input is not None:\n",
    "            torch.cuda.current_stream().wait_stream(self.stream)\n",
    "            input = self.next_input\n",
    "            target = self.next_target\n",
    "            self.preload()\n",
    "            count += 1\n",
    "            yield input, target\n",
    "            if type(self.stop_after) is int and (count > self.stop_after):\n",
    "                break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def crop_bg_pil(x,y):\n",
    "    w, h = x.size\n",
    "    top = int(h/3.75)\n",
    "    bot = int(h*.9 + h/150)\n",
    "    pad_right=16-w%16\n",
    "    return TTF.crop(x, top, 0, bot-top, w+pad_right), TTF.crop(y, top, 0, bot-top, w+pad_right)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RHF(object):\n",
    "    def __init__(self, p=0.5): self.p = p\n",
    "    def __call__(self, x, y):\n",
    "        if random.random() < self.p:\n",
    "            return TTF.hflip(x), TTF.hflip(y)\n",
    "        return x,y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RR(object):\n",
    "    def __init__(self, degrees=2): self.degrees = degrees\n",
    "    def __call__(self, x, y):\n",
    "        angle = random.uniform(-self.degrees, self.degrees)\n",
    "        return TTF.rotate(x, angle), TTF.rotate(y, angle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tfm_x_wrapper(tfm):\n",
    "    return lambda x,y: (tfm(x), y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RRC(transforms.RandomResizedCrop):\n",
    "    def __call__(self, x, y):\n",
    "        i, j, h, w = self.get_params(x, self.scale, self.ratio)\n",
    "        x = TTF.resized_crop(x, i, j, h, w, self.size, self.interpolation)\n",
    "        y = TTF.resized_crop(y, i, j, h, w, self.size, self.interpolation)\n",
    "        return x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RC():\n",
    "    def __init__(self, targ_sz):\n",
    "        self.targ_sz = targ_sz\n",
    "\n",
    "    def __call__(self, x, y):\n",
    "        rand_w = random.uniform(0, 1)\n",
    "        rand_h = random.uniform(0, 1)\n",
    "        w,h = x.size\n",
    "        t_w,t_h = self.targ_sz\n",
    "        start_x = np.floor(rand_w*(w-t_w)).astype(int)\n",
    "        start_y = np.floor(rand_h*(h-t_h)).astype(int)\n",
    "        return TTF.crop(x, start_y, start_x, t_h, t_w), TTF.crop(y, start_y, start_x, t_h, t_w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_y(y_img):\n",
    "    y_new = np.zeros(y_img.shape, dtype=int)\n",
    "    y_new[y_img==VEHICLES] = 1\n",
    "    cutoff_y = int(y_new.shape[0]*.83)\n",
    "    y_new[cutoff_y:,:] = 0\n",
    "\n",
    "    y_new[y_img==ROADS] = 2\n",
    "    y_new[y_img==ROAD_LINES] = 2\n",
    "    return torch.from_numpy(y_new).long()\n",
    "\n",
    "def xy_tensor(x,y):\n",
    "    y_img = np.array(y, np.int32, copy=False)\n",
    "    return TTF.to_tensor(x), convert_y(y_img[:,:,0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_names = np.sort(np.array(glob(str(PATH/f'CameraRGB'/'*.png'))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def torch_loader(f_ext, data_path, bs, size, workers=7, random_crop=False):\n",
    "    # Data loading code\n",
    "    x_names = np.sort(np.array(glob(str(data_path/f'CameraRGB{f_ext}'/'*.png'))))\n",
    "    y_names = np.sort(np.array(glob(str(data_path/f'CameraSeg{f_ext}'/'*.png'))))\n",
    "    \n",
    "    x_n = x_names.shape[0]\n",
    "    val_idxs = list(range(x_n-300, x_n))\n",
    "#     val_idxs = list(range(300))\n",
    "    ((val_x,trn_x),(val_y,trn_y)) = split_by_idx(val_idxs, x_names, y_names)\n",
    "    normalize = transforms.Normalize(mean=[0.4914 , 0.48216, 0.44653], std=[0.24703, 0.24349, 0.26159])\n",
    "    \n",
    "    train_tfms = [\n",
    "        crop_bg_pil,\n",
    "        tfm_x_wrapper(transforms.ColorJitter(.2,.2,.2)),\n",
    "#         tfm_x_wrapper(Lighting(0.1, __imagenet_pca['eigval'], __imagenet_pca['eigvec'])),\n",
    "        RR(),\n",
    "        RHF(),\n",
    "        xy_tensor,\n",
    "        tfm_x_wrapper(normalize),\n",
    "    ]\n",
    "    if random_crop:\n",
    "        train_tfms.insert(3,RRC(size, scale=(0.4, 1.0)))\n",
    "    train_dataset = MatchedFilesDataset(trn_x, trn_y, train_tfms, path='')\n",
    "    train_loader = torch.utils.data.DataLoader(\n",
    "        train_dataset, batch_size=bs, shuffle=True,\n",
    "        num_workers=workers, pin_memory=True)\n",
    "\n",
    "    val_tfms = [\n",
    "        crop_bg_pil,\n",
    "        xy_tensor,\n",
    "        tfm_x_wrapper(normalize)\n",
    "    ]\n",
    "    val_dataset = MatchedFilesDataset(val_x, val_y, val_tfms, path='')\n",
    "    val_loader = torch.utils.data.DataLoader(\n",
    "        val_dataset, batch_size=bs, shuffle=False,\n",
    "        num_workers=workers, pin_memory=True)\n",
    "\n",
    "    train_loader = DataPrefetcher(train_loader)\n",
    "    val_loader = DataPrefetcher(val_loader)\n",
    "    \n",
    "    data = ModelData(data_path, train_loader, val_loader)\n",
    "    return data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def denorm(x):\n",
    "    x_np = x.cpu().numpy()\n",
    "    x_np = np.rollaxis(x_np, 0, 3)\n",
    "    mean=np.array([0.4914 , 0.48216, 0.44653])\n",
    "    std=np.array([0.24703, 0.24349, 0.26159])\n",
    "    x_np = x_np*std+mean\n",
    "    return x_np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def new_acc(preds, targs):\n",
    "    mx,idx = torch.max(preds, 1)\n",
    "    return (idx == targs).float().mean()\n",
    "def dice_mult(pred, targs):\n",
    "#     pred = (pred>0).float()\n",
    "    mx,idx = torch.max(pred, 1)\n",
    "    pred = idx.float()\n",
    "    targs = targs.float()\n",
    "    return 2. * (pred*targs).sum() / (pred+targs).sum()\n",
    "def dice(pred, targs):\n",
    "    pred = (pred>0).float()\n",
    "    return 2. * (pred*targs).sum() / (pred+targs).sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## U-net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "BN_EPS = 1e-4  #1e-4  #1e-5\n",
    "class ConvBnRelu2d(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, kernel_size=3, padding=1, dilation=1, stride=1, groups=1, is_bn=True, is_relu=True):\n",
    "        super(ConvBnRelu2d, self).__init__()\n",
    "        self.conv = nn.Conv2d(in_channels, out_channels, kernel_size=kernel_size, padding=padding, stride=stride, dilation=dilation, groups=groups, bias=False)\n",
    "        self.bn   = nn.BatchNorm2d(out_channels, eps=BN_EPS)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        if is_bn   is False: self.bn  =None\n",
    "        if is_relu is False: self.relu=None\n",
    "\n",
    "\n",
    "    def forward(self,x):\n",
    "        x = self.conv(x)\n",
    "        if self.bn   is not None: x = self.bn(x)\n",
    "        if self.relu is not None: x = self.relu(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConvResidual (nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, stride=1):\n",
    "        super(ConvResidual, self).__init__()\n",
    "\n",
    "        self.block = nn.Sequential(\n",
    "            ConvBnRelu2d(in_channels,  out_channels, kernel_size=3, padding=1,  stride=1 ),\n",
    "            ConvBnRelu2d(out_channels, out_channels, kernel_size=3, padding=1,  stride=1, is_relu=False),\n",
    "        )\n",
    "        self.shortcut = None\n",
    "        if in_channels!=out_channels or stride!=1:\n",
    "            self.shortcut = nn.Conv2d(in_channels, out_channels, kernel_size=1, padding=0, stride=stride,  bias=True)\n",
    "\n",
    "    def forward(self, x):\n",
    "        r = x if self.shortcut is None else self.shortcut(x)\n",
    "        x = self.block(x)\n",
    "        x = F.relu(x.add_(r), inplace=True)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "## origainl 3x3 stack filters used in UNet\n",
    "class StackEncoder (nn.Module):\n",
    "    def __init__(self, x_channels, y_channels, kernel_size=3, max_pool=True):\n",
    "        super(StackEncoder, self).__init__()\n",
    "        padding=(kernel_size-1)//2\n",
    "        self.encode = nn.Sequential(\n",
    "            ConvBnRelu2d(x_channels, y_channels, kernel_size=kernel_size, padding=padding, dilation=1, stride=1, groups=1),\n",
    "            ConvBnRelu2d(y_channels, y_channels, kernel_size=kernel_size, padding=padding, dilation=1, stride=1, groups=1),\n",
    "        )\n",
    "        self.mp = nn.MaxPool2d(2) if max_pool else nn.Conv2d(y_channels, y_channels, kernel_size=kernel_size, padding=padding, stride=2)\n",
    "\n",
    "    def forward(self,x):\n",
    "        y = self.encode(x)\n",
    "        y_small = self.mp(y)\n",
    "        return y, y_small\n",
    "\n",
    "\n",
    "class StackDecoder (nn.Module):\n",
    "    def __init__(self, x_big_channels, x_channels, y_channels, kernel_size=3):\n",
    "        super(StackDecoder, self).__init__()\n",
    "        padding=(kernel_size-1)//2\n",
    "\n",
    "        self.decode = nn.Sequential(\n",
    "            ConvBnRelu2d(x_big_channels+x_channels, y_channels, kernel_size=kernel_size, padding=padding, dilation=1, stride=1, groups=1),\n",
    "            ConvBnRelu2d(y_channels, y_channels, kernel_size=kernel_size, padding=padding, dilation=1, stride=1, groups=1),\n",
    "            ConvBnRelu2d(y_channels, y_channels, kernel_size=kernel_size, padding=padding, dilation=1, stride=1, groups=1),\n",
    "        )\n",
    "\n",
    "    def forward(self, x_big, x):\n",
    "        N,C,H,W = x_big.size()\n",
    "        y = F.upsample(x, size=(H,W),mode='bilinear')\n",
    "        #y = F.upsample(x, scale_factor=2,mode='bilinear')\n",
    "        y = torch.cat([y,x_big],1)\n",
    "        y = self.decode(y)\n",
    "        return  y\n",
    "##---------------------------------------------------------------\n",
    "\n",
    "\n",
    "## origainl 3x3 stack filters used in UNet\n",
    "class ResStackEncoder (nn.Module):\n",
    "    def __init__(self, x_channels, y_channels):\n",
    "        super(ResStackEncoder, self).__init__()\n",
    "        self.encode = ConvResidual(x_channels, y_channels)\n",
    "\n",
    "    def forward(self,x):\n",
    "        y = self.encode(x)\n",
    "        y_small = F.max_pool2d(y, kernel_size=2, stride=2)\n",
    "        return y, y_small\n",
    "\n",
    "\n",
    "class ResStackDecoder (nn.Module):\n",
    "    def __init__(self, x_big_channels, x_channels, y_channels, kernel_size=3):\n",
    "        super(ResStackDecoder, self).__init__()\n",
    "        padding=(kernel_size-1)//2\n",
    "\n",
    "        self.decode = nn.Sequential(\n",
    "            ConvBnRelu2d(x_big_channels+x_channels, y_channels, kernel_size=kernel_size, padding=padding, dilation=1, stride=1, groups=1),\n",
    "            ConvResidual(y_channels, y_channels)\n",
    "        )\n",
    "\n",
    "    def forward(self, x_big, x):\n",
    "        N,C,H,W = x_big.size()\n",
    "        y = F.upsample(x, size=(H,W),mode='bilinear')\n",
    "        #y = F.upsample(x, scale_factor=2,mode='bilinear')\n",
    "        y = torch.cat([y,x_big],1)\n",
    "        y = self.decode(y)\n",
    "        return  y\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 256x256\n",
    "class Unet256 (nn.Module):\n",
    "    def __init__(self, out_c=3, in_c=3, f=2):\n",
    "        super().__init__()\n",
    "#         C,H,W = in_shape\n",
    "        #assert(C==3)\n",
    "\n",
    "        #256\n",
    "        self.down2 = StackEncoder(in_c,   64//f, kernel_size=3)   #128\n",
    "        self.down3 = StackEncoder( 64//f,  128//f, kernel_size=3)   # 64\n",
    "        self.down4 = StackEncoder(128//f,  256//f, kernel_size=3)   # 32\n",
    "        self.down5 = StackEncoder(256//f,  512//f, kernel_size=3)   # 16\n",
    "        self.down6 = StackEncoder(512//f, 1024//f, kernel_size=3)   #  8\n",
    "\n",
    "        self.center = nn.Sequential(\n",
    "            #ConvBnRelu2d( 512, 1024, kernel_size=3, padding=1, stride=1 ),\n",
    "            ConvBnRelu2d(1024//f, 1024//f, kernel_size=3, padding=1, stride=1 ),\n",
    "        )\n",
    "\n",
    "        # 8\n",
    "        # x_big_channels, x_channels, y_channels\n",
    "        self.up6 = StackDecoder(1024//f,1024//f, 512//f, kernel_size=3)  # 16\n",
    "        self.up5 = StackDecoder( 512//f, 512//f, 256//f, kernel_size=3)  # 32\n",
    "        self.up4 = StackDecoder( 256//f, 256//f, 128//f, kernel_size=3)  # 64\n",
    "        self.up3 = StackDecoder( 128//f, 128//f,  64//f, kernel_size=3)  #128\n",
    "        self.up2 = StackDecoder(  64//f,  64//f,  32, kernel_size=3)  #256\n",
    "        self.classify = nn.Conv2d(32, out_c, kernel_size=1, padding=0, stride=1, bias=True)\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "\n",
    "        out = x                       #;print('x    ',x.size())\n",
    "                                      #\n",
    "        down2,out = self.down2(out)   #;print('down2',down2.size())  #128\n",
    "        down3,out = self.down3(out)   #;print('down3',down3.size())  #64\n",
    "        down4,out = self.down4(out)   #;print('down4',down4.size())  #32\n",
    "        down5,out = self.down5(out)   #;print('down5',down5.size())  #16\n",
    "        down6,out = self.down6(out)   #;print('down6',down6.size())  #8\n",
    "        pass                          #;print('out  ',out.size())\n",
    "\n",
    "        out = self.center(out)\n",
    "        out = self.up6(down6, out)\n",
    "        out = self.up5(down5, out)\n",
    "        out = self.up4(down4, out)\n",
    "        out = self.up3(down3, out)\n",
    "        out = self.up2(down2, out)\n",
    "\n",
    "        out = self.classify(out)\n",
    "        out = torch.squeeze(out, dim=1)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# 128x128\n",
    "class Unet128 (nn.Module):\n",
    "    def __init__(self, in_c=3, out_c=3, f=1):\n",
    "        super().__init__()\n",
    "        #assert(C==3)\n",
    "\n",
    "        #128\n",
    "        self.down3 = StackEncoder(in_c,   128//f, kernel_size=3)   # 64\n",
    "        self.down4 = StackEncoder(128//f,  256//f, kernel_size=3)   # 32\n",
    "        self.down5 = StackEncoder(256//f,  512//f, kernel_size=3)   # 16\n",
    "        self.down6 = StackEncoder(512//f, 1024//f, kernel_size=3)   #  8\n",
    "\n",
    "        self.center = nn.Sequential(\n",
    "            ConvBnRelu2d(1024//f, 1024//f, kernel_size=3, padding=1, stride=1 ),\n",
    "        )\n",
    "\n",
    "        # 8\n",
    "        # x_big_channels, x_channels, y_channels\n",
    "        self.up6 = StackDecoder(1024//f,1024//f, 512//f, kernel_size=3)  # 16\n",
    "        self.up5 = StackDecoder( 512//f, 512//f, 256//f, kernel_size=3)  # 32\n",
    "        self.up4 = StackDecoder( 256//f, 256//f, 128//f, kernel_size=3)  # 64\n",
    "        self.up3 = StackDecoder( 128//f, 128//f,  64//f, kernel_size=3)  #128\n",
    "        self.classify = nn.Conv2d(64//f, out_c, kernel_size=1, padding=0, stride=1, bias=True)\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "\n",
    "        out = x                       #;print('x    ',x.size())\n",
    "        down3,out = self.down3(out)   #;print('down3',down3.size())  #64\n",
    "        down4,out = self.down4(out)   #;print('down4',down4.size())  #32\n",
    "        down5,out = self.down5(out)   #;print('down5',down5.size())  #16\n",
    "        down6,out = self.down6(out)   #;print('down6',down6.size())  #8\n",
    "        pass                          #;print('out  ',out.size())\n",
    "\n",
    "        out = self.center(out)\n",
    "        out = self.up6(down6, out)\n",
    "        out = self.up5(down5, out)\n",
    "        out = self.up4(down4, out)\n",
    "        out = self.up3(down3, out)\n",
    "        out = self.classify(out)\n",
    "        out = torch.squeeze(out, dim=1)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "m = Unet256().cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ext = '-150'\n",
    "sz = 96\n",
    "bs = 96\n",
    "md = torch_loader('-150', PATH, bs, sz, random_crop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x,y = next(iter(md.trn_dl))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "out = m(VV(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## U-net (ish)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_base(f):\n",
    "    cut,lr_cut = model_meta[f]\n",
    "    layers = cut_model(f(True), cut)\n",
    "    return nn.Sequential(*layers), lr_cut"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SaveFeatures():\n",
    "    features=None\n",
    "    def __init__(self, m): self.hook = m.register_forward_hook(self.hook_fn)\n",
    "    def hook_fn(self, module, input, output): self.features = output\n",
    "    def remove(self): self.hook.remove()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "class UnetModel():\n",
    "    def __init__(self,model,name='unet'):\n",
    "        self.model,self.name = model,name\n",
    "\n",
    "    def get_layer_groups(self, precompute):\n",
    "        if isinstance(self.model, FP16):\n",
    "            model = self.model.module\n",
    "        else:\n",
    "            model = self.model\n",
    "        if isinstance(model, Unet128):\n",
    "            return children(model)\n",
    "        lgs = list(split_by_idxs(children(model.rn), [model.lr_cut]))\n",
    "#         print('LGS:', lgs)\n",
    "#         print('Add:', children(model)[1:])\n",
    "        return lgs + [children(model)[1:]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_learner(md, m_fn=Unet256):\n",
    "    m = m_fn(f=2)\n",
    "#     models = UnetModel(m)\n",
    "    learn = Learner.from_model_data(m, md)\n",
    "    learn.opt_fn=optim.Adam\n",
    "    class_weights = torch.FloatTensor([1,10,2]).cuda()\n",
    "    learn.crit=nn.CrossEntropyLoss(weight=class_weights)\n",
    "#     learn.crit=nn.CrossEntropyLoss()\n",
    "#     learn.crit=FocalLoss(2)\n",
    "#     learn.crit = nn.BCEWithLogitsLoss()\n",
    "    learn.unfreeze()\n",
    "    learn.metrics=[new_acc]\n",
    "    return learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# learn.half()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading from train6 model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "ext = '-150'\n",
    "sz = 96\n",
    "bs = 96\n",
    "md = torch_loader('-150', PATH, bs, sz, random_crop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn = get_learner(md)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e2695b0862d942aa85872160cb694c50",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Epoch', max=1), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 99%|█████████▊| 72/73 [00:16<00:00,  4.44it/s, loss=4.79] "
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "learn.lr_find()\n",
    "learn.sched.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr=5e-2\n",
    "# lr=4e-3\n",
    "wd=1e-5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "87c6559100e14931ac5dc69ea0cc3ef3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Epoch', max=40), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch      trn_loss   val_loss   new_acc                   \n",
      "    0      0.404344   0.247974   0.895632  \n",
      "    1      0.268309   0.251193   0.891703                  \n",
      "    2      0.21175    0.147406   0.942676                  \n",
      "    3      0.191938   0.113204   0.974054                  \n",
      "    4      0.175985   0.081943   0.978403                  \n",
      "    5      0.164289   0.128552   0.985535                  \n",
      "    6      0.158646   0.080001   0.985574                  \n",
      "    7      0.163008   0.11943    0.987003                  \n",
      "    8      0.15425    0.090882   0.982102                  \n",
      "    9      0.155836   0.098248   0.97914                   \n",
      "    10     0.148313   0.084658   0.979875                  \n",
      "    11     0.150872   1.040283   0.617129                  \n",
      "    12     0.162918   0.089358   0.986898                  \n",
      "    13     0.14094    0.056331   0.988456                  \n",
      "    14     0.143582   0.12636    0.985061                  \n",
      "    15     0.144303   0.218569   0.903258                  \n",
      "    16     0.138624   0.086148   0.984488                  \n",
      "    17     0.135753   0.15687    0.919254                  \n",
      "    18     0.129882   0.078743   0.98673                   \n",
      "    19     0.123154   0.064353   0.984302                  \n",
      "    20     0.131644   0.057988   0.983235                  \n",
      "    21     0.127374   0.054317   0.990653                  \n",
      "    22     0.124765   0.070842   0.985057                  \n",
      "    23     0.117533   0.042111   0.988722                  \n",
      "    24     0.112757   0.049194   0.987771                  \n",
      "    25     0.113346   0.05811    0.983414                  \n",
      "    26     0.107412   0.039475   0.990965                  \n",
      "    27     0.10744    0.12845    0.955217                  \n",
      "    28     0.110972   0.047271   0.986561                  \n",
      "    29     0.103397   0.035669   0.989127                  \n",
      "    30     0.099499   0.031681   0.99017                    \n",
      "    31     0.094634   0.032908   0.992615                   \n",
      "    32     0.092067   0.030483   0.991567                   \n",
      "    33     0.091262   0.029718   0.991862                   \n",
      "    34     0.089695   0.028623   0.991727                   \n",
      "    35     0.089771   0.026925   0.993287                   \n",
      "    36     0.090053   0.027764   0.992335                   \n",
      "    37     0.088808   0.026532   0.992636                   \n",
      "    38     0.088055   0.026416   0.992834                   \n",
      "    39     0.086729   0.026023   0.992764                   \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[array([0.02602]), 0.9927640843391419]"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "learn.fit(lr,1,wds=wd,cycle_len=40,use_clr_beta=(20,20,.95,.85))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.save(f'128urn-{S_PREFIX}-tmp')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.load(f'128urn-{S_PREFIX}-tmp')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "learn.fit(lr/100,1,wds=wd,cycle_len=10,use_clr_beta=(20,20,.95,.85))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.save(f'128urn-{S_PREFIX}-0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.load(f'128urn-{S_PREFIX}-0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ext = '-150'\n",
    "sz = 96\n",
    "bs = 96\n",
    "md = torch_loader('-150', PATH, bs, sz, random_crop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.fit(lr,1,wds=wd,cycle_len=40,use_clr_beta=(20,20,.95,.85))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x,y = next(iter(md.val_dl))\n",
    "py = to_np(learn.model(V(x)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "py = np.argmax(py,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_img(denorm(x[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_img(py[0]);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_img(y[0]);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 256x256"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "ext = '-300'\n",
    "sz=192\n",
    "bs=32\n",
    "# md = torch_loader(ext, PATH, bs, sz)\n",
    "md = torch_loader(ext, PATH, bs, sz, random_crop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr=1e-2\n",
    "wd=1e-6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8611"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import gc\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn = get_learner(md)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "41459ace0aa141a5a24094c4a90e9e13",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Epoch', max=1), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 79%|███████▊  | 172/219 [00:28<00:07,  6.03it/s, loss=0.499]"
     ]
    }
   ],
   "source": [
    "learn.lr_find()\n",
    "learn.sched.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "learn.load(f'128urn-{S_PREFIX}-0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.fit(lr,1,wds=wd,cycle_len=30,use_clr_beta=(20,20,.95,.85))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.save(f'256urn-{S_PREFIX}-tmp')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ext = '-300'\n",
    "sz=192\n",
    "bs=32\n",
    "# md = torch_loader(ext, PATH, bs, sz)\n",
    "md = torch_loader(ext, PATH, bs, sz, random_crop=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn = get_learner(md)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.load(f'256urn-{S_PREFIX}-tmp')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.fit(lr/100,1,wds=wd,cycle_len=10,use_clr_beta=(20,20,.95,.85))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.save(f'256urn-{S_PREFIX}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.load(f'256urn-{S_PREFIX}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x,y = next(iter(md.trn_dl))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x,y = next(iter(md.val_dl))\n",
    "py = to_np(learn.model(V(x)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "py = np.argmax(py,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(denorm(x[-1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_img(py[-1]);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "py[-1].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y[-1].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_img(y[-1]);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 512x512"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAIN_DN = 'CameraRGB'\n",
    "MASKS_DN = 'CameraSeg'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ext = ''\n",
    "sz=288\n",
    "bs=16\n",
    "md = torch_loader(ext, PATH, bs, sz, random_crop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn = get_learner(md)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.lr_find()\n",
    "learn.sched.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.load(f'256urn-{S_PREFIX}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr=1e-3\n",
    "wd=5e-7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "learn.fit(lr,1,wds=wd,cycle_len=8,use_clr_beta=(20,20,.95,.85))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.save(f'600urn-{S_PREFIX}-tmp')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.load(f'600urn-{S_PREFIX}-tmp')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.fit(lr/10,1,wds=wd,cycle_len=8,use_clr_beta=(20,10,.95,.85))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.fit(lr/100,1,wds=wd,cycle_len=8,use_clr_beta=(20,20,.95,.85))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.sched.plot_loss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.save(f'600urn-{S_PREFIX}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x,y = next(iter(md.val_dl))\n",
    "py = to_np(learn.model(V(x)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "py = np.argmax(py,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(denorm(x[10]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_img(py[10]);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "py[-1].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y[-1].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_img(y[10]);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys, skvideo.io, json, base64\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "from io import BytesIO, StringIO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m_base = get_base()\n",
    "m = to_gpu(Unet34(m_base))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model_summary(m, [3,608,800])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# learn.load('1024urn')\n",
    "load_model(m, str(PATH/f'models/600urn-{S_PREFIX}.h5'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# file = sys.argv[-1]\n",
    "file = 'test_video.mp4'\n",
    "\n",
    "if file == 'demo.py':\n",
    "    print(\"Error loading video\")\n",
    "    quit\n",
    "\n",
    "# Define encoder function\n",
    "def encode(array):\n",
    "    pil_img = Image.fromarray(array)\n",
    "    buff = BytesIO()\n",
    "    pil_img.save(buff, format=\"PNG\")\n",
    "    return base64.b64encode(buff.getvalue()).decode(\"utf-8\")\n",
    "\n",
    "video = skvideo.io.vread(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# resized_video = np.array([scipy.misc.imresize(f, size=(512,512)) for f in video])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize(x):\n",
    "    if np.mean(x) > 1:\n",
    "        x = x/255\n",
    "    m,s = imagenet_stats\n",
    "    x = (x-m)/s\n",
    "    return x\n",
    "def preprocess(video):\n",
    "    f1_norm = normalize(video)\n",
    "    f1_roll = np.rollaxis(f1_norm, 3, 1)\n",
    "    f1_pad = np.pad(f1_roll, [(0,0),(0,0),(0,8),(0,0)], mode='constant')\n",
    "    return f1_pad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f1 = preprocess(video)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = []\n",
    "for i in range(0,f1.shape[0],8):\n",
    "    xv = VV(torch.from_numpy(f1[i:i+8]).contiguous().float())\n",
    "    preds = m(xv)\n",
    "    mx,idx = torch.max(preds, 1)\n",
    "    idx_slice = idx[:,:-8,:]\n",
    "    results.append(idx_slice)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r_stacked = torch.cat(results,0)\n",
    "r_np = r_stacked.data.cpu().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "def plot_res(index):\n",
    "    f, (ax1, ax2) = plt.subplots(1, 2, figsize=(24, 15))\n",
    "    f.tight_layout()\n",
    "    ax1.imshow(video[index])\n",
    "    ax2.imshow(r_np[index])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_res(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "answer_key = {}\n",
    "\n",
    "# Frame numbering starts at 1\n",
    "frame_idx = 1\n",
    "for frame in r_np:\n",
    "    # Look for red cars :)\n",
    "    binary_car_result = (frame==1).astype('uint8')\n",
    "#     print(np.mean(binary_car_result))\n",
    "    \n",
    "    # Look for road :)\n",
    "    binary_road_result = (frame==2).astype('uint8')\n",
    "\n",
    "    answer_key[frame_idx] = [encode(binary_car_result), encode(binary_road_result)]\n",
    "    \n",
    "    # Increment frame\n",
    "    frame_idx+=1\n",
    "\n",
    "# Print output in proper json format\n",
    "tester_data = json.dumps(answer_key)\n",
    "with open('tester_data_multi_take2', 'w') as f:\n",
    "    f.write(tester_data)\n",
    "print(json.dumps(answer_key))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decoding car"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy import misc\n",
    "def decode(packet):\n",
    "\timg = base64.b64decode(packet)\n",
    "\tfilename = PATH/'image.png'\n",
    "\twith open(filename, 'wb') as f:\n",
    "\t\t\tf.write(img)\n",
    "\tresult = misc.imread(filename)\n",
    "\treturn result\n",
    "\n",
    "with open('results.json') as json_data:\n",
    "\tans_data = json.loads(json_data.read())\n",
    "\tjson_data.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_ans(index):\n",
    "    ans = decode(ans_data[str(index)][0])\n",
    "    f, (ax1, ax2, ax3) = plt.subplots(1, 3, figsize=(24, 15))\n",
    "    f.tight_layout()\n",
    "    ax1.imshow(r_np[index])\n",
    "    ax1.set_title('Mine', fontsize=35)\n",
    "    ax2.imshow(ans)\n",
    "    ax2.set_title('Answer', fontsize=35)\n",
    "    ax3.imshow(video[index])\n",
    "    ax2.set_title('Original', fontsize=35)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_ans(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ans = decode(ans_data['1'][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "def plot_res(index):\n",
    "    f, (ax1, ax2) = plt.subplots(1, 2, figsize=(24, 15))\n",
    "    f.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  },
  "toc": {
   "colors": {
    "hover_highlight": "#DAA520",
    "navigate_num": "#000000",
    "navigate_text": "#333333",
    "running_highlight": "#FF0000",
    "selected_highlight": "#FFD700",
    "sidebar_border": "#EEEEEE",
    "wrapper_background": "#FFFFFF"
   },
   "moveMenuLeft": true,
   "nav_menu": {
    "height": "86px",
    "width": "252px"
   },
   "navigate_menu": true,
   "number_sections": true,
   "sideBar": true,
   "threshold": 4,
   "toc_cell": false,
   "toc_section_display": "block",
   "toc_window_display": false,
   "widenNotebook": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
