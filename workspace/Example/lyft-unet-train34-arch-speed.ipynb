{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "%reload_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fastai.conv_learner import *\n",
    "# from fastai.dataset import *\n",
    "from fastai.models.resnet import vgg_resnet50\n",
    "\n",
    "import json\n",
    "from glob import glob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.backends.cudnn.benchmark=True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH = Path('../data/all')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_img(im, figsize=None, ax=None, alpha=None):\n",
    "    if not ax: fig,ax = plt.subplots(figsize=figsize)\n",
    "    ax.imshow(im, alpha=alpha)\n",
    "    ax.set_axis_off()\n",
    "    return ax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "VEHICLES=10\n",
    "ROADS=7\n",
    "ROAD_LINES=6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAIN_DN = 'CameraRGB'\n",
    "MASKS_DN = 'CameraSeg'\n",
    "workers=7\n",
    "random_crop=False\n",
    "pseudo_label=False\n",
    "val_folder = 'test'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "S_PREFIX = '33-test_val'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision.datasets.folder import pil_loader\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.transforms.functional as TTF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MatchedFilesDataset(Dataset):\n",
    "    def __init__(self, fnames, y, tfms, path):\n",
    "        self.path,self.fnames = path,fnames\n",
    "        self.open_fn = pil_loader\n",
    "        self.y=y\n",
    "        self.open_y_fn = pil_loader\n",
    "        assert(len(fnames)==len(y))\n",
    "        \n",
    "        self.n = self.get_n()\n",
    "        self.c = self.get_c()\n",
    "        self.tfms = tfms\n",
    "        \n",
    "    def get_x(self, i): return self.open_fn(os.path.join(self.path, self.fnames[i]))\n",
    "    def get_y(self, i): return self.open_y_fn(os.path.join(self.path, self.y[i]))\n",
    "    def get_n(self): return len(self.fnames)\n",
    "    def get_c(self): return 2\n",
    "    \n",
    "    def get(self, tfms, x, y):\n",
    "        for fn in tfms:\n",
    "            #pdb.set_trace()\n",
    "            x, y = fn(x, y)\n",
    "        return (x, y)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        x,y = self.get_x(idx),self.get_y(idx)\n",
    "        return self.get(self.tfms, x, y)\n",
    "    \n",
    "    def __len__(self): return self.n\n",
    "\n",
    "    def resize_imgs(self, targ, new_path):\n",
    "        dest = resize_imgs(self.fnames, targ, self.path, new_path)\n",
    "        return self.__class__(self.fnames, self.y, self.transform, dest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Seems to speed up training by ~2%\n",
    "class DataPrefetcher():\n",
    "    def __init__(self, loader, stop_after=None):\n",
    "        self.loader = loader\n",
    "        self.dataset = loader.dataset\n",
    "        self.stream = torch.cuda.Stream()\n",
    "        self.stop_after = stop_after\n",
    "        self.next_input = None\n",
    "        self.next_target = None\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.loader)\n",
    "    \n",
    "    def preload(self):\n",
    "        try:\n",
    "            self.next_input, self.next_target = next(self.loaditer)\n",
    "        except StopIteration:\n",
    "            self.next_input = None\n",
    "            self.next_target = None\n",
    "            return\n",
    "        with torch.cuda.stream(self.stream):\n",
    "            self.next_input = self.next_input.cuda(async=True)\n",
    "            self.next_target = self.next_target.cuda(async=True)\n",
    "\n",
    "    def __iter__(self):\n",
    "        count = 0\n",
    "        self.loaditer = iter(self.loader)\n",
    "        self.preload()\n",
    "        while self.next_input is not None:\n",
    "            torch.cuda.current_stream().wait_stream(self.stream)\n",
    "            input = self.next_input\n",
    "            target = self.next_target\n",
    "            self.preload()\n",
    "            count += 1\n",
    "            yield input, target\n",
    "            if type(self.stop_after) is int and (count > self.stop_after):\n",
    "                break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def crop_bg_pil(x,y):\n",
    "    w, h = x.size\n",
    "    top = int(h/3.75)\n",
    "    bot = int(h*.9 + h/150)\n",
    "    pad_right=32-w%32\n",
    "    return TTF.crop(x, top, 0, bot-top, w+pad_right), TTF.crop(y, top, 0, bot-top, w+pad_right)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RHF(object):\n",
    "    def __init__(self, p=0.5): self.p = p\n",
    "    def __call__(self, x, y):\n",
    "        if random.random() < self.p:\n",
    "            return TTF.hflip(x), TTF.hflip(y)\n",
    "        return x,y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RR(object):\n",
    "    def __init__(self, degrees=2): self.degrees = degrees\n",
    "    def __call__(self, x, y):\n",
    "        angle = random.uniform(-self.degrees, self.degrees)\n",
    "        return TTF.rotate(x, angle), TTF.rotate(y, angle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tfm_x_wrapper(tfm):\n",
    "    return lambda x,y: (tfm(x), y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RC():\n",
    "    def __init__(self, targ_sz):\n",
    "        self.targ_sz = targ_sz\n",
    "\n",
    "    def __call__(self, x, y):\n",
    "        rand_w = random.uniform(0, 1)\n",
    "        rand_h = random.uniform(0, 1)\n",
    "        w,h = x.size\n",
    "        t_w,t_h = self.targ_sz\n",
    "        start_x = np.floor(rand_w*(w-t_w)).astype(int)\n",
    "        start_y = np.floor(rand_h*(h-t_h)).astype(int)\n",
    "        return TTF.crop(x, start_y, start_x, t_h, t_w), TTF.crop(y, start_y, start_x, t_h, t_w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_y(y_img):\n",
    "    yr = (y_img==ROADS) | (y_img==ROAD_LINES)\n",
    "    yc = (y_img==VEHICLES)\n",
    "    cutoff_y = int(yc.shape[0]*.87)\n",
    "    yc[cutoff_y:,:] = 0\n",
    "    return torch.from_numpy(np.stack((yc,yr)).astype(int))\n",
    "\n",
    "\n",
    "def xy_tensor(x,y):\n",
    "    y_img = np.array(y, np.int32, copy=False)\n",
    "    return TTF.to_tensor(x), convert_y(y_img[:,:,0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RRC(transforms.RandomResizedCrop):\n",
    "    def __call__(self, x, y):\n",
    "        i, j, h, w = self.get_params(x, self.scale, self.ratio)\n",
    "        x = TTF.resized_crop(x, i, j, h, w, self.size, self.interpolation)\n",
    "        y = TTF.resized_crop(y, i, j, h, w, self.size, self.interpolation)\n",
    "        return x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def torch_loader(f_ext, data_path, bs, size, workers=7, random_crop=False, pseudo_label=False, val_folder=None):\n",
    "    # Data loading code\n",
    "    x_names = np.sort(np.array(glob(str(data_path/f'CameraRGB{f_ext}'/'*.png'))))\n",
    "    y_names = np.sort(np.array(glob(str(data_path/f'CameraSeg{f_ext}'/'*.png'))))\n",
    "\n",
    "    x_n = x_names.shape[0]\n",
    "    val_idxs = list(range(x_n-300, x_n))\n",
    "    \n",
    "    if pseudo_label:\n",
    "        x_names_test = np.sort(np.array(glob(f'../data/pseudo/CameraRGB{f_ext}/*.png')))\n",
    "        y_names_test = np.sort(np.array(glob(f'../data/pseudo/CameraSeg{f_ext}/*.png')))\n",
    "        x_names = np.concatenate((x_names, x_names_test))\n",
    "        x_names = np.concatenate((y_names, y_names_test))\n",
    "        print(f'Pseudo-Labels: {len(x_names_test)}')\n",
    "    if val_folder:\n",
    "        x_names_val = np.sort(np.array(glob(f'../data/{val_folder}/CameraRGB{f_ext}/*.png')))\n",
    "        y_names_val = np.sort(np.array(glob(f'../data/{val_folder}/CameraSeg{f_ext}/*.png')))\n",
    "        val_x,val_y = x_names_val, y_names_val\n",
    "        trn_x,trn_y = x_names, y_names\n",
    "        print(f'Val Labels:', len(val_x))\n",
    "    else:\n",
    "        ((val_x,trn_x),(val_y,trn_y)) = split_by_idx(val_idxs, x_names, y_names)\n",
    "    print(f'Val x:{len(val_x)}, y:{len(val_y)}')\n",
    "    print(f'Trn x:{len(trn_x)}, y:{len(trn_y)}')\n",
    "    print(f'All x:{len(x_names)}')\n",
    "    normalize = transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "    \n",
    "    train_tfms = [\n",
    "        crop_bg_pil,\n",
    "        tfm_x_wrapper(transforms.ColorJitter(.2,.2,.2)),\n",
    "#         tfm_x_wrapper(Lighting(0.1, __imagenet_pca['eigval'], __imagenet_pca['eigvec'])),\n",
    "        RR(),\n",
    "        RHF(),\n",
    "#         RC((size,size)),\n",
    "        xy_tensor,\n",
    "        tfm_x_wrapper(normalize),\n",
    "    ]\n",
    "    if random_crop:\n",
    "        train_tfms.insert(3,RRC(size, scale=(0.4, 1.0)))\n",
    "    train_dataset = MatchedFilesDataset(trn_x, trn_y, train_tfms, path='')\n",
    "    train_loader = torch.utils.data.DataLoader(\n",
    "        train_dataset, batch_size=bs, shuffle=True,\n",
    "        num_workers=workers, pin_memory=True)\n",
    "\n",
    "    val_tfms = [\n",
    "        crop_bg_pil,\n",
    "        xy_tensor,\n",
    "        tfm_x_wrapper(normalize)\n",
    "    ]\n",
    "    val_dataset = MatchedFilesDataset(val_x, val_y, val_tfms, path='')\n",
    "    val_loader = torch.utils.data.DataLoader(\n",
    "        val_dataset, batch_size=bs, shuffle=False,\n",
    "        num_workers=workers, pin_memory=True)\n",
    "\n",
    "    train_loader = DataPrefetcher(train_loader)\n",
    "    val_loader = DataPrefetcher(val_loader)\n",
    "    \n",
    "    data = ModelData(data_path, train_loader, val_loader)\n",
    "    return data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def denorm(x):\n",
    "    x_np = x.cpu().numpy()\n",
    "    x_np = np.rollaxis(x_np, 0, 3)\n",
    "    mean=np.array([0.4914 , 0.48216, 0.44653])\n",
    "    std=np.array([0.24703, 0.24349, 0.26159])\n",
    "    x_np = x_np*std+mean\n",
    "    return x_np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## U-net (ish)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision.models import vgg11_bn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vgg11(pre): return children(vgg11_bn(pre))[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_meta = {\n",
    "    resnet18:[8,6], resnet34:[8,6], resnet50:[8,6], resnet101:[8,6], resnet152:[8,6],\n",
    "    vgg11:[0,13], vgg16:[0,22], vgg19:[0,22],\n",
    "    resnext50:[8,6], resnext101:[8,6], resnext101_64:[8,6],\n",
    "    wrn:[8,6], inceptionresnet_2:[-2,9], inception_4:[-1,9],\n",
    "    dn121:[0,7], dn161:[0,7], dn169:[0,7], dn201:[0,7],\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_base(f):\n",
    "    cut,lr_cut = model_meta[f]\n",
    "    layers = cut_model(f(True), cut)\n",
    "    return nn.Sequential(*layers), lr_cut"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SaveFeatures():\n",
    "    features=None\n",
    "    def __init__(self, m): self.hook = m.register_forward_hook(self.hook_fn)\n",
    "    def hook_fn(self, module, input, output): self.features = output\n",
    "    def remove(self): self.hook.remove()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "BN_EPS = 1e-4  #1e-4  #1e-5\n",
    "class ConvBnRelu2d(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, kernel_size=3, padding=1, dilation=1, stride=1, groups=1, is_bn=True, is_relu=True):\n",
    "        super(ConvBnRelu2d, self).__init__()\n",
    "        self.conv = nn.Conv2d(in_channels, out_channels, kernel_size=kernel_size, padding=padding, stride=stride, dilation=dilation, groups=groups, bias=False)\n",
    "        self.bn   = nn.BatchNorm2d(out_channels, eps=BN_EPS)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        if is_bn   is False: self.bn  =None\n",
    "        if is_relu is False: self.relu=None\n",
    "\n",
    "\n",
    "    def forward(self,x):\n",
    "        x = self.conv(x)\n",
    "        if self.bn   is not None: x = self.bn(x)\n",
    "        if self.relu is not None: x = self.relu(x)\n",
    "        return x\n",
    "    \n",
    "class ConvResidual (nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, stride=1):\n",
    "        super(ConvResidual, self).__init__()\n",
    "\n",
    "        self.block = nn.Sequential(\n",
    "            ConvBnRelu2d(in_channels,  out_channels, kernel_size=3, padding=1,  stride=1 ),\n",
    "            ConvBnRelu2d(out_channels, out_channels, kernel_size=3, padding=1,  stride=1, is_relu=False),\n",
    "        )\n",
    "        self.shortcut = None\n",
    "        if in_channels!=out_channels or stride!=1:\n",
    "            self.shortcut = nn.Conv2d(in_channels, out_channels, kernel_size=1, padding=0, stride=stride,  bias=True)\n",
    "\n",
    "    def forward(self, x):\n",
    "        r = x if self.shortcut is None else self.shortcut(x)\n",
    "        x = self.block(x)\n",
    "        x = F.relu(x.add_(r), inplace=True)\n",
    "        return x\n",
    "    \n",
    "\n",
    "## origainl 3x3 stack filters used in UNet## origa \n",
    "class StackEncoder (nn.Module):\n",
    "    def __init__(self, x_channels, y_channels, kernel_size=3, max_pool=False):\n",
    "        super(StackEncoder, self).__init__()\n",
    "        padding=(kernel_size-1)//2\n",
    "        self.encode = nn.Sequential(\n",
    "            ConvBnRelu2d(x_channels, y_channels, kernel_size=kernel_size, padding=padding, dilation=1, stride=1, groups=1),\n",
    "            ConvBnRelu2d(y_channels, y_channels, kernel_size=kernel_size, padding=padding, dilation=1, stride=1, groups=1),\n",
    "        )\n",
    "        self.mp = nn.MaxPool2d(2) if max_pool else nn.Conv2d(y_channels, y_channels, kernel_size=kernel_size, padding=padding, stride=2)\n",
    "\n",
    "    def forward(self,x):\n",
    "        y = self.encode(x)\n",
    "        y_small = self.mp(y)\n",
    "        return y, y_small\n",
    "\n",
    "\n",
    "class StackDecoder (nn.Module):\n",
    "    def __init__(self, x_big_channels, x_channels, y_channels, kernel_size=3):\n",
    "        super(StackDecoder, self).__init__()\n",
    "        padding=(kernel_size-1)//2\n",
    "\n",
    "        self.decode = nn.Sequential(\n",
    "            ConvBnRelu2d(x_big_channels+x_channels, y_channels, kernel_size=kernel_size, padding=padding, dilation=1, stride=1, groups=1),\n",
    "            ConvBnRelu2d(y_channels, y_channels, kernel_size=kernel_size, padding=padding, dilation=1, stride=1, groups=1),\n",
    "            ConvBnRelu2d(y_channels, y_channels, kernel_size=kernel_size, padding=padding, dilation=1, stride=1, groups=1),\n",
    "        )\n",
    "\n",
    "    def forward(self, x_big, x):\n",
    "        N,C,H,W = x_big.size()\n",
    "        y = F.upsample(x, size=(H,W),mode='bilinear')\n",
    "        #y = F.upsample(x, scale_factor=2,mode='bilinear')\n",
    "        y = torch.cat([y,x_big],1)\n",
    "        y = self.decode(y)\n",
    "        return  y\n",
    "    # 256x256\n",
    "    \n",
    "class Unet256 (nn.Module):\n",
    "    def __init__(self, out_c=2, in_c=3, f=2):\n",
    "        super().__init__()\n",
    "#         C,H,W = in_shape\n",
    "        #assert(C==3)\n",
    "\n",
    "        #256\n",
    "        self.down2 = StackEncoder(in_c,   64//f, kernel_size=3)   #128\n",
    "        self.down3 = StackEncoder( 64//f,  128//f, kernel_size=3)   # 64\n",
    "        self.down4 = StackEncoder(128//f,  256//f, kernel_size=3)   # 32\n",
    "        self.down5 = StackEncoder(256//f,  512//f, kernel_size=3)   # 16\n",
    "        self.down6 = StackEncoder(512//f, 1024//f, kernel_size=3)   #  8\n",
    "\n",
    "        self.center = nn.Sequential(\n",
    "            #ConvBnRelu2d( 512, 1024, kernel_size=3, padding=1, stride=1 ),\n",
    "            ConvBnRelu2d(1024//f, 1024//f, kernel_size=3, padding=1, stride=1 ),\n",
    "        )\n",
    "\n",
    "        # 8\n",
    "        # x_big_channels, x_channels, y_channels\n",
    "        self.up6 = StackDecoder(1024//f,1024//f, 512//f, kernel_size=3)  # 16\n",
    "        self.up5 = StackDecoder( 512//f, 512//f, 256//f, kernel_size=3)  # 32\n",
    "        self.up4 = StackDecoder( 256//f, 256//f, 128//f, kernel_size=3)  # 64\n",
    "        self.up3 = StackDecoder( 128//f, 128//f,  64//f, kernel_size=3)  #128\n",
    "        self.up2 = StackDecoder(  64//f,  64//f,  32, kernel_size=3)  #256\n",
    "        self.classify = nn.Conv2d(32, out_c, kernel_size=1, padding=0, stride=1, bias=True)\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "\n",
    "        out = x                       #;print('x    ',x.size())\n",
    "                                      #\n",
    "        down2,out = self.down2(out)   #;print('down2',down2.size())  #128\n",
    "        down3,out = self.down3(out)   #;print('down3',down3.size())  #64\n",
    "        down4,out = self.down4(out)   #;print('down4',down4.size())  #32\n",
    "        down5,out = self.down5(out)   #;print('down5',down5.size())  #16\n",
    "        down6,out = self.down6(out)   #;print('down6',down6.size())  #8\n",
    "        pass                          #;print('out  ',out.size())\n",
    "\n",
    "        out = self.center(out)\n",
    "        out = self.up6(down6, out)\n",
    "        out = self.up5(down5, out)\n",
    "        out = self.up4(down4, out)\n",
    "        out = self.up3(down3, out)\n",
    "        out = self.up2(down2, out)\n",
    "\n",
    "        out = self.classify(out)\n",
    "        out = torch.squeeze(out, dim=1)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "class UnetBlock(nn.Module):\n",
    "    def __init__(self, up_in, x_in, n_out):\n",
    "        super().__init__()\n",
    "        up_out = x_out = n_out//2\n",
    "        self.x_conv  = nn.Conv2d(x_in,  x_out,  1)\n",
    "        self.tr_conv = nn.ConvTranspose2d(up_in, up_out, 2, stride=2)\n",
    "        self.bn = nn.BatchNorm2d(n_out)\n",
    "        \n",
    "    def forward(self, up_p, x_p):\n",
    "        up_p = self.tr_conv(up_p)\n",
    "        x_p = self.x_conv(x_p)\n",
    "        cat_p = torch.cat([up_p,x_p], dim=1)\n",
    "        return self.bn(F.relu(cat_p, inplace=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Unet34(nn.Module):\n",
    "    def __init__(self, f=resnet34):\n",
    "        super().__init__()\n",
    "        m_base, lr_cut = get_base(f)\n",
    "        self.rn = m_base\n",
    "        self.lr_cut = lr_cut\n",
    "        self.sfs = [SaveFeatures(self.rn[i]) for i in [2,4,5,6]]\n",
    "        self.up1 = UnetBlock(512,256,256)\n",
    "        self.up2 = UnetBlock(256,128,256)\n",
    "        self.up3 = UnetBlock(256,64,256)\n",
    "        self.up4 = UnetBlock(256,64,256)\n",
    "        self.up5 = UnetBlock(256,3,16)\n",
    "        self.up6 = nn.ConvTranspose2d(16, 2, 1)\n",
    "        \n",
    "    def forward(self,x):\n",
    "        inp = x\n",
    "        x = F.relu(self.rn(x), inplace=True)\n",
    "        x = self.up1(x, self.sfs[3].features)\n",
    "        x = self.up2(x, self.sfs[2].features)\n",
    "        x = self.up3(x, self.sfs[1].features)\n",
    "        x = self.up4(x, self.sfs[0].features)\n",
    "        x = self.up5(x, inp)\n",
    "        x = self.up6(x)\n",
    "        return torch.squeeze(x)\n",
    "    \n",
    "    def close(self):\n",
    "        for sf in self.sfs: sf.remove()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Unet34Mod(nn.Module):\n",
    "    def __init__(self, f=resnet34):\n",
    "        super().__init__()\n",
    "        m_base, lr_cut = get_base(f)\n",
    "        self.rn = m_base\n",
    "        self.lr_cut = lr_cut\n",
    "        self.sfs = [SaveFeatures(self.rn[i]) for i in [2,4,5,6]]\n",
    "        self.up1 = UnetBlock(512,256,256)\n",
    "        self.up2 = UnetBlock(256,128,256)\n",
    "        self.up3 = UnetBlock(256,64,256)\n",
    "        self.up4 = UnetBlock(256,64,256)\n",
    "        self.up5 = UnetBlock(256,16,16)\n",
    "        self.up6 = nn.ConvTranspose2d(16, 2, 1)\n",
    "        self.x_skip = nn.Sequential(\n",
    "            nn.Conv2d(3,16,1),\n",
    "            nn.BatchNorm2d(16),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "        \n",
    "    def forward(self,x):\n",
    "        x_skip = self.x_skip(x)\n",
    "        x = F.relu(self.rn(x), inplace=True)\n",
    "        x = self.up1(x, self.sfs[3].features)\n",
    "        x = self.up2(x, self.sfs[2].features)\n",
    "        x = self.up3(x, self.sfs[1].features)\n",
    "        x = self.up4(x, self.sfs[0].features)\n",
    "        x = self.up5(x, x_skip)\n",
    "        x = self.up6(x)\n",
    "        return torch.squeeze(x)\n",
    "    \n",
    "    def close(self):\n",
    "        for sf in self.sfs: sf.remove()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Unet11(nn.Module):\n",
    "    def __init__(self, f=vgg11):\n",
    "        super().__init__()\n",
    "        m_base, lr_cut = get_base(f)\n",
    "        self.rn = m_base\n",
    "        self.lr_cut = lr_cut\n",
    "        self.sfs = [SaveFeatures(self.rn[0][i]) for i in [2,6,13,20,27]]\n",
    "        self.up0 = UnetBlock(512,512,256)\n",
    "        self.up1 = UnetBlock(256,512,256)\n",
    "        self.up2 = UnetBlock(256,256,256)\n",
    "        self.up3 = UnetBlock(256,128,256)\n",
    "        self.up4 = UnetBlock(256,64,256)\n",
    "        self.up5  = nn.Conv2d(256,2,1)\n",
    "        \n",
    "    def forward(self,x):\n",
    "        x = F.relu(self.rn(x))\n",
    "        x = self.up0(x, self.sfs[4].features)\n",
    "        x = self.up1(x, self.sfs[3].features)\n",
    "        x = self.up2(x, self.sfs[2].features)\n",
    "        x = self.up3(x, self.sfs[1].features)\n",
    "        x = self.up4(x, self.sfs[0].features)\n",
    "        x = self.up5(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "class UnetModel():\n",
    "    def __init__(self,model,name='unet'):\n",
    "        self.model,self.name = model,name\n",
    "\n",
    "    def get_layer_groups(self, precompute):\n",
    "        if isinstance(self.model, FP16):\n",
    "            model = self.model.module\n",
    "        else:\n",
    "            model = self.model\n",
    "        lgs = list(split_by_idxs(children(model.rn), [model.lr_cut]))\n",
    "#         print('LGS:', lgs)\n",
    "#         print('Add:', children(model)[1:])\n",
    "        return lgs + [children(model)[1:]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def car_f_p_r(pred, targs):\n",
    "    p2 = F.sigmoid(pred)\n",
    "    return fbeta_score(targs[:,0,:,:], p2[:,0,:,:], beta=2, threshold=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rd_f(pred, targs):\n",
    "    mx,idx = torch.max(pred, 1)\n",
    "    p2 = F.sigmoid(pred)\n",
    "    f,p,r = fbeta_score(targs[:,1,:,:], p2[:,1,:,:], beta=0.5, threshold=0.5)\n",
    "    return f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fbeta_score(y_true, y_pred, beta, threshold=None, eps=1e-9):\n",
    "    beta2 = beta**2\n",
    "\n",
    "    if threshold:\n",
    "        y_pred = torch.ge(y_pred.float(), threshold).float()\n",
    "    else:\n",
    "        y_pred = y_pred.float()\n",
    "    y_true = y_true.float()\n",
    "\n",
    "    true_positive = (y_pred * y_true).sum()\n",
    "    precision = true_positive/(y_pred.sum()+(eps))\n",
    "    recall = true_positive/(y_true.sum()+eps)\n",
    "    \n",
    "    fb = (precision*recall)/(precision*beta2 + recall + eps)*(1+beta2)\n",
    "    \n",
    "    return fb, precision, recall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "def new_acc(pred, targs):\n",
    "    p2 = F.sigmoid(pred)\n",
    "    return ((p2>0.5).long() == targs).float().mean()\n",
    "def dice_mult(pred, targs):\n",
    "#     pred = (pred>0).float()\n",
    "    mx,idx = torch.max(pred, 1)\n",
    "    pred = idx.float()\n",
    "    targs = targs.float()\n",
    "    return 2. * (pred*targs).sum() / (pred+targs).sum()\n",
    "def dice(pred, targs):\n",
    "    pred = (pred>0).float()\n",
    "    return 2. * (pred*targs).sum() / (pred+targs).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dice_coeff_weight(pred, target, weight):\n",
    "    smooth = 1.\n",
    "    num,c,h,w = pred.shape\n",
    "    m1 = pred.view(num, c, -1)  # Flatten\n",
    "    m2 = target.view(num, c, -1)  # Flatten\n",
    "    intersection = (m1 * m2)\n",
    "    w = V(torch.cuda.FloatTensor(weight).view(1,-1,1))\n",
    "    i_w = (w*intersection).sum()\n",
    "    m1_w = (w*m1).sum()\n",
    "    m2_w = (w*m2).sum()\n",
    "    return (2. * i_w + smooth) / (m1_w + m2_w + smooth)\n",
    "\n",
    "def dice_coeff(pred, target):\n",
    "    smooth = 1.\n",
    "    num,c,h,w = pred.shape\n",
    "    m1 = pred.view(num, c, -1)  # Flatten\n",
    "    m2 = target.view(num, c, -1)  # Flatten\n",
    "    intersection = (m1 * m2).sum()\n",
    "    return (2. * intersection + smooth) / (m1.sum() + m2.sum() + smooth)\n",
    "\n",
    "\n",
    "class SoftDiceLoss(nn.Module):\n",
    "    def __init__(self, weight=None, size_average=True):\n",
    "        super(SoftDiceLoss, self).__init__()\n",
    "        self.weight = weight\n",
    "\n",
    "    def forward(self, logits, targets):\n",
    "        probs = F.sigmoid(logits)\n",
    "        num = targets.size(0)  # Number of batches\n",
    "\n",
    "        if self.weight:\n",
    "            score = dice_coeff_weight(probs, targets.float(), self.weight)\n",
    "        else:\n",
    "            score = dice_coeff(probs, targets.float())\n",
    "        score = 1 - score.sum() / num\n",
    "        return score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_learner(md, m_fn=Unet34):\n",
    "    m = to_gpu(m_fn())\n",
    "    models = UnetModel(m)\n",
    "    learn = ConvLearner(md, models)\n",
    "    learn.opt_fn=optim.Adam\n",
    "#     class_weights = torch.FloatTensor([1,3,2]).cuda()\n",
    "#     learn.crit=nn.CrossEntropyLoss(weight=class_weights)\n",
    "    learn.crit=SoftDiceLoss(weight=[1,1/100])\n",
    "    learn.metrics=[new_acc, rd_f, car_f_p_r]\n",
    "    return learn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Unet34"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val x:300, y:300\n",
      "Trn x:6980, y:6980\n",
      "All x:7280\n"
     ]
    }
   ],
   "source": [
    "ext = '-150'\n",
    "sz = 96\n",
    "bs = 128\n",
    "md = torch_loader(ext, PATH, bs, sz, workers, random_crop, pseudo_label)\n",
    "\n",
    "learn = get_learner(md, m_fn=Unet34)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Epoch', max=15), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch      trn_loss   val_loss   new_acc    rd_f       car_f_p_r  \n",
      "    0      0.995757   0.998675   0.649951   0.381529   0.274048   0.076501   0.874585  \n",
      "    1      0.99622    0.996088   0.873326   0.565419   0.252827   0.670706   0.223495  \n",
      "    2      0.995634   0.997873   0.720794   0.403747   0.285335   0.133232   0.430019  \n",
      "    3      0.995147   0.999536   0.712422   0.207848   0.112715   0.029258   0.448258  \n",
      "    4      0.995      0.995697   0.896916   0.669429   0.531416   0.466303   0.577838  \n",
      "    5      0.995155   0.994137   0.777962   0.442009   0.589179   0.700061   0.567799  \n",
      "    6      0.994823   0.995838   0.886459   0.632443   0.648728   0.393139   0.829484  \n",
      "    7      0.99491    0.997384   0.840897   0.109644   0.528947   0.240293   0.820316  \n",
      "    8      0.994867   0.994801   0.827438   0.505419   0.49205    0.612135   0.478694  \n",
      "    9      0.994765   0.999592   0.661632   0.393339   0.090181   0.021898   0.470762  \n",
      "    10     0.99461    0.99674    0.851195   0.000119   0.599804   0.455286   0.687895  \n",
      "    11     0.994448   0.994651   0.855943   0.148377   0.663937   0.720839   0.653244  \n",
      "    12     0.994356   0.992855   0.91551    0.69623    0.71914    0.762567   0.709734  \n",
      "    13     0.994311   0.993258   0.865116   0.566503   0.720388   0.739726   0.721861  \n",
      "    14     0.994242   0.992789   0.927332   0.743571   0.691158   0.845922   0.661873  \n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[array([0.99279]),\n",
       " 0.9273323551813761,\n",
       " 0.7435713576810119,\n",
       " 0.6911577680491728,\n",
       " 0.8459216177691435,\n",
       " 0.6618729190709222]"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lr=1e-1\n",
    "wd=1e-5\n",
    "learn.fit(lr,1,wds=wd,cycle_len=15,use_clr=(5,8))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Unet34-Mod"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val x:300, y:300\n",
      "Trn x:6980, y:6980\n",
      "All x:7280\n"
     ]
    }
   ],
   "source": [
    "ext = '-150'\n",
    "sz = 96\n",
    "bs = 128\n",
    "md = torch_loader(ext, PATH, bs, sz, workers, random_crop, pseudo_label)\n",
    "\n",
    "learn = get_learner(md, m_fn=Unet34Mod)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Epoch', max=15), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch      trn_loss   val_loss   new_acc    rd_f       car_f_p_r  \n",
      "    0      0.995955   0.997948   0.70653    0.411775   0.395894   0.130579   0.901688  \n",
      "    1      0.996291   0.995748   0.820317   0.503778   0.28063    0.45092    0.259412  \n",
      "    2      0.995906   0.997025   0.738731   0.354821   0.366208   0.330307   0.405448  \n",
      "    3      0.995363   0.999387   0.779061   0.597251   0.114889   0.030124   0.433363  \n",
      "    4      0.995096   0.995426   0.884654   0.61745    0.390542   0.58222    0.367978  \n",
      "    5      0.995088   0.998242   0.666772   0.376902   0.362389   0.122178   0.78331   \n",
      "    6      0.995085   0.994677   0.897554   0.649635   0.32618    0.820497   0.284132  \n",
      "    7      0.995203   0.994761   0.863264   0.555144   0.523495   0.661163   0.503039  \n",
      "    8      0.995072   0.995295   0.867916   0.577368   0.317151   0.769341   0.276839  \n",
      "    9      0.994821   0.995292   0.897009   0.649539   0.239248   0.860685   0.203745  \n",
      "    10     0.994579   0.994327   0.891842   0.629439   0.508736   0.839308   0.464929  \n",
      "    11     0.994465   0.993744   0.851288   0.549805   0.565056   0.756651   0.533671  \n",
      "    12     0.994379   0.993134   0.906403   0.665259   0.608022   0.868193   0.566388  \n",
      "    13     0.994406   0.993352   0.895296   0.641835   0.716701   0.774208   0.704586  \n",
      "    14     0.994292   0.996683   0.89094    0.664894   0.503784   0.266661   0.704761  \n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[array([0.99668]),\n",
       " 0.8909398976961772,\n",
       " 0.6648937471712361,\n",
       " 0.5037837062397947,\n",
       " 0.2666611634567854,\n",
       " 0.7047605882565942]"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lr=1e-1\n",
    "wd=1e-5\n",
    "learn.fit(lr,1,wds=wd,cycle_len=15,use_clr=(5,8))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Unet256"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_unet256_learner(md, m_fn):\n",
    "    m = m_fn(f=2)\n",
    "    learn = Learner.from_model_data(m, md)\n",
    "    learn.opt_fn=optim.Adam\n",
    "#     class_weights = torch.FloatTensor([1,3,2]).cuda()\n",
    "#     learn.crit=nn.CrossEntropyLoss(weight=class_weights)\n",
    "    learn.crit=SoftDiceLoss(weight=[1,1/100])\n",
    "    learn.metrics=[new_acc, rd_f, car_f_p_r]\n",
    "    return learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val x:300, y:300\n",
      "Trn x:6980, y:6980\n",
      "All x:7280\n"
     ]
    }
   ],
   "source": [
    "ext = '-150'\n",
    "sz = 96\n",
    "bs = 128\n",
    "md = torch_loader(ext, PATH, bs, sz, workers, random_crop, pseudo_label)\n",
    "\n",
    "learn = get_unet256_learner(md, m_fn=Unet256)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dd3cf73fc4c84c73a5b3d4f013e643c4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Epoch', max=15), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch      trn_loss   val_loss   new_acc    rd_f       car_f_p_r  \n",
      "    0      0.99759    0.999496   0.549292   0.329369   0.064396   0.017711   0.21505   \n",
      "    1      0.997147   0.999464   0.581651   0.329369   0.020531   0.006625   0.06362   \n",
      "    2      0.997041   0.99787    0.633898   0.329369   0.004846   0.27836    0.00389   \n",
      "    3      0.996982   0.999031   0.591616   0.329369   0.111669   0.031427   0.316671  \n",
      "    4      0.996901   0.999599   0.31975    0.329369   0.092622   0.020423   0.898683  \n",
      "    5      0.996826   0.99735    0.633967   0.329369   0.014767   0.336428   0.011918  \n",
      "    6      0.997002   0.997492   0.633838   0.329369   0.0        0.0        0.0       \n",
      "    7      0.99706    0.997468   0.633882   0.329369   0.0        0.0        0.0       \n",
      "    8      0.997068   0.997491   0.633833   0.329419   0.0        0.0        0.0       \n",
      "    9      0.997058   0.997489   0.633851   0.32942    0.0        0.0        0.0       \n",
      "    10     0.997041   0.997534   0.63384    0.329431   0.0        0.0        0.0       \n",
      "    11     0.997025   0.997567   0.633937   0.329443   0.011651   0.214513   0.009423  \n",
      "    12     0.997003   0.997475   0.633882   0.329369   0.0        0.0        0.0       \n",
      "    13     0.997004   0.997484   0.633851   0.32941    0.0        0.0        0.0       \n",
      "    14     0.997036   0.997488   0.63388    0.329369   0.0        0.0        0.0       \n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[array([0.99749]), 0.6338796544075013, 0.329368738706921, 0.0, 0.0, 0.0]"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lr=1e-1\n",
    "wd=1e-5\n",
    "learn.fit(lr,1,wds=wd,cycle_len=15,use_clr=(5,8))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Unet11"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val x:300, y:300\n",
      "Trn x:6980, y:6980\n",
      "All x:7280\n"
     ]
    }
   ],
   "source": [
    "ext = '-150'\n",
    "sz = 96\n",
    "bs = 128\n",
    "md = torch_loader(ext, PATH, bs, sz, workers, random_crop, pseudo_label)\n",
    "\n",
    "learn = get_learner(md, m_fn=Unet11)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Epoch', max=15), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch      trn_loss   val_loss   new_acc    rd_f       car_f_p_r  \n",
      "    0      0.995109   0.999037   0.692892   0.442539   0.212356   0.054974   0.829949  \n",
      "    1      0.995092   0.999455   0.645618   0.494868   0.113549   0.026868   0.674744  \n",
      "    2      0.994787   0.994858   0.926342   0.775052   0.525516   0.540308   0.533218  \n",
      "    3      0.994555   0.994226   0.938998   0.78364    0.671442   0.566461   0.728374  \n",
      "    4      0.994347   0.994266   0.78847    0.457138   0.483492   0.873292   0.436065  \n",
      "    5      0.994245   0.994151   0.935164   0.769386   0.548665   0.612113   0.545308  \n",
      "    6      0.994113   0.993785   0.938096   0.80512    0.452985   0.887821   0.403618  \n",
      "    7      0.994241   0.99337    0.936344   0.856293   0.593633   0.854688   0.552496  \n",
      "    8      0.994091   0.993212   0.965152   0.881042   0.722872   0.634158   0.767674  \n",
      "    9      0.994166   0.993583   0.936696   0.796099   0.628609   0.696393   0.621932  \n",
      "    10     0.994051   0.993655   0.957059   0.836327   0.675156   0.557402   0.729191  \n",
      "    11     0.993969   0.992092   0.976914   0.947259   0.744858   0.800201   0.733003  \n",
      "    12     0.993812   0.992544   0.914047   0.680528   0.764117   0.776688   0.764388  \n",
      "    13     0.9938     0.992858   0.976187   0.923324   0.77184    0.669268   0.823492  \n",
      "    14     0.993703   0.992143   0.979252   0.92758    0.657154   0.882579   0.617941  \n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[array([0.99214]),\n",
       " 0.9792524631818136,\n",
       " 0.9275796857288896,\n",
       " 0.6571543484412488,\n",
       " 0.8825794208929445,\n",
       " 0.6179412810897752]"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lr=1e-1\n",
    "wd=1e-5\n",
    "learn.fit(lr,1,wds=wd,cycle_len=15,use_clr=(5,8))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Unet11-Mini"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = vgg11(pre=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Sequential(\n",
       "  (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True)\n",
       "  (2): ReLU(inplace)\n",
       "  (3): MaxPool2d(kernel_size=(2, 2), stride=(2, 2), dilation=(1, 1), ceil_mode=False)\n",
       "  (4): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (5): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True)\n",
       "  (6): ReLU(inplace)\n",
       "  (7): MaxPool2d(kernel_size=(2, 2), stride=(2, 2), dilation=(1, 1), ceil_mode=False)\n",
       "  (8): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (9): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True)\n",
       "  (10): ReLU(inplace)\n",
       "  (11): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (12): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True)\n",
       "  (13): ReLU(inplace)\n",
       "  (14): MaxPool2d(kernel_size=(2, 2), stride=(2, 2), dilation=(1, 1), ceil_mode=False)\n",
       "  (15): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (16): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True)\n",
       "  (17): ReLU(inplace)\n",
       "  (18): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (19): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True)\n",
       "  (20): ReLU(inplace)\n",
       "  (21): MaxPool2d(kernel_size=(2, 2), stride=(2, 2), dilation=(1, 1), ceil_mode=False)\n",
       "  (22): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (23): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True)\n",
       "  (24): ReLU(inplace)\n",
       "  (25): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (26): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True)\n",
       "  (27): ReLU(inplace)\n",
       "  (28): MaxPool2d(kernel_size=(2, 2), stride=(2, 2), dilation=(1, 1), ceil_mode=False)\n",
       ")"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Unet11Mini(nn.Module):\n",
    "    def __init__(self, f=vgg11):\n",
    "        super().__init__()\n",
    "        m_base, lr_cut = get_base(f)\n",
    "        self.rn = m_base\n",
    "        self.lr_cut = lr_cut\n",
    "        self.sfs = [SaveFeatures(self.rn[0][i]) for i in [2,6,13,20,27]]\n",
    "        self.up0 = UnetBlock(512,512,256)\n",
    "        self.up1 = UnetBlock(256,512,256)\n",
    "        self.up2 = UnetBlock(256,256,256)\n",
    "        self.up3 = UnetBlock(256,128,128)\n",
    "        self.up4 = UnetBlock(128,64,64)\n",
    "        self.up5  = nn.Conv2d(64,2,1)\n",
    "        \n",
    "    def forward(self,x):\n",
    "        x = F.relu(self.rn(x))\n",
    "        x = self.up0(x, self.sfs[4].features)\n",
    "        x = self.up1(x, self.sfs[3].features)\n",
    "        x = self.up2(x, self.sfs[2].features)\n",
    "        x = self.up3(x, self.sfs[1].features)\n",
    "        x = self.up4(x, self.sfs[0].features)\n",
    "        x = self.up5(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val x:300, y:300\n",
      "Trn x:6980, y:6980\n",
      "All x:7280\n"
     ]
    }
   ],
   "source": [
    "ext = '-150'\n",
    "sz = 96\n",
    "bs = 128\n",
    "md = torch_loader(ext, PATH, bs, sz, workers, random_crop, pseudo_label)\n",
    "\n",
    "learn = get_learner(md, m_fn=Unet11Mini)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "84d27189191146e0bf0b526e7cef56ac",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Epoch', max=15), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch      trn_loss   val_loss   new_acc    rd_f       car_f_p_r  \n",
      "    0      0.995271   0.999623   0.578141   0.620581   0.099298   0.021895   0.979557  \n",
      "    1      0.994763   0.999042   0.766232   0.599266   0.207313   0.052537   0.885654  \n",
      "    2      0.994749   0.993679   0.87984    0.602259   0.560635   0.749528   0.528512  \n",
      "    3      0.994441   0.996227   0.928851   0.79984    0.567761   0.287903   0.808903  \n",
      "    4      0.994496   0.993823   0.935322   0.787189   0.489986   0.800444   0.448098  \n",
      "    5      0.994293   0.993857   0.906809   0.668762   0.479968   0.886636   0.432401  \n",
      "    6      0.994309   0.993882   0.911666   0.687226   0.702128   0.604251   0.754536  \n",
      "    7      0.994552   0.997677   0.903929   0.721601   0.290022   0.13701    0.429104  \n",
      "    8      0.994537   0.9944     0.927072   0.735713   0.323486   0.884108   0.279818  \n",
      "    9      0.994392   0.993301   0.937605   0.771508   0.577911   0.779382   0.543697  \n",
      "    10     0.994221   0.993344   0.937727   0.771228   0.526049   0.828909   0.48234   \n",
      "    11     0.994132   0.992712   0.937741   0.767922   0.693977   0.803979   0.672162  \n",
      "    12     0.994159   0.993043   0.943896   0.799058   0.628742   0.796284   0.598706  \n",
      "    13     0.994046   0.992726   0.934728   0.757539   0.645422   0.869113   0.607029  \n",
      "    14     0.994107   0.99295    0.939093   0.7748     0.747918   0.703415   0.770298  \n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[array([0.99295]),\n",
       " 0.9390928109486898,\n",
       " 0.7748000189510555,\n",
       " 0.74791829254722,\n",
       " 0.703414797815009,\n",
       " 0.7702982393883164]"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lr=1e-1\n",
    "wd=1e-5\n",
    "learn.fit(lr,1,wds=wd,cycle_len=15,use_clr=(5,8))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Unet34-ModWide"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Unet34ModWide(nn.Module):\n",
    "    def __init__(self, f=resnet34):\n",
    "        super().__init__()\n",
    "        m_base, lr_cut = get_base(f)\n",
    "        self.rn = m_base\n",
    "        self.lr_cut = lr_cut\n",
    "        self.sfs = [SaveFeatures(self.rn[i]) for i in [2,4,5,6]]\n",
    "        self.up1 = UnetBlock(512,256,256)\n",
    "        self.up2 = UnetBlock(256,128,256)\n",
    "        self.up3 = UnetBlock(256,64,256)\n",
    "        self.up4 = UnetBlock(256,64,256)\n",
    "        self.up5 = UnetBlock(256,32,32)\n",
    "        self.up6 = nn.ConvTranspose2d(32, 2, 1)\n",
    "        self.x_skip = nn.Sequential(\n",
    "            nn.Conv2d(3,32,1),\n",
    "            nn.BatchNorm2d(32),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "        \n",
    "    def forward(self,x):\n",
    "        x_skip = self.x_skip(x)\n",
    "        x = F.relu(self.rn(x), inplace=True)\n",
    "        x = self.up1(x, self.sfs[3].features)\n",
    "        x = self.up2(x, self.sfs[2].features)\n",
    "        x = self.up3(x, self.sfs[1].features)\n",
    "        x = self.up4(x, self.sfs[0].features)\n",
    "        x = self.up5(x, x_skip)\n",
    "        x = self.up6(x)\n",
    "        return torch.squeeze(x)\n",
    "    \n",
    "    def close(self):\n",
    "        for sf in self.sfs: sf.remove()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val x:300, y:300\n",
      "Trn x:6980, y:6980\n",
      "All x:7280\n"
     ]
    }
   ],
   "source": [
    "ext = '-150'\n",
    "sz = 96\n",
    "bs = 128\n",
    "md = torch_loader(ext, PATH, bs, sz, workers, random_crop, pseudo_label)\n",
    "\n",
    "learn = get_learner(md, m_fn=Unet34ModWide)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "de06bcd110794074a046344e0cc1a175",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Epoch', max=15), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch      trn_loss   val_loss   new_acc    rd_f       car_f_p_r  \n",
      "    0      0.995684   0.999085   0.732984   0.51987    0.214276   0.05399    0.938878  \n",
      "    1      0.996277   0.998894   0.837445   0.565755   0.063257   0.031844   0.087394  \n",
      "    2      0.996596   0.996335   0.791452   0.465972   0.129097   0.491905   0.110321  \n",
      "    3      0.995704   0.997055   0.863653   0.60141    0.477726   0.219044   0.740917  \n",
      "    4      0.995041   0.997293   0.867035   0.62005    0.43503    0.187021   0.713084  \n",
      "    5      0.994923   0.993695   0.846692   0.542982   0.622922   0.714568   0.604308  \n",
      "    6      0.994846   0.994169   0.894069   0.638061   0.569111   0.612374   0.565585  \n",
      "    7      0.994586   0.994805   0.88764    0.612637   0.665314   0.593014   0.703626  \n",
      "    8      0.994687   0.994993   0.884644   0.623509   0.647842   0.438031   0.773144  \n",
      "    9      0.994793   0.99424    0.875201   0.592203   0.498725   0.77367    0.458743  \n",
      "    10     0.994775   0.994664   0.890124   0.634241   0.631164   0.489859   0.714497  \n",
      "    11     0.994809   0.994476   0.870013   0.58332    0.434485   0.777451   0.391609  \n",
      "    12     0.994567   0.993729   0.875221   0.596461   0.633122   0.803918   0.604439  \n",
      "    13     0.99442    0.993467   0.897902   0.645602   0.520509   0.89321    0.47194   \n",
      "    14     0.994355   0.994004   0.873441   0.596521   0.720375   0.544942   0.80904   \n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[array([0.994]),\n",
       " 0.8734409904479981,\n",
       " 0.5965213615032877,\n",
       " 0.7203751306039818,\n",
       " 0.5449415823947977,\n",
       " 0.809040420483691]"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lr=1e-1\n",
    "wd=1e-5\n",
    "learn.fit(lr,1,wds=wd,cycle_len=15,use_clr=(5,8))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  },
  "toc": {
   "colors": {
    "hover_highlight": "#DAA520",
    "navigate_num": "#000000",
    "navigate_text": "#333333",
    "running_highlight": "#FF0000",
    "selected_highlight": "#FFD700",
    "sidebar_border": "#EEEEEE",
    "wrapper_background": "#FFFFFF"
   },
   "moveMenuLeft": true,
   "nav_menu": {
    "height": "86px",
    "width": "252px"
   },
   "navigate_menu": true,
   "number_sections": true,
   "sideBar": true,
   "threshold": 4,
   "toc_cell": false,
   "toc_section_display": "block",
   "toc_window_display": false,
   "widenNotebook": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
