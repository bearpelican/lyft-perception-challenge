{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "%reload_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fastai.conv_learner import *\n",
    "# from fastai.dataset import *\n",
    "from fastai.models.resnet import vgg_resnet50\n",
    "\n",
    "import json\n",
    "from glob import glob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.backends.cudnn.benchmark=True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH = Path('../data/all')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_img(im, figsize=None, ax=None, alpha=None):\n",
    "    if not ax: fig,ax = plt.subplots(figsize=figsize)\n",
    "    ax.imshow(im, alpha=alpha)\n",
    "    ax.set_axis_off()\n",
    "    return ax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "VEHICLES=10\n",
    "ROADS=7\n",
    "ROAD_LINES=6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAIN_DN = 'CameraRGB'\n",
    "MASKS_DN = 'CameraSeg'\n",
    "workers=7\n",
    "random_crop=True\n",
    "pseudo_label=False\n",
    "val_folder = 'sample_test_sync'\n",
    "# val_folder = 'val'\n",
    "S_PREFIX = '45_19ft'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision.datasets.folder import pil_loader\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.transforms.functional as TTF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MatchedFilesDataset(Dataset):\n",
    "    def __init__(self, fnames, y, tfms, path):\n",
    "        self.path,self.fnames = path,fnames\n",
    "        self.open_fn = pil_loader\n",
    "        self.y=y\n",
    "        self.open_y_fn = pil_loader\n",
    "        assert(len(fnames)==len(y))\n",
    "        \n",
    "        self.n = self.get_n()\n",
    "        self.c = self.get_c()\n",
    "        self.tfms = tfms\n",
    "        \n",
    "    def get_x(self, i): return self.open_fn(os.path.join(self.path, self.fnames[i]))\n",
    "    def get_y(self, i): return self.open_y_fn(os.path.join(self.path, self.y[i]))\n",
    "    def get_n(self): return len(self.fnames)\n",
    "    def get_c(self): return 2\n",
    "    \n",
    "    def get(self, tfms, x, y):\n",
    "        for fn in tfms:\n",
    "            #pdb.set_trace()\n",
    "            x, y = fn(x, y)\n",
    "        return (x, y)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        x,y = self.get_x(idx),self.get_y(idx)\n",
    "        return self.get(self.tfms, x, y)\n",
    "    \n",
    "    def __len__(self): return self.n\n",
    "\n",
    "    def resize_imgs(self, targ, new_path):\n",
    "        dest = resize_imgs(self.fnames, targ, self.path, new_path)\n",
    "        return self.__class__(self.fnames, self.y, self.transform, dest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Seems to speed up training by ~2%\n",
    "class DataPrefetcher():\n",
    "    def __init__(self, loader, stop_after=None):\n",
    "        self.loader = loader\n",
    "        self.dataset = loader.dataset\n",
    "        self.stream = torch.cuda.Stream()\n",
    "        self.stop_after = stop_after\n",
    "        self.next_input = None\n",
    "        self.next_target = None\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.loader)\n",
    "    \n",
    "    def preload(self):\n",
    "        try:\n",
    "            self.next_input, self.next_target = next(self.loaditer)\n",
    "        except StopIteration:\n",
    "            self.next_input = None\n",
    "            self.next_target = None\n",
    "            return\n",
    "        with torch.cuda.stream(self.stream):\n",
    "            self.next_input = self.next_input.cuda(async=True)\n",
    "            self.next_target = self.next_target.cuda(async=True)\n",
    "\n",
    "    def __iter__(self):\n",
    "        count = 0\n",
    "        self.loaditer = iter(self.loader)\n",
    "        self.preload()\n",
    "        while self.next_input is not None:\n",
    "            torch.cuda.current_stream().wait_stream(self.stream)\n",
    "            input = self.next_input\n",
    "            target = self.next_target\n",
    "            self.preload()\n",
    "            count += 1\n",
    "            yield input, target\n",
    "            if type(self.stop_after) is int and (count > self.stop_after):\n",
    "                break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def crop_bg_pil(x,y):\n",
    "    w, h = x.size\n",
    "    top = int(h/3.75)\n",
    "    bot = int(h*.9 + h/150)\n",
    "    pad_right=32-w%32\n",
    "    if pad_right == 32: pad_right = 0\n",
    "    return TTF.crop(x, top, 0, bot-top, w+pad_right), TTF.crop(y, top, 0, bot-top, w+pad_right)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RHF(object):\n",
    "    def __init__(self, p=0.5): self.p = p\n",
    "    def __call__(self, x, y):\n",
    "        if random.random() < self.p:\n",
    "            return TTF.hflip(x), TTF.hflip(y)\n",
    "        return x,y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RR(object):\n",
    "    def __init__(self, degrees=2): self.degrees = degrees\n",
    "    def __call__(self, x, y):\n",
    "        angle = random.uniform(-self.degrees, self.degrees)\n",
    "        return TTF.rotate(x, angle), TTF.rotate(y, angle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tfm_x_wrapper(tfm):\n",
    "    return lambda x,y: (tfm(x), y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RC():\n",
    "    def __init__(self, targ_sz):\n",
    "        self.targ_sz = targ_sz\n",
    "\n",
    "    def __call__(self, x, y):\n",
    "        rand_w = random.uniform(0, 1)\n",
    "        rand_h = random.uniform(0, 1)\n",
    "        w,h = x.size\n",
    "        t_w,t_h = self.targ_sz\n",
    "        start_x = np.floor(rand_w*(w-t_w)).astype(int)\n",
    "        start_y = np.floor(rand_h*(h-t_h)).astype(int)\n",
    "        return TTF.crop(x, start_y, start_x, t_h, t_w), TTF.crop(y, start_y, start_x, t_h, t_w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_y_ce(y_img):\n",
    "    y_new = np.zeros(y_img.shape, dtype=int)\n",
    "    y_new[y_img==VEHICLES] = 1\n",
    "    cutoff_y = int(y_new.shape[0]*.875)\n",
    "    y_new[cutoff_y:,:] = 0\n",
    "\n",
    "    y_new[y_img==ROADS] = 2\n",
    "    y_new[y_img==ROAD_LINES] = 2\n",
    "    return torch.from_numpy(y_new).long()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_y(y_img):\n",
    "    yr = (y_img==ROADS) | (y_img==ROAD_LINES)\n",
    "    yc = (y_img==VEHICLES)\n",
    "    cutoff_y = int(yc.shape[0]*.875)\n",
    "    yc[cutoff_y:,:] = 0\n",
    "    rn = ~(yr | yc)\n",
    "    return torch.from_numpy(np.stack((rn,yc,yr)).astype(int))\n",
    "\n",
    "\n",
    "def xy_tensor(x,y):\n",
    "    y_img = np.array(y, np.int32, copy=False)\n",
    "    return TTF.to_tensor(x), convert_y_ce(y_img[:,:,0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RRC(transforms.RandomResizedCrop):\n",
    "    def __call__(self, x, y):\n",
    "        i, j, h, w = self.get_params(x, self.scale, self.ratio)\n",
    "        x = TTF.resized_crop(x, i, j, h, w, self.size, self.interpolation)\n",
    "        y = TTF.resized_crop(y, i, j, h, w, self.size, self.interpolation)\n",
    "        return x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def torch_loader(f_ext, data_path, bs, size, workers=7, random_crop=False, pseudo_label=False, val_folder=None, val_bs=None):\n",
    "    # Data loading code\n",
    "    x_names = np.sort(np.array(glob(str(data_path/f'CameraRGB{f_ext}'/'*.png'))))\n",
    "    y_names = np.sort(np.array(glob(str(data_path/f'CameraSeg{f_ext}'/'*.png'))))\n",
    "\n",
    "    x_n = x_names.shape[0]\n",
    "    val_idxs = list(range(x_n-300, x_n))\n",
    "    \n",
    "    if pseudo_label:\n",
    "        x_names_test = np.sort(np.array(glob(f'../data/pseudo/CameraRGB{f_ext}/*.png')))\n",
    "        y_names_test = np.sort(np.array(glob(f'../data/pseudo/CameraSeg{f_ext}/*.png')))\n",
    "        x_names = np.concatenate((x_names, x_names_test))\n",
    "        x_names = np.concatenate((y_names, y_names_test))\n",
    "        print(f'Pseudo-Labels: {len(x_names_test)}')\n",
    "    if val_folder:\n",
    "        x_names_val = np.sort(np.array(glob(f'../data/{val_folder}/CameraRGB{f_ext}/*.png')))\n",
    "        y_names_val = np.sort(np.array(glob(f'../data/{val_folder}/CameraSeg{f_ext}/*.png')))\n",
    "        val_x,val_y = x_names_val, y_names_val\n",
    "        trn_x,trn_y = x_names, y_names\n",
    "        print(f'Val Labels:', len(val_x))\n",
    "    else:\n",
    "        ((val_x,trn_x),(val_y,trn_y)) = split_by_idx(val_idxs, x_names, y_names)\n",
    "    print(f'Val x:{len(val_x)}, y:{len(val_y)}')\n",
    "    print(f'Trn x:{len(trn_x)}, y:{len(trn_y)}')\n",
    "    print(f'All x:{len(x_names)}')\n",
    "    normalize = transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "    \n",
    "    train_tfms = [\n",
    "        crop_bg_pil,\n",
    "        tfm_x_wrapper(transforms.ColorJitter(.2,.2,.2)),\n",
    "#         tfm_x_wrapper(Lighting(0.1, __imagenet_pca['eigval'], __imagenet_pca['eigvec'])),\n",
    "        RR(),\n",
    "        RHF(),\n",
    "#         RC((size,size)),\n",
    "        xy_tensor,\n",
    "        tfm_x_wrapper(normalize),\n",
    "    ]\n",
    "    if random_crop:\n",
    "        train_tfms.insert(3,RRC(size, scale=(0.4, 1.0)))\n",
    "    train_dataset = MatchedFilesDataset(trn_x, trn_y, train_tfms, path='')\n",
    "    train_loader = torch.utils.data.DataLoader(\n",
    "        train_dataset, batch_size=bs, shuffle=True,\n",
    "        num_workers=workers, pin_memory=True)\n",
    "\n",
    "    val_tfms = [\n",
    "        crop_bg_pil,\n",
    "        xy_tensor,\n",
    "        tfm_x_wrapper(normalize)\n",
    "    ]\n",
    "    val_dataset = MatchedFilesDataset(val_x, val_y, val_tfms, path='')\n",
    "    if val_bs is None: val_bs = bs\n",
    "    val_loader = torch.utils.data.DataLoader(\n",
    "        val_dataset, batch_size=val_bs, shuffle=False,\n",
    "        num_workers=workers, pin_memory=True)\n",
    "\n",
    "    train_loader = DataPrefetcher(train_loader)\n",
    "    val_loader = DataPrefetcher(val_loader)\n",
    "    \n",
    "    data = ModelData(data_path, train_loader, val_loader)\n",
    "    return data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def denorm(x):\n",
    "    x_np = x.cpu().numpy()\n",
    "    x_np = np.rollaxis(x_np, 0, 3)\n",
    "    mean=np.array([0.485, 0.456, 0.406])\n",
    "    std=np.array([0.229, 0.224, 0.225])\n",
    "    x_np = x_np*std+mean\n",
    "    return x_np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## U-net (ish)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision.models import vgg11_bn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vgg11(pre): return children(vgg11_bn(pre))[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_meta = {\n",
    "    resnet18:[8,6], resnet34:[8,6], resnet50:[8,6], resnet101:[8,6], resnet152:[8,6],\n",
    "    vgg11:[0,13], vgg16:[0,22], vgg19:[0,22],\n",
    "    resnext50:[8,6], resnext101:[8,6], resnext101_64:[8,6],\n",
    "    wrn:[8,6], inceptionresnet_2:[-2,9], inception_4:[-1,9],\n",
    "    dn121:[0,7], dn161:[0,7], dn169:[0,7], dn201:[0,7],\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_base(f):\n",
    "    cut,lr_cut = model_meta[f]\n",
    "    layers = cut_model(f(True), cut)\n",
    "    return nn.Sequential(*layers), lr_cut"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SaveFeatures():\n",
    "    features=None\n",
    "    def __init__(self, m): self.hook = m.register_forward_hook(self.hook_fn)\n",
    "    def hook_fn(self, module, input, output): self.features = output\n",
    "    def remove(self): self.hook.remove()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "class UnetBlock(nn.Module):\n",
    "    def __init__(self, up_in, x_in, n_out):\n",
    "        super().__init__()\n",
    "        up_out = x_out = n_out//2\n",
    "        self.x_conv  = nn.Conv2d(x_in,  x_out,  1)\n",
    "        self.tr_conv = nn.ConvTranspose2d(up_in, up_out, 2, stride=2)\n",
    "        self.bn = nn.BatchNorm2d(n_out)\n",
    "        \n",
    "    def forward(self, up_p, x_p):\n",
    "        up_p = self.tr_conv(up_p)\n",
    "        x_p = self.x_conv(x_p)\n",
    "        cat_p = torch.cat([up_p,x_p], dim=1)\n",
    "        return self.bn(F.relu(cat_p, inplace=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Unet34(nn.Module):\n",
    "    def __init__(self, out=3, f=resnet34):\n",
    "        super().__init__()\n",
    "        m_base, lr_cut = get_base(f)\n",
    "        self.rn = m_base\n",
    "        self.lr_cut = lr_cut\n",
    "        self.sfs = [SaveFeatures(self.rn[i]) for i in [2,4,5,6]]\n",
    "        self.up1 = UnetBlock(512,256,256)\n",
    "        self.up2 = UnetBlock(256,128,256)\n",
    "        self.up3 = UnetBlock(256,64,256)\n",
    "        self.up4 = UnetBlock(256,64,256)\n",
    "        self.up5 = UnetBlock(256,out,16)\n",
    "        self.up6 = nn.ConvTranspose2d(16, out, 1)\n",
    "        \n",
    "    def forward(self,x):\n",
    "        inp = x\n",
    "        x = self.rn(x)\n",
    "        x = self.up1(x, self.sfs[3].features)\n",
    "        x = self.up2(x, self.sfs[2].features)\n",
    "        x = self.up3(x, self.sfs[1].features)\n",
    "        x = self.up4(x, self.sfs[0].features)\n",
    "        x = self.up5(x, inp)\n",
    "        x = self.up6(x)\n",
    "        return torch.squeeze(x)\n",
    "    \n",
    "    def close(self):\n",
    "        for sf in self.sfs: sf.remove()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Unet34Mod(nn.Module):\n",
    "    def __init__(self, out=3, f=resnet34):\n",
    "        super().__init__()\n",
    "        m_base, lr_cut = get_base(f)\n",
    "        self.rn = m_base\n",
    "        self.lr_cut = lr_cut\n",
    "        self.sfs = [SaveFeatures(self.rn[i]) for i in [2,4,5,6]]\n",
    "        self.up1 = UnetBlock(512,256,256)\n",
    "        self.up2 = UnetBlock(256,128,256)\n",
    "        self.up3 = UnetBlock(256,64,256)\n",
    "        self.up4 = UnetBlock(256,64,256)\n",
    "        self.up5 = UnetBlock(256,16,16)\n",
    "        self.up6 = nn.ConvTranspose2d(16, out, 1)\n",
    "        self.x_skip = nn.Sequential(\n",
    "            nn.Conv2d(out,16,1),\n",
    "            nn.BatchNorm2d(16),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "        \n",
    "    def forward(self,x):\n",
    "        x_skip = self.x_skip(x)\n",
    "        x = self.rn(x)\n",
    "        x = self.up1(x, self.sfs[3].features)\n",
    "        x = self.up2(x, self.sfs[2].features)\n",
    "        x = self.up3(x, self.sfs[1].features)\n",
    "        x = self.up4(x, self.sfs[0].features)\n",
    "        x = self.up5(x, x_skip)\n",
    "        x = self.up6(x)\n",
    "        return torch.squeeze(x)\n",
    "    \n",
    "    def close(self):\n",
    "        for sf in self.sfs: sf.remove()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Unet34Mod2(nn.Module):\n",
    "    def __init__(self, out=3, f=resnet34):\n",
    "        super().__init__()\n",
    "        m_base, lr_cut = get_base(f)\n",
    "        self.rn = m_base\n",
    "        self.lr_cut = lr_cut\n",
    "        self.sfs = [SaveFeatures(self.rn[i]) for i in [2,4,5,6]]\n",
    "        self.up1 = UnetBlock(512,256,256)\n",
    "        self.up2 = UnetBlock(256,128,256)\n",
    "        self.up3 = UnetBlock(256,64,128)\n",
    "        self.up4 = UnetBlock(128,64,64)\n",
    "        self.up5 = UnetBlock(64,32,32)\n",
    "        self.up6 = nn.ConvTranspose2d(32, out, 1)\n",
    "        self.x_skip = nn.Sequential(\n",
    "            nn.Conv2d(out,32,1),\n",
    "            nn.BatchNorm2d(32),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "        \n",
    "    def forward(self,x):\n",
    "        x_skip = self.x_skip(x)\n",
    "        x = self.rn(x)\n",
    "        x = self.up1(x, self.sfs[3].features)\n",
    "        x = self.up2(x, self.sfs[2].features)\n",
    "        x = self.up3(x, self.sfs[1].features)\n",
    "        x = self.up4(x, self.sfs[0].features)\n",
    "        x = self.up5(x, x_skip)\n",
    "        x = self.up6(x)\n",
    "        return torch.squeeze(x)\n",
    "    \n",
    "    def close(self):\n",
    "        for sf in self.sfs: sf.remove()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "class UnetModel():\n",
    "    def __init__(self,model,name='unet'):\n",
    "        self.model,self.name = model,name\n",
    "\n",
    "    def get_layer_groups(self, precompute):\n",
    "        if isinstance(self.model, FP16):\n",
    "            model = self.model.module\n",
    "        else:\n",
    "            model = self.model\n",
    "        lgs = list(split_by_idxs(children(model.rn), [model.lr_cut]))\n",
    "        return lgs + [children(model)[1:]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def carce_f_p_r(pred, targs):\n",
    "    _,idx = torch.max(pred, 1)\n",
    "    return fbeta_score(idx==1, targs[:,:,:]==1, beta=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rdce_f(pred, targs):\n",
    "    _,idx = torch.max(pred, 1)\n",
    "    f,p,r = fbeta_score(idx==2, targs[:,:,:]==2, beta=0.5)\n",
    "    return f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def carsig_f_p_r(pred, targs):\n",
    "    p2 = F.sigmoid(pred)\n",
    "    return fbeta_score(p2[:,0,:,:], targs[:,0,:,:], beta=2, threshold=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rdsig_f(pred, targs):\n",
    "    p2 = F.sigmoid(pred)\n",
    "    f,p,r = fbeta_score(p2[:,1,:,:], targs[:,1,:,:], beta=0.5, threshold=0.5)\n",
    "    return f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def car_f_p_r(pred, targs):\n",
    "    _,idx = torch.max(pred, 1)\n",
    "    return fbeta_score(idx==1, targs[:,1,:,:], beta=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rd_f(pred, targs):\n",
    "    _,idx = torch.max(pred, 1)\n",
    "    f,p,r = fbeta_score(idx==2, targs[:,2,:,:], beta=0.5)\n",
    "    return f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fbeta_score(y_pred, y_true, beta, threshold=None, eps=1e-9):\n",
    "    beta2 = beta**2\n",
    "\n",
    "    if threshold:\n",
    "        y_pred = torch.ge(y_pred.float(), threshold).float()\n",
    "    else:\n",
    "        y_pred = y_pred.float()\n",
    "    y_true = y_true.float()\n",
    "\n",
    "    true_positive = (y_pred * y_true).sum()\n",
    "    precision = true_positive/(y_pred.sum()+(eps))\n",
    "    recall = true_positive/(y_true.sum()+eps)\n",
    "    \n",
    "    fb = (precision*recall)/(precision*beta2 + recall + eps)*(1+beta2)\n",
    "    \n",
    "    return fb, precision, recall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def new_acc_sig(pred, targs):\n",
    "    p2 = F.sigmoid(pred)\n",
    "    return ((p2>0.5).long() == targs).float().mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def new_acc_ce(preds, targs):\n",
    "    mx,idx = torch.max(preds, 1)\n",
    "    return (idx == targs).float().mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def new_acc(pred, targs):\n",
    "    _,idx = torch.max(pred, 1)\n",
    "    _,t_idx = torch.max(targs,1)\n",
    "    return (idx == t_idx).float().mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dice_coeff_weight(pred, target, weight):\n",
    "    smooth = 1.\n",
    "    num,c,h,w = pred.shape\n",
    "    m1 = pred.view(num, c, -1)  # Flatten\n",
    "    m2 = target.view(num, c, -1)  # Flatten\n",
    "    intersection = (m1 * m2)\n",
    "    w = V(weight.view(1,-1,1))\n",
    "    i_w = (w*intersection).sum()\n",
    "    m1_w = (w*m1).sum()\n",
    "    m2_w = (w*m2).sum()\n",
    "    return (2. * i_w + smooth) / (m1_w + m2_w + smooth)\n",
    "\n",
    "def dice_coeff(pred, target):\n",
    "    smooth = 1.\n",
    "    num,c,h,w = pred.shape\n",
    "    m1 = pred.view(num, c, -1)  # Flatten\n",
    "    m2 = target.view(num, c, -1)  # Flatten\n",
    "    intersection = (m1 * m2).sum()\n",
    "    return (2. * intersection + smooth) / (m1.sum() + m2.sum() + smooth)\n",
    "\n",
    "\n",
    "class SoftDiceLoss(nn.Module):\n",
    "    def __init__(self, weight=None, size_average=True, softmax=True):\n",
    "        super(SoftDiceLoss, self).__init__()\n",
    "        self.weight = weight\n",
    "        self.softmax = softmax\n",
    "\n",
    "    def forward(self, logits, targets):\n",
    "        probs = F.softmax(logits) if self.softmax else F.sigmoid(logits)\n",
    "        num = targets.size(0)  # Number of batches\n",
    "        targets = torch.cat(((targets==0).unsqueeze(1), (targets==1).unsqueeze(1), (targets==2).unsqueeze(1)), dim=1).float()\n",
    "        if isinstance(logits.data, torch.cuda.HalfTensor):\n",
    "            targets = targets.half()\n",
    "        else:\n",
    "            targets = targets.float()\n",
    "        if self.weight is not None:\n",
    "            score = dice_coeff_weight(probs, targets, self.weight)\n",
    "        else:\n",
    "            score = dice_coeff(probs, targets)\n",
    "        score = 1 - score.sum() / num\n",
    "        return score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_learner(md, m_fn=Unet34Mod, weights=[1,200,2], half=False, softmax=True, dice=False):\n",
    "    out_sz = 3 if softmax else 2\n",
    "    m = to_gpu(m_fn(out_sz))\n",
    "    models = UnetModel(m)\n",
    "    learn = ConvLearner(md, models)\n",
    "    learn.opt_fn=optim.Adam\n",
    "    class_weights = torch.cuda.FloatTensor(weights)\n",
    "    if half:\n",
    "        class_weights = class_weights.half()\n",
    "        learn.half()\n",
    "        \n",
    "#     learn.crit=nn.CrossEntropyLoss(weight=class_weights)\n",
    "    learn.metrics = [new_acc_ce, rdce_f, carce_f_p_r]\n",
    "    \n",
    "    \n",
    "    learn.crit=SoftDiceLoss(weight=class_weights, softmax=softmax)\n",
    "#     if softmax: learn.metrics=[new_acc, rd_f, car_f_p_r]\n",
    "#     else: learn.metrics = [new_acc_sig, rdsig_f, carsig_f_p_r]\n",
    "    return learn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val Labels: 400\n",
      "Val x:400, y:400\n",
      "Trn x:10880, y:10880\n",
      "All x:10880\n"
     ]
    }
   ],
   "source": [
    "ext = '-300'\n",
    "sz=192\n",
    "bs=64\n",
    "random_crop=True\n",
    "md = torch_loader(ext, PATH, bs, sz, workers, random_crop, pseudo_label, val_folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn = get_learner(md, m_fn=Unet34, weights=[1,10,2], softmax=True)\n",
    "learn.load(f'600urn-19-weights')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2778849c4eb84eb789541db654dd2307",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Epoch', max=1), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 85%|████████▌ | 145/170 [00:24<00:04,  5.96it/s, loss=0.63] "
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "learn.lr_find()\n",
    "learn.sched.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr=1e-4\n",
    "wd=1e-7\n",
    "\n",
    "lrs = np.array([lr/200,lr/20,lr])/2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.unfreeze()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e986924de8be4922981c3ad1ef8fa0a5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Epoch', max=2), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 13%|█▎        | 22/170 [00:05<00:37,  3.96it/s, loss=0.986]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Process Process-130:\n",
      "Process Process-131:\n",
      "Process Process-133:\n",
      "Process Process-129:\n",
      "Process Process-127:\n",
      "Process Process-128:\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/paperspace/anaconda3/envs/lyft/lib/python3.6/multiprocessing/process.py\", line 258, in _bootstrap\n",
      "    self.run()\n",
      "Traceback (most recent call last):\n",
      "Process Process-132:\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/paperspace/anaconda3/envs/lyft/lib/python3.6/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/paperspace/anaconda3/envs/lyft/lib/python3.6/multiprocessing/process.py\", line 258, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/home/paperspace/anaconda3/envs/lyft/lib/python3.6/multiprocessing/process.py\", line 258, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/home/paperspace/anaconda3/envs/lyft/lib/python3.6/site-packages/torch/utils/data/dataloader.py\", line 57, in _worker_loop\n",
      "    samples = collate_fn([dataset[i] for i in batch_indices])\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/paperspace/anaconda3/envs/lyft/lib/python3.6/multiprocessing/process.py\", line 258, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/home/paperspace/anaconda3/envs/lyft/lib/python3.6/multiprocessing/process.py\", line 258, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/home/paperspace/anaconda3/envs/lyft/lib/python3.6/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/paperspace/anaconda3/envs/lyft/lib/python3.6/multiprocessing/process.py\", line 258, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/home/paperspace/anaconda3/envs/lyft/lib/python3.6/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/paperspace/anaconda3/envs/lyft/lib/python3.6/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/paperspace/anaconda3/envs/lyft/lib/python3.6/site-packages/torch/utils/data/dataloader.py\", line 57, in _worker_loop\n",
      "    samples = collate_fn([dataset[i] for i in batch_indices])\n",
      "  File \"/home/paperspace/anaconda3/envs/lyft/lib/python3.6/site-packages/torch/utils/data/dataloader.py\", line 57, in _worker_loop\n",
      "    samples = collate_fn([dataset[i] for i in batch_indices])\n",
      "  File \"/home/paperspace/anaconda3/envs/lyft/lib/python3.6/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/paperspace/anaconda3/envs/lyft/lib/python3.6/site-packages/torch/utils/data/dataloader.py\", line 57, in <listcomp>\n",
      "    samples = collate_fn([dataset[i] for i in batch_indices])\n",
      "  File \"/home/paperspace/anaconda3/envs/lyft/lib/python3.6/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/paperspace/anaconda3/envs/lyft/lib/python3.6/site-packages/torch/utils/data/dataloader.py\", line 57, in _worker_loop\n",
      "    samples = collate_fn([dataset[i] for i in batch_indices])\n",
      "  File \"/home/paperspace/anaconda3/envs/lyft/lib/python3.6/site-packages/torch/utils/data/dataloader.py\", line 57, in <listcomp>\n",
      "    samples = collate_fn([dataset[i] for i in batch_indices])\n",
      "  File \"/home/paperspace/anaconda3/envs/lyft/lib/python3.6/site-packages/torch/utils/data/dataloader.py\", line 57, in _worker_loop\n",
      "    samples = collate_fn([dataset[i] for i in batch_indices])\n",
      "  File \"<ipython-input-9-e4a515af82db>\", line 25, in __getitem__\n",
      "    x,y = self.get_x(idx),self.get_y(idx)\n",
      "  File \"/home/paperspace/anaconda3/envs/lyft/lib/python3.6/site-packages/torch/utils/data/dataloader.py\", line 57, in _worker_loop\n",
      "    samples = collate_fn([dataset[i] for i in batch_indices])\n",
      "  File \"/home/paperspace/anaconda3/envs/lyft/lib/python3.6/multiprocessing/process.py\", line 258, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/home/paperspace/anaconda3/envs/lyft/lib/python3.6/site-packages/torch/utils/data/dataloader.py\", line 57, in <listcomp>\n",
      "    samples = collate_fn([dataset[i] for i in batch_indices])\n",
      "  File \"/home/paperspace/anaconda3/envs/lyft/lib/python3.6/site-packages/torch/utils/data/dataloader.py\", line 57, in <listcomp>\n",
      "    samples = collate_fn([dataset[i] for i in batch_indices])\n",
      "  File \"<ipython-input-9-e4a515af82db>\", line 26, in __getitem__\n",
      "    return self.get(self.tfms, x, y)\n",
      "  File \"<ipython-input-9-e4a515af82db>\", line 26, in __getitem__\n",
      "    return self.get(self.tfms, x, y)\n",
      "  File \"/home/paperspace/anaconda3/envs/lyft/lib/python3.6/site-packages/torch/utils/data/dataloader.py\", line 57, in <listcomp>\n",
      "    samples = collate_fn([dataset[i] for i in batch_indices])\n",
      "  File \"<ipython-input-9-e4a515af82db>\", line 14, in get_y\n",
      "    def get_y(self, i): return self.open_y_fn(os.path.join(self.path, self.y[i]))\n",
      "  File \"<ipython-input-9-e4a515af82db>\", line 26, in __getitem__\n",
      "    return self.get(self.tfms, x, y)\n",
      "  File \"<ipython-input-9-e4a515af82db>\", line 26, in __getitem__\n",
      "    return self.get(self.tfms, x, y)\n",
      "  File \"/home/paperspace/anaconda3/envs/lyft/lib/python3.6/site-packages/torchvision/datasets/folder.py\", line 52, in pil_loader\n",
      "    return img.convert('RGB')\n",
      "  File \"<ipython-input-9-e4a515af82db>\", line 21, in get\n",
      "    x, y = fn(x, y)\n",
      "  File \"/home/paperspace/anaconda3/envs/lyft/lib/python3.6/site-packages/torch/utils/data/dataloader.py\", line 57, in <listcomp>\n",
      "    samples = collate_fn([dataset[i] for i in batch_indices])\n",
      "  File \"/home/paperspace/anaconda3/envs/lyft/lib/python3.6/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"<ipython-input-9-e4a515af82db>\", line 21, in get\n",
      "    x, y = fn(x, y)\n",
      "  File \"<ipython-input-9-e4a515af82db>\", line 21, in get\n",
      "    x, y = fn(x, y)\n",
      "  File \"/home/paperspace/anaconda3/envs/lyft/lib/python3.6/site-packages/PIL/Image.py\", line 879, in convert\n",
      "    self.load()\n",
      "  File \"<ipython-input-9-e4a515af82db>\", line 21, in get\n",
      "    x, y = fn(x, y)\n",
      "  File \"<ipython-input-18-715c74425d45>\", line 4, in __call__\n",
      "    x = TTF.resized_crop(x, i, j, h, w, self.size, self.interpolation)\n",
      "  File \"<ipython-input-9-e4a515af82db>\", line 25, in __getitem__\n",
      "    x,y = self.get_x(idx),self.get_y(idx)\n",
      "  File \"<ipython-input-14-6d70a2ea31a4>\", line 2, in <lambda>\n",
      "    return lambda x,y: (tfm(x), y)\n",
      "  File \"<ipython-input-14-6d70a2ea31a4>\", line 2, in <lambda>\n",
      "    return lambda x,y: (tfm(x), y)\n",
      "  File \"<ipython-input-14-6d70a2ea31a4>\", line 2, in <lambda>\n",
      "    return lambda x,y: (tfm(x), y)\n",
      "  File \"/home/paperspace/anaconda3/envs/lyft/lib/python3.6/site-packages/PIL/ImageFile.py\", line 231, in load\n",
      "    n, err_code = decoder.decode(b)\n",
      "  File \"/home/paperspace/anaconda3/envs/lyft/lib/python3.6/site-packages/torchvision/transforms/functional.py\", line 287, in resized_crop\n",
      "    img = resize(img, size, interpolation)\n",
      "  File \"<ipython-input-9-e4a515af82db>\", line 14, in get_y\n",
      "    def get_y(self, i): return self.open_y_fn(os.path.join(self.path, self.y[i]))\n",
      "  File \"/home/paperspace/anaconda3/envs/lyft/lib/python3.6/site-packages/torchvision/transforms/transforms.py\", line 118, in __call__\n",
      "    return F.normalize(tensor, self.mean, self.std)\n",
      "KeyboardInterrupt\n",
      "  File \"/home/paperspace/anaconda3/envs/lyft/lib/python3.6/site-packages/torchvision/transforms/functional.py\", line 199, in resize\n",
      "    return img.resize(size[::-1], interpolation)\n",
      "  File \"/home/paperspace/anaconda3/envs/lyft/lib/python3.6/site-packages/torchvision/transforms/transforms.py\", line 579, in __call__\n",
      "    return transform(img)\n",
      "  File \"/home/paperspace/anaconda3/envs/lyft/lib/python3.6/site-packages/PIL/Image.py\", line 1749, in resize\n",
      "    return self._new(self.im.resize(size, resample, box))\n",
      "  File \"/home/paperspace/anaconda3/envs/lyft/lib/python3.6/site-packages/torchvision/transforms/transforms.py\", line 42, in __call__\n",
      "    img = t(img)\n",
      "KeyboardInterrupt\n",
      "KeyboardInterrupt\n",
      "  File \"/home/paperspace/anaconda3/envs/lyft/lib/python3.6/site-packages/torchvision/transforms/transforms.py\", line 232, in __call__\n",
      "    return self.lambd(img)\n",
      "  File \"/home/paperspace/anaconda3/envs/lyft/lib/python3.6/site-packages/torchvision/transforms/transforms.py\", line 554, in <lambda>\n",
      "    transforms.append(Lambda(lambda img: F.adjust_contrast(img, contrast_factor)))\n",
      "  File \"/home/paperspace/anaconda3/envs/lyft/lib/python3.6/site-packages/torchvision/datasets/folder.py\", line 52, in pil_loader\n",
      "    return img.convert('RGB')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  File \"/home/paperspace/anaconda3/envs/lyft/lib/python3.6/site-packages/torchvision/transforms/transforms.py\", line 118, in __call__\n",
      "    return F.normalize(tensor, self.mean, self.std)\n",
      "  File \"/home/paperspace/anaconda3/envs/lyft/lib/python3.6/site-packages/torchvision/transforms/functional.py\", line 425, in adjust_contrast\n",
      "    enhancer = ImageEnhance.Contrast(img)\n",
      "  File \"/home/paperspace/anaconda3/envs/lyft/lib/python3.6/site-packages/torch/utils/data/dataloader.py\", line 57, in _worker_loop\n",
      "    samples = collate_fn([dataset[i] for i in batch_indices])\n",
      "  File \"/home/paperspace/anaconda3/envs/lyft/lib/python3.6/site-packages/PIL/ImageEnhance.py\", line 66, in __init__\n",
      "    mean = int(ImageStat.Stat(image.convert(\"L\")).mean[0] + 0.5)\n",
      "  File \"/home/paperspace/anaconda3/envs/lyft/lib/python3.6/site-packages/torchvision/transforms/functional.py\", line 160, in normalize\n",
      "    for t, m, s in zip(tensor, mean, std):\n",
      "  File \"/home/paperspace/anaconda3/envs/lyft/lib/python3.6/site-packages/PIL/Image.py\", line 979, in convert\n",
      "    im = self.im.convert(mode, dither)\n",
      "  File \"/home/paperspace/anaconda3/envs/lyft/lib/python3.6/site-packages/torch/tensor.py\", line 359, in __iter__\n",
      "    if self.dim() == 0:\n",
      "KeyboardInterrupt\n",
      "  File \"<ipython-input-18-715c74425d45>\", line 5, in __call__\n",
      "    y = TTF.resized_crop(y, i, j, h, w, self.size, self.interpolation)\n",
      "  File \"/home/paperspace/anaconda3/envs/lyft/lib/python3.6/site-packages/torch/utils/data/dataloader.py\", line 57, in <listcomp>\n",
      "    samples = collate_fn([dataset[i] for i in batch_indices])\n",
      "  File \"/home/paperspace/anaconda3/envs/lyft/lib/python3.6/site-packages/PIL/Image.py\", line 879, in convert\n",
      "    self.load()\n",
      "  File \"/home/paperspace/anaconda3/envs/lyft/lib/python3.6/site-packages/PIL/ImageFile.py\", line 231, in load\n",
      "    n, err_code = decoder.decode(b)\n",
      "  File \"<ipython-input-9-e4a515af82db>\", line 26, in __getitem__\n",
      "    return self.get(self.tfms, x, y)\n",
      "KeyboardInterrupt\n",
      "  File \"<ipython-input-9-e4a515af82db>\", line 21, in get\n",
      "    x, y = fn(x, y)\n",
      "KeyboardInterrupt\n",
      "  File \"/home/paperspace/anaconda3/envs/lyft/lib/python3.6/site-packages/torchvision/transforms/functional.py\", line 160, in normalize\n",
      "    for t, m, s in zip(tensor, mean, std):\n",
      "ERROR:root:Internal Python error in the inspect module.\n",
      "Below is the traceback from this internal error.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/home/paperspace/anaconda3/envs/lyft/lib/python3.6/site-packages/IPython/core/interactiveshell.py\", line 2963, in run_code\n",
      "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "  File \"<ipython-input-87-31709630340d>\", line 1, in <module>\n",
      "    learn.fit(lrs, 1, wds=wd, cycle_len=2,use_clr=(20,10))\n",
      "  File \"/home/paperspace/anaconda3/envs/lyft/lib/python3.6/site-packages/fastai/learner.py\", line 287, in fit\n",
      "    return self.fit_gen(self.model, self.data, layer_opt, n_cycle, **kwargs)\n",
      "  File \"/home/paperspace/anaconda3/envs/lyft/lib/python3.6/site-packages/fastai/learner.py\", line 234, in fit_gen\n",
      "    swa_eval_freq=swa_eval_freq, **kwargs)\n",
      "  File \"/home/paperspace/anaconda3/envs/lyft/lib/python3.6/site-packages/fastai/model.py\", line 139, in fit\n",
      "    loss = model_stepper.step(V(x),V(y), epoch)\n",
      "  File \"/home/paperspace/anaconda3/envs/lyft/lib/python3.6/site-packages/fastai/model.py\", line 54, in step\n",
      "    loss = raw_loss = self.crit(output, y)\n",
      "  File \"/home/paperspace/anaconda3/envs/lyft/lib/python3.6/site-packages/torch/nn/modules/module.py\", line 491, in __call__\n",
      "    result = self.forward(*input, **kwargs)\n",
      "  File \"<ipython-input-81-07a7f051937a>\", line 37, in forward\n",
      "    score = dice_coeff_weight(probs, targets, self.weight)\n",
      "  File \"<ipython-input-81-07a7f051937a>\", line 8, in dice_coeff_weight\n",
      "    i_w = (w*intersection).sum()\n",
      "KeyboardInterrupt\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/paperspace/anaconda3/envs/lyft/lib/python3.6/site-packages/IPython/core/interactiveshell.py\", line 1863, in showtraceback\n",
      "    stb = value._render_traceback_()\n",
      "AttributeError: 'KeyboardInterrupt' object has no attribute '_render_traceback_'\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/paperspace/anaconda3/envs/lyft/lib/python3.6/site-packages/IPython/core/ultratb.py\", line 1095, in get_records\n",
      "    return _fixed_getinnerframes(etb, number_of_lines_of_context, tb_offset)\n",
      "  File \"/home/paperspace/anaconda3/envs/lyft/lib/python3.6/site-packages/IPython/core/ultratb.py\", line 311, in wrapped\n",
      "    return f(*args, **kwargs)\n",
      "  File \"/home/paperspace/anaconda3/envs/lyft/lib/python3.6/site-packages/IPython/core/ultratb.py\", line 345, in _fixed_getinnerframes\n",
      "    records = fix_frame_records_filenames(inspect.getinnerframes(etb, context))\n",
      "  File \"/home/paperspace/anaconda3/envs/lyft/lib/python3.6/inspect.py\", line 1483, in getinnerframes\n",
      "    frameinfo = (tb.tb_frame,) + getframeinfo(tb, context)\n",
      "  File \"/home/paperspace/anaconda3/envs/lyft/lib/python3.6/inspect.py\", line 1441, in getframeinfo\n",
      "    filename = getsourcefile(frame) or getfile(frame)\n",
      "  File \"/home/paperspace/anaconda3/envs/lyft/lib/python3.6/inspect.py\", line 696, in getsourcefile\n",
      "    if getattr(getmodule(object, filename), '__loader__', None) is not None:\n",
      "  File \"/home/paperspace/anaconda3/envs/lyft/lib/python3.6/inspect.py\", line 733, in getmodule\n",
      "    if ismodule(module) and hasattr(module, '__file__'):\n",
      "  File \"/home/paperspace/anaconda3/envs/lyft/lib/python3.6/site-packages/torch/utils/data/dataloader.py\", line 178, in handler\n",
      "    _error_if_any_worker_fails()\n",
      "RuntimeError: DataLoader worker (pid 1656) exited unexpectedly with exit code 1.\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  File \"/home/paperspace/anaconda3/envs/lyft/lib/python3.6/site-packages/torch/tensor.py\", line 361, in <lambda>\n",
      "    return iter(imap(lambda i: self[i], range(self.size(0))))\n",
      "KeyboardInterrupt\n"
     ]
    }
   ],
   "source": [
    "learn.fit(lrs, 1, wds=wd, cycle_len=2,use_clr=(20,10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Diff weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2f93d80616374f9fbb183a7f62e7e0dd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Epoch', max=3), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch      trn_loss   val_loss   new_acc_ce rdce_f     carce_f_p_r \n",
      "    0      0.585691   0.430237   0.982815   0.990479   0.863085   0.726227   0.907799  \n",
      "    1      0.507419   0.352849   0.981836   0.990482   0.86298    0.685963   0.924875  \n",
      "    2      0.441042   0.359947   0.981674   0.99042    0.863402   0.690874   0.923274  \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.3599466514587402,\n",
       " 0.981674427986145,\n",
       " 0.9904195880889892,\n",
       " 0.8634018087387085,\n",
       " 0.6908737778663635,\n",
       " 0.9232737779617309]"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "learn = get_learner(md, m_fn=Unet34, weights=[1,1000,2], softmax=True)\n",
    "learn.load(f'600urn-19-weights')\n",
    "\n",
    "learn.unfreeze()\n",
    "\n",
    "learn.fit(lrs, 1, wds=wd, cycle_len=3,use_clr=(20,10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn = get_learner(md, m_fn=Unet34, weights=[1/1000,1,1/50], softmax=True)\n",
    "learn.load(f'600urn-19-weights')\n",
    "\n",
    "learn.unfreeze()\n",
    "\n",
    "learn.fit(lrs, 1, wds=wd, cycle_len=2,use_clr=(20,10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn = get_learner(md, m_fn=Unet34, weights=[1,1000,2], softmax=True)\n",
    "learn.load(f'600urn-19-weights')\n",
    "\n",
    "learn.unfreeze()\n",
    "\n",
    "learn.fit(lrs, 1, wds=wd, cycle_len=2,use_clr=(20,10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 512"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val Labels: 400\n",
      "Val x:400, y:400\n",
      "Trn x:10880, y:10880\n",
      "All x:10880\n"
     ]
    }
   ],
   "source": [
    "ext = ''\n",
    "sz=320\n",
    "bs=32\n",
    "random_crop=True\n",
    "md = torch_loader(ext, PATH, bs, sz, workers, random_crop, pseudo_label, val_folder, val_bs=16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn = get_learner(md, m_fn=Unet34, weights=[1,4,2], softmax=True)\n",
    "learn.load(f'600urn-19-weights')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr=1e-2\n",
    "wd=1e-7\n",
    "\n",
    "lrs = np.array([lr/200,lr/20,lr])/2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.unfreeze()\n",
    "learn.set_bn_freeze(learn.model.rn, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b2de2eaf125d417cb9e45f4ab58490e9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Epoch', max=1), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch      trn_loss   val_loss   new_acc_ce rdce_f     carce_f_p_r \n",
      "    0      0.066339   0.063312   0.986369   0.989354   0.831383   0.687918   0.882404  \n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.06331216678023338,\n",
       " 0.9863685894012452,\n",
       " 0.9893538594245911,\n",
       " 0.8313831675052643,\n",
       " 0.6879177880287171,\n",
       " 0.8824036765098572]"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "learn.fit(lrs, 1, wds=wd, cycle_len=1,use_clr=(20,2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6ad9db5764ee444a8e7678ebd91b6cf3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Epoch', max=3), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch      trn_loss   val_loss   new_acc_ce rdce_f     carce_f_p_r \n",
      "    0      0.06418    0.063924   0.986295   0.989288   0.832011   0.681567   0.886217  \n",
      "    1      0.062894   0.063157   0.98638    0.989386   0.833749   0.680557   0.889093  \n",
      "    2      0.062201   0.062188   0.986423   0.989399   0.833299   0.683626   0.886873  \n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.06218768492341042,\n",
       " 0.9864228820800781,\n",
       " 0.989399163722992,\n",
       " 0.8332987737655639,\n",
       " 0.6836258298158646,\n",
       " 0.8868733263015747]"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "learn.fit(lrs/20, 1, wds=wd, cycle_len=3,use_clr=(20,2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.save(f'600urn-{S_PREFIX}-w4-320')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Try 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val Labels: 400\n",
      "Val x:400, y:400\n",
      "Trn x:10880, y:10880\n",
      "All x:10880\n"
     ]
    }
   ],
   "source": [
    "ext = ''\n",
    "sz=320\n",
    "bs=24\n",
    "random_crop=True\n",
    "md = torch_loader(ext, PATH, bs, sz, workers, random_crop, pseudo_label, val_folder, val_bs=16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn = get_learner(md, m_fn=Unet34, weights=[1,25,2], softmax=True)\n",
    "learn.load(f'600urn-19-weights')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr=1e-2\n",
    "wd=1e-7\n",
    "\n",
    "lrs = np.array([lr/200,lr/20,lr])/2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.unfreeze()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Epoch', max=1), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch      trn_loss   val_loss   new_acc_ce rdce_f     carce_f_p_r \n",
      "    0      0.083335   0.081102   0.978033   0.989201   0.783248   0.489305   0.957276  \n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.08110182389616966,\n",
       " 0.9780332469940185,\n",
       " 0.9892006802558899,\n",
       " 0.7832478368282318,\n",
       " 0.48930494338274,\n",
       " 0.9572764658927917]"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "learn.fit(lrs, 1, wds=wd, cycle_len=1,use_clr=(20,2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.save(f'600urn-{S_PREFIX}-w25')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val Labels: 400\n",
      "Val x:400, y:400\n",
      "Trn x:10880, y:10880\n",
      "All x:10880\n"
     ]
    }
   ],
   "source": [
    "ext = ''\n",
    "sz=384\n",
    "bs=24\n",
    "random_crop=True\n",
    "md = torch_loader(ext, PATH, bs, sz, workers, random_crop, pseudo_label, val_folder, val_bs=16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn = get_learner(md, m_fn=Unet34, weights=[1,4,2], softmax=True)\n",
    "learn.load(f'600urn-19-weights')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr=1e-3\n",
    "wd=1e-7\n",
    "\n",
    "lrs = np.array([lr/200,lr/20,lr])/2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.unfreeze()\n",
    "# learn.bn_freeze(True)\n",
    "learn.set_bn_freeze(learn.model.rn, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Epoch', max=1), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch      trn_loss   val_loss   new_acc_ce rdce_f     carce_f_p_r \n",
      "    0      0.086951   0.048171   0.988757   0.990054   0.865028   0.776669   0.892201  \n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.04817071918398142,\n",
       " 0.9887566232681274,\n",
       " 0.9900537562370301,\n",
       " 0.8650275468826294,\n",
       " 0.7766694796085357,\n",
       " 0.892201099395752]"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "learn.fit(lrs, 1, wds=wd, cycle_len=1,use_clr=(20,2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8d94e2891f4b4c4cbf248fa17c758b68",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Epoch', max=3), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch      trn_loss   val_loss   new_acc_ce rdce_f     carce_f_p_r \n",
      "    0      0.063598   0.047333   0.988861   0.990619   0.868936   0.757252   0.90496   \n",
      "    1      0.05666    0.045767   0.989742   0.992007   0.875719   0.796291   0.899614  \n",
      "    2      0.054229   0.047209   0.9898     0.991072   0.872787   0.802669   0.893699  \n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.04720917873084545,\n",
       " 0.9897998213768006,\n",
       " 0.9910722017288208,\n",
       " 0.8727874612808227,\n",
       " 0.8026689505577087,\n",
       " 0.8936987066268921]"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "learn.fit(lrs, 1, wds=wd, cycle_len=3,use_clr=(20,2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.save(f'600urn-{S_PREFIX}-w4-384')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loader 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val Labels: 400\n",
      "Val x:400, y:400\n",
      "Trn x:10880, y:10880\n",
      "All x:10880\n"
     ]
    }
   ],
   "source": [
    "ext = ''\n",
    "sz=384\n",
    "bs=16\n",
    "random_crop=False\n",
    "md = torch_loader(ext, PATH, bs, sz, workers, random_crop, pseudo_label, val_folder, val_bs=16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn = get_learner(md, m_fn=Unet34, weights=[1,4,2], softmax=True)\n",
    "learn.load(f'600urn-{S_PREFIX}-w4-384')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr=1e-3\n",
    "wd=1e-7\n",
    "\n",
    "lrs = np.array([lr/200,lr/20,lr])/2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.unfreeze()\n",
    "# learn.bn_freeze(True)\n",
    "learn.set_bn_freeze(learn.model.rn, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "67c0c06421ef489d98d567d70ccf953d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Epoch', max=3), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch      trn_loss   val_loss   new_acc_ce rdce_f     carce_f_p_r \n",
      "    0      0.020713   0.036598   0.990462   0.992489   0.880022   0.800293   0.90361   \n",
      "    1      0.01745    0.03544    0.990664   0.992643   0.88513    0.800949   0.910265  \n",
      "    2      0.016676   0.034573   0.990947   0.992732   0.88495    0.807818   0.907696  \n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.03457291729748249,\n",
       " 0.9909473490715027,\n",
       " 0.9927318120002746,\n",
       " 0.8849498629570007,\n",
       " 0.8078181207180023,\n",
       " 0.9076959681510925]"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "learn.fit(lrs, 1, wds=wd, cycle_len=3,use_clr=(20,2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.save(f'600urn-{S_PREFIX}-w4-384-nocrop')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val Labels: 400\n",
      "Val x:400, y:400\n",
      "Trn x:10880, y:10880\n",
      "All x:10880\n"
     ]
    }
   ],
   "source": [
    "ext = ''\n",
    "sz=384\n",
    "bs=24\n",
    "random_crop=True\n",
    "md = torch_loader(ext, PATH, bs, sz, workers, random_crop, pseudo_label, val_folder, val_bs=16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn = get_learner(md, m_fn=Unet34, weights=[1,4,2], softmax=True)\n",
    "learn.load(f'600urn-{S_PREFIX}-w4-384-nocrop')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr=1e-4\n",
    "wd=1e-7\n",
    "\n",
    "lrs = np.array([lr/200,lr/20,lr])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.unfreeze()\n",
    "# learn.bn_freeze(True)\n",
    "learn.set_bn_freeze(learn.model.rn, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3a7a23c5c66d4cb992376b88e79c88bd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Epoch', max=2), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch      trn_loss   val_loss   new_acc_ce rdce_f     carce_f_p_r \n",
      "    0      0.057076   0.04296    0.989948   0.991177   0.87945    0.800968   0.903084  \n",
      "    1      0.053725   0.043992   0.989946   0.991651   0.881595   0.792273   0.909094  \n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.04399244755506516,\n",
       " 0.9899460458755494,\n",
       " 0.9916511750221253,\n",
       " 0.881594581604004,\n",
       " 0.7922728323936462,\n",
       " 0.9090940356254578]"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "learn.fit(lrs, 1, wds=wd, cycle_len=2,use_clr=(20,2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Back to crop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val Labels: 400\n",
      "Val x:400, y:400\n",
      "Trn x:10880, y:10880\n",
      "All x:10880\n"
     ]
    }
   ],
   "source": [
    "ext = ''\n",
    "sz=384\n",
    "bs=24\n",
    "random_crop=False\n",
    "md = torch_loader(ext, PATH, bs, sz, workers, random_crop, pseudo_label, val_folder, val_bs=16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn = get_learner(md, m_fn=Unet34, weights=[1,4,2], softmax=True)\n",
    "learn.load(f'600urn-{S_PREFIX}-w4-384-nocrop')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3c015e051df748e3b9d72e3bed62acf7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Epoch', max=3), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch      trn_loss   val_loss   new_acc_ce rdce_f     carce_f_p_r \n",
      "    0      0.015198   0.034632   0.990812   0.992898   0.885754   0.798041   0.912007  \n",
      "    1      0.015293   0.034699   0.990916   0.99267    0.884624   0.80851    0.907058  \n",
      "    2      0.01533    0.03449    0.990868   0.992601   0.8852     0.80398    0.909299  \n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.034490275271236895,\n",
       " 0.9908682608604431,\n",
       " 0.9926014685630798,\n",
       " 0.8852004361152649,\n",
       " 0.8039799547195434,\n",
       " 0.9092989134788513]"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "learn.fit(lrs/20, 1, wds=wd, cycle_len=3,use_clr=(20,2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.save(f'600urn-{S_PREFIX}-w4-384-nocrop-2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = torch.ones([3,5,6])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 5, 6])"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "b = a.unsqueeze(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 2, 5, 6])"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cat((b,b), dim=1).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  },
  "toc": {
   "colors": {
    "hover_highlight": "#DAA520",
    "navigate_num": "#000000",
    "navigate_text": "#333333",
    "running_highlight": "#FF0000",
    "selected_highlight": "#FFD700",
    "sidebar_border": "#EEEEEE",
    "wrapper_background": "#FFFFFF"
   },
   "moveMenuLeft": true,
   "nav_menu": {
    "height": "86px",
    "width": "252px"
   },
   "navigate_menu": true,
   "number_sections": true,
   "sideBar": true,
   "threshold": 4,
   "toc_cell": false,
   "toc_section_display": "block",
   "toc_window_display": false,
   "widenNotebook": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
