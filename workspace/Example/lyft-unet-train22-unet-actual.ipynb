{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "%reload_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fastai.conv_learner import *\n",
    "# from fastai.dataset import *\n",
    "from fastai.models.resnet import vgg_resnet50\n",
    "\n",
    "import json\n",
    "from glob import glob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.backends.cudnn.benchmark=True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH = Path('../data/all')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_img(im, figsize=None, ax=None, alpha=None):\n",
    "    if not ax: fig,ax = plt.subplots(figsize=figsize)\n",
    "    ax.imshow(im, alpha=alpha)\n",
    "    ax.set_axis_off()\n",
    "    return ax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "VEHICLES=10\n",
    "ROADS=7\n",
    "ROAD_LINES=6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "S_PREFIX = '22-unet'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision.datasets.folder import pil_loader\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.transforms.functional as TTF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MatchedFilesDataset(Dataset):\n",
    "    def __init__(self, fnames, y, tfms, path):\n",
    "        self.path,self.fnames = path,fnames\n",
    "        self.open_fn = pil_loader\n",
    "        self.y=y\n",
    "        self.open_y_fn = pil_loader\n",
    "        assert(len(fnames)==len(y))\n",
    "        \n",
    "        self.n = self.get_n()\n",
    "        self.c = self.get_c()\n",
    "        self.tfms = tfms\n",
    "#         self.sz = self.get_sz()\n",
    "        \n",
    "#     def get_sz(self): return self.transform.sz\n",
    "    def get_x(self, i): return self.open_fn(os.path.join(self.path, self.fnames[i]))\n",
    "    def get_y(self, i): return self.open_y_fn(os.path.join(self.path, self.y[i]))\n",
    "    def get_n(self): return len(self.fnames)\n",
    "    def get_c(self): return 2\n",
    "    \n",
    "    def get(self, tfms, x, y):\n",
    "        for fn in tfms:\n",
    "            #pdb.set_trace()\n",
    "            x, y = fn(x, y)\n",
    "        return (x, y)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        x,y = self.get_x(idx),self.get_y(idx)\n",
    "        return self.get(self.tfms, x, y)\n",
    "    \n",
    "    def __len__(self): return self.n\n",
    "\n",
    "    def resize_imgs(self, targ, new_path):\n",
    "        dest = resize_imgs(self.fnames, targ, self.path, new_path)\n",
    "        return self.__class__(self.fnames, self.y, self.transform, dest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Seems to speed up training by ~2%\n",
    "class DataPrefetcher():\n",
    "    def __init__(self, loader, stop_after=None):\n",
    "        self.loader = loader\n",
    "        self.dataset = loader.dataset\n",
    "        self.stream = torch.cuda.Stream()\n",
    "        self.stop_after = stop_after\n",
    "        self.next_input = None\n",
    "        self.next_target = None\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.loader)\n",
    "    \n",
    "    def preload(self):\n",
    "        try:\n",
    "            self.next_input, self.next_target = next(self.loaditer)\n",
    "        except StopIteration:\n",
    "            self.next_input = None\n",
    "            self.next_target = None\n",
    "            return\n",
    "        with torch.cuda.stream(self.stream):\n",
    "            self.next_input = self.next_input.cuda(async=True)\n",
    "            self.next_target = self.next_target.cuda(async=True)\n",
    "\n",
    "    def __iter__(self):\n",
    "        count = 0\n",
    "        self.loaditer = iter(self.loader)\n",
    "        self.preload()\n",
    "        while self.next_input is not None:\n",
    "            torch.cuda.current_stream().wait_stream(self.stream)\n",
    "            input = self.next_input\n",
    "            target = self.next_target\n",
    "            self.preload()\n",
    "            count += 1\n",
    "            yield input, target\n",
    "            if type(self.stop_after) is int and (count > self.stop_after):\n",
    "                break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def crop_bg_pil(x,y):\n",
    "    w, h = x.size\n",
    "    top = int(h/3.75)\n",
    "    bot = int(h*.9 + h/150)\n",
    "    pad_right=32-w%32\n",
    "    return TTF.crop(x, top, 0, bot-top, w+pad_right), TTF.crop(y, top, 0, bot-top, w+pad_right)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RHF(object):\n",
    "    def __init__(self, p=0.5): self.p = p\n",
    "    def __call__(self, x, y):\n",
    "        if random.random() < self.p:\n",
    "            return TTF.hflip(x), TTF.hflip(y)\n",
    "        return x,y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RR(object):\n",
    "    def __init__(self, degrees=2): self.degrees = degrees\n",
    "    def __call__(self, x, y):\n",
    "        angle = random.uniform(-self.degrees, self.degrees)\n",
    "        return TTF.rotate(x, angle), TTF.rotate(y, angle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tfm_x_wrapper(tfm):\n",
    "    return lambda x,y: (tfm(x), y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RC():\n",
    "    def __init__(self, targ_sz):\n",
    "        self.targ_sz = targ_sz\n",
    "\n",
    "    def __call__(self, x, y):\n",
    "        rand_w = random.uniform(0, 1)\n",
    "        rand_h = random.uniform(0, 1)\n",
    "        w,h = x.size\n",
    "        t_w,t_h = self.targ_sz\n",
    "        start_x = np.floor(rand_w*(w-t_w)).astype(int)\n",
    "        start_y = np.floor(rand_h*(h-t_h)).astype(int)\n",
    "        return TTF.crop(x, start_y, start_x, t_h, t_w), TTF.crop(y, start_y, start_x, t_h, t_w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_y(y_img):\n",
    "    y_new = np.zeros(y_img.shape, dtype=int)\n",
    "    y_new[y_img==VEHICLES] = 1\n",
    "    cutoff_y = int(y_new.shape[0]*.83)\n",
    "    y_new[cutoff_y:,:] = 0\n",
    "\n",
    "    y_new[y_img==ROADS] = 2\n",
    "    y_new[y_img==ROAD_LINES] = 2\n",
    "    return torch.from_numpy(y_new).long()\n",
    "\n",
    "def xy_tensor(x,y):\n",
    "    y_img = np.array(y, np.int32, copy=False)\n",
    "    return TTF.to_tensor(x), convert_y(y_img[:,:,0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def torch_loader(f_ext, data_path, bs, size, workers=7, random_crop=False):\n",
    "    # Data loading code\n",
    "    x_names = np.sort(np.array(glob(str(data_path/f'CameraRGB{f_ext}'/'*.png'))))\n",
    "    y_names = np.sort(np.array(glob(str(data_path/f'CameraSeg{f_ext}'/'*.png'))))\n",
    "#     x_names_val = np.array(glob(str(data_path/f'AnswersRGB{f_ext}'/'*.png')))\n",
    "#     y_names_val = np.array(glob(str(data_path/f'AnswersSeg{f_ext}'/'*.png')))\n",
    "    val_idxs = list(range(100))\n",
    "#     val_x,val_y = x_names_val, y_names_val\n",
    "#     trn_x,trn_y = x_names, y_names\n",
    "    ((val_x,trn_x),(val_y,trn_y)) = split_by_idx(val_idxs, x_names, y_names)\n",
    "    normalize = transforms.Normalize(mean=[0.4914 , 0.48216, 0.44653], std=[0.24703, 0.24349, 0.26159])\n",
    "    \n",
    "    train_tfms = [\n",
    "        crop_bg_pil,\n",
    "        tfm_x_wrapper(transforms.ColorJitter(.2,.2,.2)),\n",
    "#         tfm_x_wrapper(Lighting(0.1, __imagenet_pca['eigval'], __imagenet_pca['eigvec'])),\n",
    "        RR(),\n",
    "        RHF(),\n",
    "#         RC((size,size)),\n",
    "        xy_tensor,\n",
    "        tfm_x_wrapper(normalize),\n",
    "    ]\n",
    "    if random_crop:\n",
    "        train_tfms.insert(3,RC((size,size)))\n",
    "    train_dataset = MatchedFilesDataset(trn_x, trn_y, train_tfms, path='')\n",
    "    train_loader = torch.utils.data.DataLoader(\n",
    "        train_dataset, batch_size=bs, shuffle=True,\n",
    "        num_workers=workers, pin_memory=True)\n",
    "\n",
    "    val_tfms = [\n",
    "        crop_bg_pil,\n",
    "        xy_tensor,\n",
    "        tfm_x_wrapper(normalize)\n",
    "    ]\n",
    "    val_dataset = MatchedFilesDataset(val_x, val_y, val_tfms, path='')\n",
    "    val_loader = torch.utils.data.DataLoader(\n",
    "        val_dataset, batch_size=bs, shuffle=False,\n",
    "        num_workers=workers, pin_memory=True)\n",
    "\n",
    "    train_loader = DataPrefetcher(train_loader)\n",
    "    val_loader = DataPrefetcher(val_loader)\n",
    "    \n",
    "    data = ModelData(data_path, train_loader, val_loader)\n",
    "    return data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def denorm(x):\n",
    "    x_np = x.cpu().numpy()\n",
    "    x_np = np.rollaxis(x_np, 0, 3)\n",
    "    mean=np.array([0.4914 , 0.48216, 0.44653])\n",
    "    std=np.array([0.24703, 0.24349, 0.26159])\n",
    "    x_np = x_np*std+mean\n",
    "    return x_np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def new_acc(preds, targs):\n",
    "    mx,idx = torch.max(preds, 1)\n",
    "    return (idx == targs).float().mean()\n",
    "def dice_mult(pred, targs):\n",
    "#     pred = (pred>0).float()\n",
    "    mx,idx = torch.max(pred, 1)\n",
    "    pred = idx.float()\n",
    "    targs = targs.float()\n",
    "    return 2. * (pred*targs).sum() / (pred+targs).sum()\n",
    "def dice(pred, targs):\n",
    "    pred = (pred>0).float()\n",
    "    return 2. * (pred*targs).sum() / (pred+targs).sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## U-net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "BN_EPS = 1e-4  #1e-4  #1e-5\n",
    "class ConvBnRelu2d(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, kernel_size=3, padding=1, dilation=1, stride=1, groups=1, is_bn=True, is_relu=True):\n",
    "        super(ConvBnRelu2d, self).__init__()\n",
    "        self.conv = nn.Conv2d(in_channels, out_channels, kernel_size=kernel_size, padding=padding, stride=stride, dilation=dilation, groups=groups, bias=False)\n",
    "        self.bn   = nn.BatchNorm2d(out_channels, eps=BN_EPS)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        if is_bn   is False: self.bn  =None\n",
    "        if is_relu is False: self.relu=None\n",
    "\n",
    "\n",
    "    def forward(self,x):\n",
    "        x = self.conv(x)\n",
    "        if self.bn   is not None: x = self.bn(x)\n",
    "        if self.relu is not None: x = self.relu(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConvResidual (nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, stride=1):\n",
    "        super(ConvResidual, self).__init__()\n",
    "\n",
    "        self.block = nn.Sequential(\n",
    "            ConvBnRelu2d(in_channels,  out_channels, kernel_size=3, padding=1,  stride=1 ),\n",
    "            ConvBnRelu2d(out_channels, out_channels, kernel_size=3, padding=1,  stride=1, is_relu=False),\n",
    "        )\n",
    "        self.shortcut = None\n",
    "        if in_channels!=out_channels or stride!=1:\n",
    "           self.shortcut = nn.Conv2d(in_channels, out_channels, kernel_size=1, padding=0, stride=stride,  bias=True)\n",
    "\n",
    "    def forward(self, x):\n",
    "        r = x if self.shortcut is None else self.shortcut(x)\n",
    "        x = self.block(x)\n",
    "        x = F.relu(x+r, inplace=True)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "## origainl 3x3 stack filters used in UNet\n",
    "class StackEncoder (nn.Module):\n",
    "    def __init__(self, x_channels, y_channels, kernel_size=3):\n",
    "        super(StackEncoder, self).__init__()\n",
    "        padding=(kernel_size-1)//2\n",
    "        self.encode = nn.Sequential(\n",
    "            ConvBnRelu2d(x_channels, y_channels, kernel_size=kernel_size, padding=padding, dilation=1, stride=1, groups=1),\n",
    "            ConvBnRelu2d(y_channels, y_channels, kernel_size=kernel_size, padding=padding, dilation=1, stride=1, groups=1),\n",
    "        )\n",
    "\n",
    "    def forward(self,x):\n",
    "        y = self.encode(x)\n",
    "        y_small = F.max_pool2d(y, kernel_size=2, stride=2)\n",
    "        return y, y_small\n",
    "\n",
    "\n",
    "class StackDecoder (nn.Module):\n",
    "    def __init__(self, x_big_channels, x_channels, y_channels, kernel_size=3):\n",
    "        super(StackDecoder, self).__init__()\n",
    "        padding=(kernel_size-1)//2\n",
    "\n",
    "        self.decode = nn.Sequential(\n",
    "            ConvBnRelu2d(x_big_channels+x_channels, y_channels, kernel_size=kernel_size, padding=padding, dilation=1, stride=1, groups=1),\n",
    "            ConvBnRelu2d(y_channels, y_channels, kernel_size=kernel_size, padding=padding, dilation=1, stride=1, groups=1),\n",
    "            ConvBnRelu2d(y_channels, y_channels, kernel_size=kernel_size, padding=padding, dilation=1, stride=1, groups=1),\n",
    "        )\n",
    "\n",
    "    def forward(self, x_big, x):\n",
    "        N,C,H,W = x_big.size()\n",
    "        y = F.upsample(x, size=(H,W),mode='bilinear')\n",
    "        #y = F.upsample(x, scale_factor=2,mode='bilinear')\n",
    "        y = torch.cat([y,x_big],1)\n",
    "        y = self.decode(y)\n",
    "        return  y\n",
    "##---------------------------------------------------------------\n",
    "\n",
    "\n",
    "## origainl 3x3 stack filters used in UNet\n",
    "class ResStackEncoder (nn.Module):\n",
    "    def __init__(self, x_channels, y_channels):\n",
    "        super(ResStackEncoder, self).__init__()\n",
    "        self.encode = ConvResidual(x_channels, y_channels)\n",
    "\n",
    "    def forward(self,x):\n",
    "        y = self.encode(x)\n",
    "        y_small = F.max_pool2d(y, kernel_size=2, stride=2)\n",
    "        return y, y_small\n",
    "\n",
    "\n",
    "class ResStackDecoder (nn.Module):\n",
    "    def __init__(self, x_big_channels, x_channels, y_channels, kernel_size=3):\n",
    "        super(ResStackDecoder, self).__init__()\n",
    "        padding=(kernel_size-1)//2\n",
    "\n",
    "        self.decode = nn.Sequential(\n",
    "            ConvBnRelu2d(x_big_channels+x_channels, y_channels, kernel_size=kernel_size, padding=padding, dilation=1, stride=1, groups=1),\n",
    "            ConvResidual(y_channels, y_channels)\n",
    "        )\n",
    "\n",
    "    def forward(self, x_big, x):\n",
    "        N,C,H,W = x_big.size()\n",
    "        y = F.upsample(x, size=(H,W),mode='bilinear')\n",
    "        #y = F.upsample(x, scale_factor=2,mode='bilinear')\n",
    "        y = torch.cat([y,x_big],1)\n",
    "        y = self.decode(y)\n",
    "        return  y\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 256x256\n",
    "class Unet256 (nn.Module):\n",
    "    def __init__(self, out_c=3, in_c=3):\n",
    "        super().__init__()\n",
    "#         C,H,W = in_shape\n",
    "        #assert(C==3)\n",
    "\n",
    "        #256\n",
    "        self.down2 = StackEncoder(in_c,   64, kernel_size=3)   #128\n",
    "        self.down3 = StackEncoder( 64,  128, kernel_size=3)   # 64\n",
    "        self.down4 = StackEncoder(128,  256, kernel_size=3)   # 32\n",
    "        self.down5 = StackEncoder(256,  512, kernel_size=3)   # 16\n",
    "        self.down6 = StackEncoder(512, 1024, kernel_size=3)   #  8\n",
    "\n",
    "        self.center = nn.Sequential(\n",
    "            #ConvBnRelu2d( 512, 1024, kernel_size=3, padding=1, stride=1 ),\n",
    "            ConvBnRelu2d(1024, 1024, kernel_size=3, padding=1, stride=1 ),\n",
    "        )\n",
    "\n",
    "        # 8\n",
    "        # x_big_channels, x_channels, y_channels\n",
    "        self.up6 = StackDecoder(1024,1024, 512, kernel_size=3)  # 16\n",
    "        self.up5 = StackDecoder( 512, 512, 256, kernel_size=3)  # 32\n",
    "        self.up4 = StackDecoder( 256, 256, 128, kernel_size=3)  # 64\n",
    "        self.up3 = StackDecoder( 128, 128,  64, kernel_size=3)  #128\n",
    "        self.up2 = StackDecoder(  64,  64,  32, kernel_size=3)  #256\n",
    "        self.classify = nn.Conv2d(32, out_c, kernel_size=1, padding=0, stride=1, bias=True)\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "\n",
    "        out = x                       #;print('x    ',x.size())\n",
    "                                      #\n",
    "        down2,out = self.down2(out)   #;print('down2',down2.size())  #128\n",
    "        down3,out = self.down3(out)   #;print('down3',down3.size())  #64\n",
    "        down4,out = self.down4(out)   #;print('down4',down4.size())  #32\n",
    "        down5,out = self.down5(out)   #;print('down5',down5.size())  #16\n",
    "        down6,out = self.down6(out)   #;print('down6',down6.size())  #8\n",
    "        pass                          #;print('out  ',out.size())\n",
    "\n",
    "        out = self.center(out)\n",
    "        out = self.up6(down6, out)\n",
    "        out = self.up5(down5, out)\n",
    "        out = self.up4(down4, out)\n",
    "        out = self.up3(down3, out)\n",
    "        out = self.up2(down2, out)\n",
    "\n",
    "        out = self.classify(out)\n",
    "        out = torch.squeeze(out, dim=1)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# 128x128\n",
    "class Unet128 (nn.Module):\n",
    "    def __init__(self, in_c=3, out_c=3, f=1):\n",
    "        super().__init__()\n",
    "        #assert(C==3)\n",
    "\n",
    "        #128\n",
    "        self.down3 = StackEncoder(in_c,   128//f, kernel_size=3)   # 64\n",
    "        self.down4 = StackEncoder(128//f,  256//f, kernel_size=3)   # 32\n",
    "        self.down5 = StackEncoder(256//f,  512//f, kernel_size=3)   # 16\n",
    "        self.down6 = StackEncoder(512//f, 1024//f, kernel_size=3)   #  8\n",
    "\n",
    "        self.center = nn.Sequential(\n",
    "            ConvBnRelu2d(1024//f, 1024//f, kernel_size=3, padding=1, stride=1 ),\n",
    "        )\n",
    "\n",
    "        # 8\n",
    "        # x_big_channels, x_channels, y_channels\n",
    "        self.up6 = StackDecoder(1024//f,1024//f, 512//f, kernel_size=3)  # 16\n",
    "        self.up5 = StackDecoder( 512//f, 512//f, 256//f, kernel_size=3)  # 32\n",
    "        self.up4 = StackDecoder( 256//f, 256//f, 128//f, kernel_size=3)  # 64\n",
    "        self.up3 = StackDecoder( 128//f, 128//f,  64//f, kernel_size=3)  #128\n",
    "        self.classify = nn.Conv2d(64//f, out_c, kernel_size=1, padding=0, stride=1, bias=True)\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "\n",
    "        out = x                       #;print('x    ',x.size())\n",
    "        down3,out = self.down3(out)   #;print('down3',down3.size())  #64\n",
    "        down4,out = self.down4(out)   #;print('down4',down4.size())  #32\n",
    "        down5,out = self.down5(out)   #;print('down5',down5.size())  #16\n",
    "        down6,out = self.down6(out)   #;print('down6',down6.size())  #8\n",
    "        pass                          #;print('out  ',out.size())\n",
    "\n",
    "        out = self.center(out)\n",
    "        out = self.up6(down6, out)\n",
    "        out = self.up5(down5, out)\n",
    "        out = self.up4(down4, out)\n",
    "        out = self.up3(down3, out)\n",
    "        out = self.classify(out)\n",
    "        out = torch.squeeze(out, dim=1)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## U-net (ish)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_base(f):\n",
    "    cut,lr_cut = model_meta[f]\n",
    "    layers = cut_model(f(True), cut)\n",
    "    return nn.Sequential(*layers), lr_cut"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SaveFeatures():\n",
    "    features=None\n",
    "    def __init__(self, m): self.hook = m.register_forward_hook(self.hook_fn)\n",
    "    def hook_fn(self, module, input, output): self.features = output\n",
    "    def remove(self): self.hook.remove()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "class UnetModel():\n",
    "    def __init__(self,model,name='unet'):\n",
    "        self.model,self.name = model,name\n",
    "\n",
    "    def get_layer_groups(self, precompute):\n",
    "        if isinstance(self.model, FP16):\n",
    "            model = self.model.module\n",
    "        else:\n",
    "            model = self.model\n",
    "        if isinstance(model, Unet128):\n",
    "            return children(model)\n",
    "        lgs = list(split_by_idxs(children(model.rn), [model.lr_cut]))\n",
    "#         print('LGS:', lgs)\n",
    "#         print('Add:', children(model)[1:])\n",
    "        return lgs + [children(model)[1:]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_learner(md, m_fn=Unet128):\n",
    "    m = m_fn(f=2)\n",
    "#     models = UnetModel(m)\n",
    "    learn = Learner.from_model_data(m, md)\n",
    "    learn.opt_fn=optim.Adam\n",
    "    class_weights = torch.FloatTensor([1,10,2]).cuda()\n",
    "    learn.crit=nn.CrossEntropyLoss(weight=class_weights)\n",
    "#     learn.crit=nn.CrossEntropyLoss()\n",
    "#     learn.crit=FocalLoss(2)\n",
    "#     learn.crit = nn.BCEWithLogitsLoss()\n",
    "    learn.unfreeze()\n",
    "    learn.metrics=[new_acc]\n",
    "    return learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "ext = '-150'\n",
    "sz = 96\n",
    "bs = 128\n",
    "md = torch_loader('-150', PATH, bs, sz)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn = get_learner(md)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "# learn.half()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "x,y = next(iter(md.trn_dl))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "100"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(md.val_ds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading from train6 model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "47a9b25eff824de9a32250a81c4db1fb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Epoch', max=1), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch      trn_loss   val_loss   new_acc                   \n",
      "    0      0.842691   1814148892590.08 0.750257  \n",
      "\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "learn.lr_find()\n",
    "learn.sched.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lr=1e-1\n",
    "lr=1e-1\n",
    "# lr=4e-3\n",
    "wd=1e-5\n",
    "\n",
    "lrs = np.array([lr/200,lr/20,lr])/2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7b711e822bec4ab78b9ccca6f68dd9c6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Epoch', max=15), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch      trn_loss   val_loss   new_acc                   \n",
      "    0      0.572614   2.873173   0.595249  \n",
      "    1      0.381271   0.347716   0.867502                  \n",
      "    2      0.27807    0.629201   0.726237                  \n",
      "    3      0.200357   0.287675   0.928396                  \n",
      "    4      0.156998   0.209122   0.928278                  \n",
      "    5      0.123174   0.290767   0.855201                  \n",
      "    6      0.102526   0.148488   0.973649                  \n",
      "    7      0.087298   0.272252   0.975354                   \n",
      "    8      0.085164   0.132615   0.952092                   \n",
      "    9      0.073602   0.097152   0.975305                   \n",
      "    10     0.063604   0.095123   0.972924                   \n",
      "    11     0.05672    0.07184    0.985555                   \n",
      "    12     0.054425   0.187924   0.930137                   \n",
      "    13     0.051507   0.15454    0.936006                   \n",
      "    14     0.050772   0.073142   0.975328                   \n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[array([0.07314]), 0.9753278493881226]"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "learn.fit(lr,1,wds=wd,cycle_len=15,use_clr=(10,4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.save(f'128urn-{S_PREFIX}-tmp')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.load(f'128urn-{S_PREFIX}-tmp')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "48b2a059e5574ba89e93eb4a56cda87e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Epoch', max=15), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch      trn_loss   val_loss   new_acc                    \n",
      "    0      0.048848   0.130075   0.959397  \n",
      "    1      0.053458   0.191285   0.959747                   \n",
      "    2      0.075465   0.663494   0.835777                   \n",
      "    3      0.072768   0.145132   0.979384                   \n",
      "    4      0.062008   0.107209   0.961669                   \n",
      "    5      0.057239   0.142442   0.940138                   \n",
      "    6      0.058539   0.537788   0.91265                    \n",
      "    7      0.054253   0.391048   0.84814                    \n",
      "    8      0.052148   0.124823   0.981365                   \n",
      "    9      0.046622   0.044095   0.987012                   \n",
      "    10     0.043014   0.080274   0.982583                   \n",
      "    11     0.044073   0.057319   0.980984                   \n",
      "    12     0.042381   0.065214   0.977059                   \n",
      "                                                            \r"
     ]
    }
   ],
   "source": [
    "learn.fit(lr,1,wds=wd,cycle_len=15,use_clr=(10,4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.save(f'128urn-{S_PREFIX}-0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.load(f'128urn-{S_PREFIX}-0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x,y = next(iter(md.val_dl))\n",
    "py = to_np(learn.model(V(x)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "py = np.argmax(py,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_img(denorm(x[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_img(py[0]);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_img(y[0]);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 256x256"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ext = '-300'\n",
    "sz=192\n",
    "bs=64\n",
    "md = torch_loader(ext, PATH, bs, sz)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr=1e-1\n",
    "wd=1e-6\n",
    "\n",
    "lrs = np.array([lr/200,lr/20,lr])/2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn = get_learner(md)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.freeze_to(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.lr_find()\n",
    "learn.sched.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "learn.load(f'128urn-{S_PREFIX}-0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.fit(lr,1,wds=wd, cycle_len=10,use_clr=(5,5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.save(f'256urn-{S_PREFIX}-tmp')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.unfreeze()\n",
    "learn.bn_freeze(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.load(f'256urn-{S_PREFIX}-tmp')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.fit(lrs/4,1,wds=wd, cycle_len=8,use_clr=(20,8))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.fit(lrs/4,1,wds=wd, cycle_len=8,use_clr=(20,8))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.save(f'256urn-{S_PREFIX}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.load(f'256urn-{S_PREFIX}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x,y = next(iter(md.trn_dl))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x,y = next(iter(md.val_dl))\n",
    "py = to_np(learn.model(V(x)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "py = np.argmax(py,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(denorm(x[-1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_img(py[-1]);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "py[-1].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y[-1].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_img(y[-1]);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 512x512"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAIN_DN = 'CameraRGB'\n",
    "MASKS_DN = 'CameraSeg'\n",
    "sz=288\n",
    "bs=16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ext = ''\n",
    "sz=288\n",
    "bs=24\n",
    "md = torch_loader(ext, PATH, bs, sz)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn = get_learner(md)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.lr_find()\n",
    "learn.sched.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.load(f'256urn-{S_PREFIX}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.freeze_to(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr=4e-2\n",
    "wd=5e-7\n",
    "\n",
    "lrs = np.array([lr/200,lr/20,lr])/2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "learn.fit(lr,1, wds=wd, cycle_len=4,use_clr=(5,4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.fit(lr,1, wds=wd, cycle_len=4,use_clr=(5,4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.save(f'600urn-{S_PREFIX}-tmp')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.load(f'600urn-{S_PREFIX}-tmp')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.unfreeze()\n",
    "learn.bn_freeze(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lrs = np.array([lr/200,lr/30,lr])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.fit(lrs/10,1, wds=wd,cycle_len=4,use_clr=(20,8))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.fit(lrs/10,1, wds=wd,cycle_len=4,use_clr=(20,8))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.sched.plot_loss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.save(f'600urn-{S_PREFIX}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.fit(lrs/8,1, wds=wd,cycle_len=4,use_clr=(20,8))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.fit(lrs/8,1, wds=wd,cycle_len=4,use_clr=(20,8))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.save(f'600urn-{S_PREFIX}-last')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.fit(lrs/8,1, wds=wd,cycle_len=20,use_clr=(20,8))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.fit(lrs/50,1, wds=wd,cycle_len=5,use_clr=(20,8))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.fit(lrs/200,1, wds=wd,cycle_len=3,use_clr=(2,8))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.save(f'600urn-{S_PREFIX}-e20')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.load(f'600urn-{S_PREFIX}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ext = ''\n",
    "sz=288\n",
    "bs=16\n",
    "md = torch_loader(ext, PATH, bs, sz, random_crop=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn = get_learner(md)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# learn.load(f'600urn-{S_PREFIX}-full')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.unfreeze()\n",
    "learn.bn_freeze(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.fit(lrs/10,1, wds=wd,cycle_len=4,use_clr=(20,8))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.fit(lrs/10,1, wds=wd,cycle_len=4,use_clr=(20,8))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.fit(lrs/10,1, wds=wd,cycle_len=4,use_clr=(20,8))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.save(f'600urn-{S_PREFIX}-full')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x,y = next(iter(md.val_dl))\n",
    "py = to_np(learn.model(V(x)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "py = np.argmax(py,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(denorm(x[10]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_img(py[10]);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "py[-1].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y[-1].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_img(y[10]);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys, skvideo.io, json, base64\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "from io import BytesIO, StringIO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m_base = get_base()\n",
    "m = to_gpu(Unet34(m_base))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model_summary(m, [3,608,800])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# learn.load('1024urn')\n",
    "load_model(m, str(PATH/f'models/600urn-{S_PREFIX}.h5'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# file = sys.argv[-1]\n",
    "file = 'test_video.mp4'\n",
    "\n",
    "if file == 'demo.py':\n",
    "    print(\"Error loading video\")\n",
    "    quit\n",
    "\n",
    "# Define encoder function\n",
    "def encode(array):\n",
    "    pil_img = Image.fromarray(array)\n",
    "    buff = BytesIO()\n",
    "    pil_img.save(buff, format=\"PNG\")\n",
    "    return base64.b64encode(buff.getvalue()).decode(\"utf-8\")\n",
    "\n",
    "video = skvideo.io.vread(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# resized_video = np.array([scipy.misc.imresize(f, size=(512,512)) for f in video])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize(x):\n",
    "    if np.mean(x) > 1:\n",
    "        x = x/255\n",
    "    m,s = imagenet_stats\n",
    "    x = (x-m)/s\n",
    "    return x\n",
    "def preprocess(video):\n",
    "    f1_norm = normalize(video)\n",
    "    f1_roll = np.rollaxis(f1_norm, 3, 1)\n",
    "    f1_pad = np.pad(f1_roll, [(0,0),(0,0),(0,8),(0,0)], mode='constant')\n",
    "    return f1_pad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f1 = preprocess(video)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = []\n",
    "for i in range(0,f1.shape[0],8):\n",
    "    xv = VV(torch.from_numpy(f1[i:i+8]).contiguous().float())\n",
    "    preds = m(xv)\n",
    "    mx,idx = torch.max(preds, 1)\n",
    "    idx_slice = idx[:,:-8,:]\n",
    "    results.append(idx_slice)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r_stacked = torch.cat(results,0)\n",
    "r_np = r_stacked.data.cpu().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "def plot_res(index):\n",
    "    f, (ax1, ax2) = plt.subplots(1, 2, figsize=(24, 15))\n",
    "    f.tight_layout()\n",
    "    ax1.imshow(video[index])\n",
    "    ax2.imshow(r_np[index])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_res(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "answer_key = {}\n",
    "\n",
    "# Frame numbering starts at 1\n",
    "frame_idx = 1\n",
    "for frame in r_np:\n",
    "    # Look for red cars :)\n",
    "    binary_car_result = (frame==1).astype('uint8')\n",
    "#     print(np.mean(binary_car_result))\n",
    "    \n",
    "    # Look for road :)\n",
    "    binary_road_result = (frame==2).astype('uint8')\n",
    "\n",
    "    answer_key[frame_idx] = [encode(binary_car_result), encode(binary_road_result)]\n",
    "    \n",
    "    # Increment frame\n",
    "    frame_idx+=1\n",
    "\n",
    "# Print output in proper json format\n",
    "tester_data = json.dumps(answer_key)\n",
    "with open('tester_data_multi_take2', 'w') as f:\n",
    "    f.write(tester_data)\n",
    "print(json.dumps(answer_key))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decoding car"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy import misc\n",
    "def decode(packet):\n",
    "\timg = base64.b64decode(packet)\n",
    "\tfilename = PATH/'image.png'\n",
    "\twith open(filename, 'wb') as f:\n",
    "\t\t\tf.write(img)\n",
    "\tresult = misc.imread(filename)\n",
    "\treturn result\n",
    "\n",
    "with open('results.json') as json_data:\n",
    "\tans_data = json.loads(json_data.read())\n",
    "\tjson_data.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_ans(index):\n",
    "    ans = decode(ans_data[str(index)][0])\n",
    "    f, (ax1, ax2, ax3) = plt.subplots(1, 3, figsize=(24, 15))\n",
    "    f.tight_layout()\n",
    "    ax1.imshow(r_np[index])\n",
    "    ax1.set_title('Mine', fontsize=35)\n",
    "    ax2.imshow(ans)\n",
    "    ax2.set_title('Answer', fontsize=35)\n",
    "    ax3.imshow(video[index])\n",
    "    ax2.set_title('Original', fontsize=35)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_ans(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ans = decode(ans_data['1'][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "def plot_res(index):\n",
    "    f, (ax1, ax2) = plt.subplots(1, 2, figsize=(24, 15))\n",
    "    f.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  },
  "toc": {
   "colors": {
    "hover_highlight": "#DAA520",
    "navigate_num": "#000000",
    "navigate_text": "#333333",
    "running_highlight": "#FF0000",
    "selected_highlight": "#FFD700",
    "sidebar_border": "#EEEEEE",
    "wrapper_background": "#FFFFFF"
   },
   "moveMenuLeft": true,
   "nav_menu": {
    "height": "86px",
    "width": "252px"
   },
   "navigate_menu": true,
   "number_sections": true,
   "sideBar": true,
   "threshold": 4,
   "toc_cell": false,
   "toc_section_display": "block",
   "toc_window_display": false,
   "widenNotebook": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
